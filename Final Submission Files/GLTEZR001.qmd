---
title: "GLTEZR001"
format: html
---

## Abstract {#sec-Abstract}

### Abstract

This study investigates the application of neural networks for avalanche hazard level prediction using a large historical dataset from the Scottish Avalanche Information Service. The models are trained on predictors such as meteorological, snowpack and topographical features. Initially, the historical data was preprocessed into training, validation and tests sets. The model was trained and performance was compared against baseline expectations with feature importance also being investigated.

The results of the model are promissing with the neural networks being able to successfully capture the non-linear and complex interactions between the features. Challenges such as misclassificationrates for high hazard levels and limited interpretability of the model were additionally observed, showing potential for improvement in the future with methods such as resampling. Overall, this investigation shows the potential and limitations of implementing neural networks for avalanche hazard level prediction.

## 1 Introduction and Lit Review {#sec-Introduction_and_Lit_Review}

### 1.1 Introduction

Scotland is a region with frequent snow accumulation as well as a high amount of human activity. It is therefore vital that avalanche forecasting is carried out as a key component of their mountain safety and reducing economic losses. Avalanche forecasting can prove to be difficult as it is dependent on many factors with complex relationships and interactions, such as snow properties, meteorological conditions and terrain features. Usually, avalanche forecasts are predicted using a combination of weather reports as well as field observations to release daily bulletins. Although these are essential, the rapidly expanding historical weather and avalanche datasets have created the oppurtunity for new, data driven methods.

Machine learning, specifically neural networks, have become strong contenders for avalanche forecasting due to their ability to model complex and multi-dimensional relationships. From the ever growing historical datasets, these methods can uncover patterns that have previously not been utilised in avalanche forecasting. The Scottish Avalanche Information Service (SAIS) provides a large amount of historical data that can be utilised to train neural networks to improve avalanche risk management. In this report, this dataset will be used to train and evaluate neural network models for the potential to forecast avalanches in the Scottish region. 

### 1.2 Literature Review

Avalanche forecasting and research spans across snow science, meteorology and data science. Initially, intuition and rules of thumb based on snowfall and temperature were used to predict avalanches. These methods proved to be inconsistent and struggled to generalise diverse snow conditions [7]. These methods moved towards statistical and probabilistic methods. This more systematic process could integrate more predictors as well as quantify uncertainty [8].

Due to the ever-growing amount of data, machine learning methods have been utilised to predict avalanches. Random forrest and support vector machines can be applied to large avalanche datasets and achieve high accuracy, especially compared to more traditional models [4]. This proves the potential for machine learning methods to capture complex and nonlinear relationships between avalanche predictor variables. From machine learning methods, neural networks stand out due to their flexibility and ability to work with high dimensional predictors. Deep learning applications related to avalanches are snow depth prediction, snowpack classification and exploratory hazard modelling [9]. Although results have been positive, neural networks have not been fully developed or implemented into avalanche forecasting.

For avalanche forecasting, snowpack stability indicators are vital. These include temperature gradients and snow temperature profiles [13]. Additionally, meteorological variables, such as wind speed, air temperature and precipitation, directly affect snow deposition and terrain features, such as aspect and slope angle affect avalanche likelyhood [1]. These predictors have complex relationships and patterns that traditional methods do not capture however mehtods such as neural networks can uncover these subtle interactions.

From the literature, it has been shown that avalanche forecasting methods have evolved from intuition and expert-based assessments to statistical methods and then further to machine learning methods. Studies have been completed to confirm the feasibility of data-driven avalanche forecasting however integration into operational practices is underdeveloped.

## 2 Data and Methods {#sec-Data_and_Methods}

The dataset used in this study consists of daily avalanche forecasts for six forecasting regions across Scotland (Creag Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon), collected by the Scottish Avalanche Information Service (SAIS) between 2009 and 2025. It contains 10,671 observations and 34 variables including weather conditions, topography, snowpack properties, and avalanche hazard levels.

The target variable for this project is the Forecast Avalanche Hazard (FAH), a categorical variable with five ordered levels: Low, Moderate, Considerable -, Considerable +, and High.

Data Cleaning and Preprocessing was done to remove observations that did not make logical sense, transform the data where needed and remove necessary variable. Furthermore, Exploratory Data Analysis (EDA) was then performed to assess variable distributions and perform imputation, outlier handling and scaling of data.

### 2.1 Data Cleaning and Preprocessing

The raw dataset underwent extensive cleaning prior to analysis. Initially, rows with missing FAH values (109 rows) were removed, and FAH levels were replaced with a numerical representation from 0 to 4 for compatibility with the neural network software. Other categorical variables, such as precipitation type and forecast region, were similarly transformed into numerical formats. The Date column was transformed into Month to capture seasonal effects.

Variables unrelated to forecasting or with high missingness (more than 20%), frequent errors, limited predictive value, or poorly defined measures were excluded, resulting in a set of well-defined predictors most relevant for forecasting.

Range checks were applied to remove physically impossible or implausible values based physical limits and regional context.

For example, altitude was limited to 0–1,345 m, the height of Ben Nevis, and wind speeds to 176 mph, reflecting the highest recorded gust in the Highlands [11].

Slope inclines were restricted to 0–90°, and wind directions were constrained to 0–360°. Cloud cover was limited to 0–100%, total snow depth to 0–1,500 cm (with unrealistic spikes removed), and foot penetration depth to 100 cm. Snow temperatures above 0.5 °C were flagged as implausible, since snow begins melting at 0 °C, and maximum temperature gradients were limited to below 25 °C per 10 cm [15]. Observations where foot penetration exceeded total snow depth were also removed.

Circular variables such as wind direction and aspect were transformed into sine and cosine components.

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(corrplot)
library(lubridate)
library(gridExtra)
library(VIM)
library(mice)
library(caret)
library(ggplot2)
```

```{r, echo=FALSE}
data <- read.csv("scotland_avalanche_forecasts_2009_2025.csv", header = TRUE)
```

```{r, echo=FALSE}
# Only keep rows where the target variable FAH is not missing (-109 rows)
data <- data %>% filter(!is.na(FAH) & FAH != "")

# Convert categorical target variable to numeric 
data$FAH <- as.integer(factor(x = data$FAH, levels = c("Low", "Moderate", "Considerable -", "Considerable +", "High"))) - 1
data$Precip.Code <- as.integer(factor(data$Precip.Code)) - 1
data$Area <- as.integer(factor(data$Area)) - 1   # 6 areas

# Create a year variable for date & Month
# Don't have any data for July, Aug and Sept (summer months) 
# Don't know if a month variable will be important? Should i make those months values be 0?
data <- data %>% mutate(Year = year(Date), Month = month(Date))
# table(data$Month)

# Remove unnecessary columns
data <- data %>% select(-OSgrid, -OAH, -Obs, -Location, -Date)
```

```{r, echo=FALSE}
missing_summary <- data %>%
  summarise_all(~sum(is.na(.))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percent = round(Missing_Count / nrow(data) * 100, 1)) %>%
  arrange(desc(Missing_Count))
# missing_summary
```

```{r, echo=FALSE}
data_cleaned <- data %>%
  mutate(
    Alt = ifelse(Alt < 0 | Alt > 1345, NA_real_, Alt),
    Aspect = ifelse(Aspect < 0 | Aspect > 360, NA_real_, Aspect),
    Incline = ifelse(Incline < 0 | Incline > 90, NA_real_, Incline),
    Wind.Speed = ifelse(Wind.Speed < 0 | Wind.Speed > 176, NA_real_, Wind.Speed),
    Cloud = ifelse(Cloud < 0 | Cloud > 100, NA_real_, Cloud),
    Total.Snow.Depth = ifelse(Total.Snow.Depth < 0 | Total.Snow.Depth > 1500, NA_real_, Total.Snow.Depth),
    Summit.Wind.Speed = ifelse(Summit.Wind.Speed < 0 | Summit.Wind.Speed > 176, NA_real_, Summit.Wind.Speed),
    Foot.Pen = ifelse(Foot.Pen < 0 | Foot.Pen > 100, NA_real_, Foot.Pen),
    Wind.Dir = ifelse(Wind.Dir < 0 | Wind.Dir > 360, NA_real_, Wind.Dir),
    Summit.Wind.Dir = ifelse(Summit.Wind.Dir < 0 | Summit.Wind.Dir > 360, NA_real_, Summit.Wind.Dir),
    Snow.Temp = ifelse(Snow.Temp > 0.5, NA_real_, Snow.Temp),
    Max.Temp.Grad = ifelse(Max.Temp.Grad < 0 | Max.Temp.Grad > 2.5, NA_real_, Max.Temp.Grad))
```

```{r, echo=FALSE}
# Convert circular variables into two features using sine and cosine transformations
data_cleaned <- data_cleaned %>% 
  mutate(Aspect_sin = sin(Aspect * pi / 180), Aspect_cos = cos(Aspect * pi / 180)) %>% 
  mutate(WindDir_sin = sin(Wind.Dir * pi / 180), WindDir_cos = cos(Wind.Dir * pi / 180)) %>%
  mutate(SummitWindDir_sin = sin(Summit.Wind.Dir * pi / 180), SummitWindDir_cos = cos(Summit.Wind.Dir * pi / 180)) %>%
  select(-Wind.Dir, -Aspect, -Summit.Wind.Dir)

```

```{r, echo=FALSE}
# Remove variables with high percentage of missing values or  those that don't make sense

data_cleaned <- data_cleaned %>%
  select(-Ski.Pen, -AV.Cat, -Crystals, -Wetness, -Year, -Snow.Index, -Insolation, -No.Settle)
#names(data_cleaned)

#dim(data_cleaned)
```

```{r, echo=FALSE}
data_cleaned <- data_cleaned %>%
  filter(is.na(Foot.Pen) | is.na(Total.Snow.Depth) | Foot.Pen <= Total.Snow.Depth)
```

```{r, echo=FALSE}
missing_summary <- data_cleaned %>%
  summarise_all(~sum(is.na(.))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percent = round(Missing_Count / nrow(data_cleaned) * 100, 1)) %>%
  arrange(desc(Missing_Count))
# missing_summary

# dim(data_cleaned)
```

### 2.2 Predictor Sets

```{r, echo=FALSE}

pred_1 <- c("Area", "longitude", "latitude", "Alt", "Incline", "Aspect_sin", "Aspect_cos")

pred_2 <- c("Air.Temp", "Wind.Speed", "Cloud", "Precip.Code", "Drift",
           "Summit.Air.Temp", "Summit.Wind.Speed", "WindDir_sin", "WindDir_cos", 
           "SummitWindDir_sin", "SummitWindDir_cos")

pred_3 <- c("Total.Snow.Depth", "Foot.Pen", "Rain.at.900", "Max.Temp.Grad", 
           "Max.Hardness.Grad", "Snow.Temp")

other_vars <- c("Month")

all_pred <- c(pred_1, pred_2, pred_3, other_vars)
```

The data was split into three predictor sets; pred1, pred2, and pred3, where each predictor set represents different types of information relating to snow and avalanche conditions. Furthermore, predictor set 1 represents the location data such as the area, exact location coordinates, and how steep the slope is etc. Predictor set 2 represents the air conditions such as air temperature, cloud cover, and wind speed etc. Lastly, predictors 3 represents the snow conditions such as how deep the snow is, the temperature of the snow at different layers and foot penetration. All three predictor sets essentially contribute to determining how the events have affected the snowfall. In addition, the month variable does not fit in any predictor set, but rather adds seasonal context since snow behaviour differs throughout the year. The predictor sets were separated into the following sets:

Predictor Set 1: Area, longitude, latitude, Alt, Incline, Aspect_sin, and Aspect_cos

Predictor Set 2: Air.Temp, Wind.Speed, Cloud, Precip.Code, Drift, Summit.Air.Temp, Summit.Wind.Speed, WindDir_sin, WindDir_cos, SummitWindDir_sin, and SummitWindDir_cos

Predictor Set 3: Total.Snow.Depth, Foot.Pen, Rain.at.900, Max.Temp.Grad, Max.Hardness.Grad, and Snow.Temp

Other Variables: Month

### 2.3 Imputation

Multiple Imputation by Chained Equations (MICE) handles missing data that creates multiple versions of a dataset rather than selecting the first best guessed value. The MICE function ran under the assumption of Missing at Random (MAR), which means that it was assumed that the probability of a value being missing depends on the observed values in the dataset, but are independent of the unobserved values [5].

```{r, echo=FALSE, message=FALSE}
# No variables have more than 50% missing data so impute all missing values.

md.pattern(data_cleaned[,c("FAH", all_pred[1:min(10, length(all_pred))])], rotate.names = TRUE)

```

The figure above shows the pattern of missing values for a subset of 10 variables, where blue represents no missing values and red represents missing values present. It can be seen that the sum of missing values for the 10 variables was 774 values, and it can be seen that aspect_sin and aspect_cos have the highest number of missing values (320 values) among these 10 variables. Furthermore, wind.speed, incline, air.temp and cloud have the next highest missing values of 59, 27, 21 and 19 respectively.

In addition, it was seen that variables like area, latitude, longitude and alt have no missing values, which shows that they are reliable predictors for the missing values in other varaiables.

```{r, echo=FALSE}

mice_data <- data_cleaned %>%
  select(FAH, all_of(all_pred))

set.seed(12345)

mice_output <- mice(mice_data, m = 5, method = 'pmm', printFlag = FALSE)

data_imputed <- complete(mice_output, 1)


```

MICE was run with five iterations creating five slightly different datasets and Predictive Mean Matching (PMM) was used, which uses the information in the dataset to predict what the missing values would most likely be. In addition, all variables that were in the cleaned dataset which contained missing values were selected for imputation, as the highest percentage of missing data was 14.2%, so there were enough observed variables to predict the unobserved variables well.

In addition, after running the MICE imputation, the density plots were analysed to determine if the variables were imputed well or not, with imputed values represented by the red line and observed values represented by the blue line. Imputed values should not change the distribution of a variable [10], thus, if the imputed variables shows a different distribution to the observed variables, the variable was removed from the dataset.

```{r, echo=FALSE}

# Diagnostic Plots
# Strip plot - shows distribution of imputed values

stripplot(mice_output, pch=20, cex=1.2)

```

The figure above displays a stripplot from the MICE imputation, which shows the distribution across all variables and the five different imputation sets of observed values represented by blue and the imputed values represented by red. It can be seen that the red and blue dots overlap well and follow similar distributions in multiple variables which shows that the imputation worked well.

However, variables such as Alt, Air.Temp, and Incline show that imputed values do not follow the same distribution as the observed values as the red dots do not overlap well and indicates that the data failed to impute well under the MAR assumption.

The density plots for the variables in question were plotted as well as the density plot of Max.Hardness.Grad which imputed well as a comparison.

```{r, echo=FALSE}

# densityplot(mice_output, ~ Alt) # bad
# densityplot(mice_output, ~ Aspect_sin) # maybe
# densityplot(mice_output, ~ Aspect_cos) 
# densityplot(mice_output, ~ Incline) # bad

# densityplot(mice_output, ~ Air.Temp) # bad
 # densityplot(mice_output, ~ WindDir_sin) # maybe
# densityplot(mice_output, ~ WindDir_cos) # maybe
# densityplot(mice_output, ~ Wind.Speed)
# densityplot(mice_output, ~ Cloud) # maybe

# densityplot(mice_output, ~ Summit.Air.Temp)
# densityplot(mice_output, ~ SummitWindDir_sin)
# densityplot(mice_output, ~ SummitWindDir_cos)
# densityplot(mice_output, ~ Summit.Wind.Speed)

# densityplot(mice_output, ~ Max.Temp.Grad) # maybe
# densityplot(mice_output, ~ Max.Hardness.Grad) 
# densityplot(mice_output, ~ Snow.Temp) # maybe


p1 <- densityplot(mice_output, ~ Alt, main = "Alt")
p2 <- densityplot(mice_output, ~ Incline, main = "Incline")
p3 <- densityplot(mice_output, ~ Air.Temp, main = "Air.Temp")
p4 <- densityplot(mice_output, ~ Max.Hardness.Grad, main = "Max.Hardness.Grad")

grid.arrange(p1, p2, p3, p4, ncol = 2)

```

It can be seen from the figure above that the imputed values in red do not follow the observed values in blue for Alt, Incline, and Air.Temp. However, comparing it to a well imputed variable like Max.Hardness.Grad, it can be seen that the imputed values follows the observed values distribution closely. This exercise was done for each of the imputed variable, however, just the variables that did not impute well and one variable that did impute well was shown in the final report.

```{r, echo=FALSE}

# Convergence plot

# plot(mice_output, main = "MICE Convergence - All Variables")

```

```{r, echo=FALSE}

bad_vars <- c("Alt", "Air.Temp", "Incline")

mice_data <- mice_data %>%
  select(FAH, all_of(setdiff(all_pred, bad_vars)))

# dim(mice_data)

set.seed(12345)

mice_output2 <- mice(mice_data, m = 5, method = 'pmm', printFlag = FALSE)
data_imputed2 <- complete(mice_output2, 1)
```

The variable that did not impute well was removed from the dataset, and the new dataset was refitted using MICE which ensured that the imputations better satisfied the MAR assumption and produced reliable results.

### 2.4 Outlier Handling

Outlier Capping or Winsorization is a method that handles outliers by converting the extreme high values to the value of the highest data point that is not considered an outlier [2]. This method was used on the imputed dataset to account for any values that were extreme to ensure that the distribution is not skewed.

```{r, echo=FALSE}

# Handle outliers by capping them

for(var in names(data_imputed2)) {
  if(is.numeric(data_imputed2[[var]])) {
    
    x <- data_imputed2[[var]]
    
    lower_bound <- quantile(x, 0.01, na.rm = TRUE)
    upper_bound <- quantile(x, 0.99, na.rm = TRUE)
    
    n_outliers <- sum(x < lower_bound | x > upper_bound, na.rm = TRUE)
    data_imputed2[[var]][x < lower_bound] <- lower_bound
    data_imputed2[[var]][x > upper_bound] <- upper_bound
    
    if(n_outliers > 0) {
      print(paste(var, "- Outliers capped:", n_outliers))
    }
  }
}
```

For each variable in the dataset, the 1st and 99th percentile was calculated to address the extreme values, preserving 98% of the data. If a value fell below the 1st percentile, it was capped to the 1st percentile value, furthermore, if a value fell above the 99th percentile, it was capped to the 99th percentile value.

In addition, outlier handling was intentionally done after imputation, in the event that MICE generated a value that fell outside of the realistic range of values in the original dataset, considering that data cleaning had already taken place.

Furthermore, it was seen that the need to cap outliers was needed as multiple variables had around \~1% of its data capped, which shows that the data contained extreme values either in the original dataset or from imputation.

### 2.5 Correlation Analysis

A correlation analysis was performed to examine the patterns in the variables and assess potential multicollinearity effects.

```{r, echo=FALSE}

# Use imputed data (no missing values)

corr_data <- data_imputed2  

corr_matrix <- cor(corr_data, use = "complete.obs")

# Correlation with FAH
# fah_corr <- corr_matrix[,"FAH"] %>%
#   as.data.frame() %>%
#   rownames_to_column("Variable") %>%
#   rename(Correlation = ".") %>%
#   filter(Variable != "FAH") %>%
#   arrange(desc(abs(Correlation)))

# Top 10 correlations with FAH
# print(head(fah_corr, 10))

# Correlation heatmap
corrplot(corr_matrix, method = "color", type = "upper", 
         order = "hclust", tl.cex = 0.7, tl.col = "black")

```

It can be seen from the figure above that correlation between variables exist as the dark blue squares represent strong positive correlations and the dark red squares represent strong negative correlations. The variables from predictor set 1 are represented by dark blue squares, indicating strong positive correlations between the variables. Moreover, FAH has moderate correlations with the variables as the colour range are in the lighter section of the colour scale.

Furthermore, it is important to note that the white squares represent independent variables (correlation of 0), and it can be seen that there are multiple variables such as Aspect_cos and Area have white squares which indicates that multicollinearity is not excessive.

Lastly, it can be seen that variables with similar correlation patterns have grouped together creating visible blocks of related variables, which confirms the predictor set groupings capture different types of information.

```{r, echo=FALSE}

# # FAH Correlation
# 
data_imputed2$FAH_factor <- factor(data_imputed2$FAH,
                           levels = 0:4,
                           labels = c("Low", "Moderate", "Considerable -", 
                                     "Considerable +", "High"))

# # Predictor 1
# 
# predictor_1 <- pred_1[pred_1 %in% names(data_imputed2)]
# 
# data1 <- data_imputed2 %>%
#   select(FAH_factor, FAH, all_of(predictor_1))
# 
# corr1 <- cor(data1[,predictor_1], data1$FAH) %>%
#   as.data.frame() %>%
#   rownames_to_column("Variable") %>%
#   rename(FAH_Correlation = "V1") %>%
#   arrange(desc(abs(FAH_Correlation)))
# 
# # Create boxplots
# plot1 <- list()
# for(i in 1:min(4, length(predictor_1))) {
#   var <- predictor_1[i]
#   plot1[[i]] <- ggplot(data1, aes(x = factor(FAH_factor), y = .data[[var]])) +
#     geom_boxplot(fill = "darkblue", alpha = 0.7) +
#     labs(title = paste(var, "vs FAH"),
#          x = "FAH", y = var) +
#     theme_minimal()
# }
# 
# do.call(grid.arrange, c(plot1, ncol = 2))
# 
# # Predictor 2
# 
# predictor_2 <- pred_2[pred_2 %in% names(data_imputed2)]
# 
# data2 <- data_imputed2 %>%
#   select(FAH_factor, FAH, all_of(predictor_2))
# 
# corr2 <- cor(data2[,predictor_2], data2$FAH) %>%
#   as.data.frame() %>%
#   rownames_to_column("Variable") %>%
#   rename(FAH_Correlation = "V1") %>%
#   arrange(desc(abs(FAH_Correlation)))
# 
# plot2 <- list()
# for(i in 1:min(4, length(predictor_2))) {
#   var <- predictor_2[i]
#   plot2[[i]] <- ggplot(data2, aes(x = factor(FAH_factor), y = .data[[var]])) +
#     geom_boxplot(fill = "darkgreen", alpha = 0.7) +
#     labs(title = paste(var, "vs FAH"),
#          x = "FAH", y = var) +
#     theme_minimal()
# }
# 
# do.call(grid.arrange, c(plot2, ncol = 2))
# 
# # Predictor 3
# 
# predictor_3 <- pred_3[pred_3 %in% names(data_imputed2)]
# 
# data3 <- data_imputed2 %>%
#   select(FAH_factor, FAH, all_of(predictor_3))
# 
# corr3 <- cor(data3[,predictor_3], data3$FAH) %>%
#   as.data.frame() %>%
#   rownames_to_column("Variable") %>%
#   rename(FAH_Correlation = "V1") %>%
#   arrange(desc(abs(FAH_Correlation)))
# 
# plot3 <- list()
# for(i in 1:min(4, length(predictor_3))) {
#   var <- predictor_3[i]
#   plot3[[i]] <- ggplot(data3, aes(x = factor(FAH_factor), y = .data[[var]])) +
#     geom_boxplot(fill = "darkred", alpha = 0.7) +
#     labs(title = paste(var, "vs FAH"),
#          x = "FAH", y = var) +
#     theme_minimal()
# }
# 
# do.call(grid.arrange, c(plot3, ncol = 2))
```

### 2.6 Data Scaling

Scaling was performed to ensure that variables with different units and scales contributed equally to the model and variables with larger numeric values did not dominate the model.

```{r, echo=FALSE}

set.seed(12345)

train_index <- createDataPartition(data_imputed2$FAH, 
                                   p = 0.7, 
                                   list = FALSE)

train_data <- data_imputed2[train_index, ]
test_data <- data_imputed2[-train_index, ]

# Standardisation

numeric_vars <- names(data_imputed2)[sapply(data_imputed2, is.numeric)]
numeric_vars <- setdiff(numeric_vars, "FAH")

preproc_obj <- preProcess(
  train_data %>% select(all_of(numeric_vars)),
  method = c("center", "scale")  
)

train_scaled <- predict(preproc_obj, train_data)
test_scaled <- predict(preproc_obj, test_data)

predictor_set_1_train = select(train_scaled, longitude:Aspect_cos)
predictor_set_2_train = select(train_scaled, Wind.Speed:Rain.at.900)
predictor_set_3_train = select(train_scaled, Max.Temp.Grad:Snow.Temp)
predictor_set_4_train = select(train_scaled, c(-FAH, -FAH_factor))
training_data = list(predictor_set_1_train, predictor_set_2_train, 
                     predictor_set_3_train, predictor_set_4_train)
# save(training_data, file = 'data/training_data.RData')

predictor_set_1_test = select(test_scaled, longitude:Aspect_cos)
predictor_set_2_test = select(test_scaled, Wind.Speed:Rain.at.900)
predictor_set_3_test = select(test_scaled, Max.Temp.Grad:Snow.Temp)
predictor_set_4_test = select(test_scaled, c(-FAH, -FAH_factor))
testing_data = list(predictor_set_1_test, predictor_set_2_test, 
                    predictor_set_3_test, predictor_set_4_test)
# save(testing_data, file = 'data/testing_data.RData')

y_train = train_scaled$FAH
# save(y_train, file = 'data/y_train.RData')
y_test  = test_scaled$FAH
# save(y_test, file = 'data/y_test.RData')

```

The data was split using a 70/30 split, which was stratified using FAH to ensure that both the training and testing sets follow a similar distribution of the target variable, FAH. Thereafter, the numeric values from the training dataset were used to calculate the mean and standard deviation of each variable using the preProcess() function and was then applied to both the training and test sets using the predict() function. Furthermore, this approach prevents data leakage which occurs when information from the test set goes into the training set.

## 3. Results {#sec-Results}

```{r packages and libraries, eval=TRUE, include =F}
library(keras)
library(kerastuneR)
library(tensorflow)
library(dplyr)
library(tidyr)
library(reticulate)
library(caret)
library(knitr)
library(kableExtra)
library(ggplot2)
```

```{r data}
load('../data/training_data.RData')
load('../data/testing_data.RData')

load('../data/y_train.RData')
load('../data/y_test.RData')

y_train = to_categorical(y_train, num_classes = 5)
y_test  = to_categorical(y_test, num_classes = 5)

comp_mat = matrix(c(colSums(y_train) / nrow(y_train), 
                    colSums(y_test) / nrow(y_test)),
                  byrow = T, nrow = 2)
colnames(comp_mat) = paste('category ', 0:4)
rownames(comp_mat) = c('train', 'test')
save(comp_mat, file = 'test/category_imbalance.RData')
```

```{r imbalance_matrix, eval=TRUE, include = F}
#| label: tbl-imbalance
load('../test/category_imbalance.RData')
kable(comp_mat, digits = 3, caption = 'Comparison of percentage of each category in the training and test sets')
```

```{r model building function, include = F}
model_builder = function(hp){
  
  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 5, step = 1)
  lr = hp$Choice('learning_rate', values = seq(from = 1e-2, to = 1e-4, length.out = 5))  
  
  n_x   = ncol(x_train)
  input = layer_input(shape = c(n_x))

  x = input
  for (i in 1:n_layers){
    
    n_nodes = hp$Int(paste0('nodes_layer_', i),
                      min_value = 30, max_value = 50, step = 10)
    
    x = x %>%
      layer_dense(units = n_nodes, activation = 'relu') %>%
      layer_dropout(rate = 0.1)
  }
  output = x %>% 
    layer_dense(units = 5, activation = 'softmax')
  
  model = keras_model(inputs = input, outputs = output)
  
  model %>% compile(loss = 'categorical_crossentropy', 
                    optimizer = optimizer_adam(learning_rate = lr),
                    metrics = c(metric_categorical_accuracy()))
  
  return(model)
}
```

```{r model tuning, include = F}
for (i in 1:4){
  
  freqs = colSums(y_train) / nrow(y_train)
  weights = sqrt(1 / freqs)
  class_weights = dict()
  for (k in 0:(length(weights)-1)) {
    class_weights[[k]] = weights[k + 1]
  }
    
  x_train = training_data_list[[i]]
  
  tuner_randomsearch = kerastuneR::RandomSearch(hypermodel = model_builder,
                                                objective = 'val_categorical_accuracy',
                                                max_trials = 75, executions_per_trial = 3,
                                                directory = 'tuning',
                                                project_name = paste('randomsearch results', i),
                                                overwrite = TRUE)
  
  tuner_randomsearch %>% fit_tuner(x = x_train,
                                   y = y_train,
                                   epochs = 100,
                                   batch_size = 32,
                                   class_weight = class_weights,
                                   validation_split = 0.2,
                                   shuffle = TRUE)
}
```


# Results

This section presents the comprehensive outcomes of our neural network approach to avalanche hazard prediction. We begin by examining the performance hierarchy across different predictor sets, followed by detailed analysis of optimal hyperparameter configurations. The section culminates in an evaluation of the final model's predictive capability, with particular attention to class-wise performance across the five avalanche hazard levels.

## Hyperparameter Tuning Outcomes

```{r}
library(dplyr)
library(tidyr)
library(jsonlite)

tuning_results = function(directory, max_layers){
  
  json_path = paste0(directory, '/trial.json')
  tuning_results = fromJSON(txt = json_path)
  hp               = tuning_results$hyperparameters$values
  number_of_layers = hp$number_of_layers
  learning_rate    = hp$learning_rate
  score            = tuning_results$score
  
  layers_nodes     = matrix(rep(NA, max_layers), nrow = 1)
  colnames(layers_nodes) = paste('nodes on layer ', 1:max_layers)
  for (i in 1:number_of_layers) {
    node_name = paste0('nodes_layer_', i)
    layers_nodes[1, i] = hp[[node_name]]
  }
  
  row_df = data.frame( Val_accuracy = score,  LR = learning_rate, 
                       layers = number_of_layers, layers_nodes, 
                       check.names = FALSE)
  return(row_df)
}

# Test
# tuning_results('tuning/randomsearch results 1/trial_19', 5)

directories              = list.dirs('tuning', recursive = FALSE)
randomsearch_search      = 'randomsearch'
randomsearch_directories = grep(randomsearch_search, directories, value = TRUE)

results_compiler = function(method_directory){

  for (i in 1:length(method_directory)){
    trial_directories = list.dirs(method_directory[i])
    results_df = do.call(rbind, lapply(trial_directories[-1], tuning_results, max_layers = 5))
    results_df = arrange(results_df, desc(Val_accuracy))
    save(results_df, file = paste0(method_directory[i], '/summary.RData'))
  }
}

# Test
#results_compiler('tuning/randomsearch results 1')

# the results automatically get saved locally, inside the same tuning folder
results_compiler(randomsearch_directories)
```


```{r}
library(dplyr)
library(keras)
library(tensorflow)
library(reticulate)
library(caret)
library(knitr)
library(kableExtra)

files = c('tuning/randomsearch results 1/summary.RData',
          'tuning/randomsearch results 2/summary.RData',
          'tuning/randomsearch results 3/summary.RData',
          'tuning/randomsearch results 4/summary.RData')
var_names = c('rs_results_1', 'rs_results_2', 'rs_results_3', 'rs_results_4')

for (i in seq_along(files)){
  temp_env = new.env()                       # temporary environment
  load(files[i], envir = temp_env)           # load into temp
  assign(var_names[i], temp_env$results_df)  # assign with custom name
}

results = list(rs_results_1, rs_results_2, rs_results_3, rs_results_4)
df = data.frame()
for (i in 1:4){
  results_df = results[[i]]
  top_3_models = results_df[1:3, ]
  top_3_models = mutate(top_3_models, across(c(Val_accuracy, LR), round, 5))  
  colnames(top_3_models) = c('Validation accuracy',
                             'Learning rate',
                             'Number of layers',
                             paste('nodes on layer ', 1:5)) # make sure to make this the maximum layers
  df = bind_rows(df, top_3_models)
}
df = mutate(df, 'Predictor set' = rep(1:4, each = 3), .before = 'Validation accuracy')

save(df, file = 'tuning/tuning_summary_table.RData')
```

```{r myplot, results='asis', eval=TRUE, fig.cap="Validation accuracy by hyperparameter configuration across four predictor sets"}

#| label: fig-myplot
#| 
load('../tuning/tuning_summary_table.RData')

### The Tuning plot
names(df) <- gsub(" ", "_", names(df))


set.seed(1)  # for reproducibility of shuffle

top_10_shuffled <- df %>%
  group_by(Predictor_set) %>%
  sample_frac(1) %>%  # shuffle rows within each Predictor_set
  ungroup() %>%
  rowwise() %>%
  mutate(
    combo_label = paste0(
      "LR=", Learning_rate,
      ", Layers=", Number_of_layers,
      ", Nodes=[", 
      paste(na.omit(c_across(starts_with("nodes_on_layer_"))), collapse = ", "),
      "]"
    )
  ) %>%
  ungroup()

# Now reorder combo_label factor so predictor sets are grouped but shuffled inside
top_10_shuffled <- top_10_shuffled %>%
  arrange(Predictor_set) %>%  # predictor sets grouped in order
  mutate(combo_label = factor(combo_label, levels = unique(combo_label)))

# Find optimal points per predictor set (same as before)
optimal_points <- top_10_shuffled %>%
  group_by(Predictor_set) %>%
  filter(Validation_accuracy == max(Validation_accuracy)) %>%
  ungroup()

my_colors <- c(
  "1" = "#e41a1c",  # bright red
  "2" = "#377eb8",  # strong blue
  "3" = "#4daf4a",  # vivid green
  "4" = "#ff7f00"   # bright orange
)


# Plot
ggplot(top_10_shuffled, aes(x = combo_label, y = Validation_accuracy, color = factor(Predictor_set))) +
  geom_point(size = 3) +
  geom_point(data = optimal_points, aes(x = combo_label, y = Validation_accuracy),
             color = "black", size = 5, shape = 8) +
  geom_point(size = 3)+
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 70, hjust = 1),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  xlab("Hyperparameter Combination") +
  ylab("Validation Accuracy") +
  scale_color_manual(values = my_colors, name = "Predictor Set")
###############



```

Systematic hyperparameter tuning across four predictor sets revealed distinct performance patterns, visualized in the above figure. The plot shows validation accuracy distributions for various architectural configurations, with Predictor Set 2 (weather conditions) achieving the highest peak performance (~66%), closely followed by Predictor Set 4 (all variables) at ~65%.


@tbl-tuning details the top three configurations for each predictor set, confirming the performance hierarchy. The marginal improvement of Set 2 over Set 4 indicates that weather variables capture the most critical forecasting signals, with topographic and snow-pack data providing limited incremental value.



```{r tuning-table, results='asis', eval=TRUE}
#| label: tbl-tuning
load('../tuning/tuning_summary_table.RData')

summary_table = kable(df, booktabs = TRUE, caption = "Top hyperparameter configurations per predictor set, ranked by validation accuracy.") %>% 
  kable_styling(full_width = FALSE, position = 'center') %>%
  collapse_rows(columns = 1, valign = 'middle')

for (i in seq(3, nrow(df), by = 3)) {
  summary_table = row_spec(summary_table, i, 
                           extra_css = "border-bottom: 2px solid black;")
}

summary_table
```

Architecturally, models with 4–5 layers and 30–50 nodes per layer consistently outperformed others, with no clear gains beyond 5 layers. The optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with \[30, 40, 40, 40\] nodes and achieved 64.9% validation accuracy.

The optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy, representing the best balance of performance and generalizability.

## Optimal Architecture

```{r best_model_plot, fig.cap="Optimal neural network architecture"}

model_builder = function(hp){
  
  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 3, step = 1)
  lr = hp$Choice('learning_rate', values = c(1e-2, 1e-3, 1e-4))  
  
  n_x   = ncol(x_train)
  input = layer_input(shape = c(n_x))
  
  x = input
  for (i in 1:n_layers){
    
    n_nodes = hp$Int(paste0('nodes_layer_', i),
                     min_value = 30, max_value = 50, step = 10)
    
    x = x %>%
      layer_dense(units = n_nodes, activation = 'relu') %>%
      layer_dropout(rate = 0.1)
  }
  output = x %>% 
    layer_dense(units = 5, activation = 'softmax')
  
  model = keras_model(inputs = input, outputs = output)
  
  model %>% compile(loss = 'categorical_crossentropy', 
                    optimizer = optimizer_adam(learning_rate = lr),
                    metrics = c(metric_categorical_accuracy()))
  
  return(model)
}

tuner = kerastuneR::RandomSearch(hypermodel = model_builder, 
                                 objective = 'val_categorical_accuracy',
                                 max_trials = 1, 
                                 executions_per_trial = 1,
                                 directory = 'tuning',
                                 project_name = 'randomsearch results 4')

tuner$reload()
best_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] 

tuner$reload()
best_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] 

tf = tensorflow::tf
plot_model = tf$keras$utils$plot_model
plot_model(best_model, show_shapes = TRUE, show_layer_names = TRUE,
           expand_nested = FALSE,
           show_layer_activations = TRUE,
           dpi = 500,
           to_file = 'best_model_plot.png')
```

<img src="best_model_plot.png" alt="Configuration of the best model" width="500"/>

The selected architecture utilizes ReLU activation in hidden layers and softmax output activation, appropriate for the multi-class ordinal nature of avalanche hazard prediction. This configuration represents an optimal balance between model complexity and predictive performance for the comprehensive feature set. This architecture balanced model complexity with predictive accuracy and was used for final evaluation on the test set.

## Model Evaluation 

### Confusion Matrix


```{r predictions}


model_builder = function(hp){
  
  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 3, step = 1)
  lr = hp$Choice('learning_rate', values = c(1e-2, 1e-3, 1e-4))  
  
  n_x   = ncol(x_train)
  input = layer_input(shape = c(n_x))
  
  x = input
  for (i in 1:n_layers){
    
    n_nodes = hp$Int(paste0('nodes_layer_', i),
                     min_value = 30, max_value = 50, step = 10)
    
    x = x %>%
      layer_dense(units = n_nodes, activation = 'relu') %>%
      layer_dropout(rate = 0.1)
  }
  output = x %>% 
    layer_dense(units = 5, activation = 'softmax')
  
  model = keras_model(inputs = input, outputs = output)
  
  model %>% compile(loss = 'categorical_crossentropy', 
                    optimizer = optimizer_adam(learning_rate = lr),
                    metrics = c(metric_categorical_accuracy()))
  
  return(model)
}

tuner = kerastuneR::RandomSearch(hypermodel = model_builder, 
                                 objective = 'val_categorical_accuracy',
                                 max_trials = 1, 
                                 executions_per_trial = 1,
                                 directory = 'tuning',
                                 project_name = 'randomsearch results 4')

tuner$reload()
best_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] 

results      = best_model %>% evaluate(x_test, y_test)
y_pred_probs = best_model %>% predict(x_test)
y_pred       = max.col(y_pred_probs) - 1
true_classes = apply(y_test, 1, which.max) - 1 # it was one-hot encoded so i changed it back
metrics_list = confusionMatrix(factor(y_pred), factor(true_classes))

con_mat = as.data.frame(metrics_list$table)
con_mat = rename(con_mat, Predicted = Prediction, Reference = Reference, Count = Freq)
con_mat =  tidyr::pivot_wider(con_mat, names_from = Reference, values_from = Count,
                              values_fill = 0)
save(con_mat, file = 'test/con_mat.RData')
save(metrics_mat, file = 'test/metrics_mat.RData')
```

```{r confusion, results='asis', eval=TRUE}
#| label: tbl-confusion
load('../test/con_mat.RData')
kable(con_mat, format = 'html', caption = "Confusion matrix for avalanche hazard predictions.") %>%
  kable_styling(full_width = FALSE, position = 'center') %>%
  add_header_above(c(' ' = 1, 'Predicted' = 5))
```

The confusion matrix of the the fitted model is reported in @tbl-confusion. It is clear that category 0 is the best estimated while categories 3 and 4 are estimated the worst. This result was expected since category 3 and 4 are very underepresented in both the training and test data. Concrete metrics of the model performance is given in the table below.

### Class-Level Metrics

```{r metrics matrix, results='asis', eval=TRUE}
#| label: tbl-class

load('../test/metrics_mat.RData')
kable(metrics_mat, digits = 3, caption = "Comprehensive classification metrics by avalanche hazard class.") %>%
  kable_styling(full_width = FALSE, position = 'center')
```

Sensitivity gives the percent of the time the model predicted an observation as belonging to a category and it actually belonging to that category. The sensitivity for class 0 is the best with a value of 0,854. There is a steep drop off for the other classes but class 2 is the second highest with a value of 0,557 and the rest are all below 0.5 meaning that more often than not, the model is unable to identify the correct category. An analogy for sensitivity is a test with high sensitivity(close to the maximum of 1) will identify most of the patients with with the flu as having the flu but this may mean that lots of patients without the flu also get identified as having the flu. An extreme case may be if the model predicted all observations as belonging to category 0. Then the sensitivity would be 1,meaning that all observations that belong to category 0 are predicted to be category 0. So there need to be a balance because we do not want this

Specificity gives the percent of time the an observation does not belong to a specific category and the model predicts it as not belonging to that category. High specificity(close to the maximum of 1) is analogous to a test rarely every flagging someone as having the flu when they don't. A perfect model will have high sensitivity with a high specificity, meaning that it is able to identify when observations belong to a category and does not incorrectly predict other observations as belonging to that category. The table above indicates that all the categories have relatively high specificity. Class 4 has a specificity of 0.996 which on the surface looks great but because the sensitivity is so low, this high specificity just means that the model rarely every predicts any observations as belonging to category 4. The same follows for category 3. This is also seen in the extremely low detection rate and prevalence for these categories. These results, specifically for categories 3 and 4 are an indication that perhaps just reweighting the classes was not enough to overcome the imbalance in the data.

A better metric for instances such as this where the data is imbalanced is the F1 score. The F1 score is the harmonic mean of the precision and recall. The precision is the percentage of observations that belonged to a category and were correctly predicted as belonging to that category. The recall is just the sensitivity. The F1 score, same as the other metrics, ranges from 0 to 1 with 0 being the worst and 1 being the best. The F1 score is useful because the harmonic mean because it is less affected by extreme values than the arithmetic mean. Categories 4 and 3 are by far the worst with values of 0,104 and 0,119. Category 0 has the best F1 score with a value of 0,716. Then category 1 has a value of 0,383 and category 3 has a score of 0,487.


All these values, except for category are quite poor and indicate lack of predictive power. This is reflected in the fact that the model achieved an accuracy of 51,7% on the test set. The model is better than blindly guessing which we expect to return an accuracy of 20% but the model lacks predictive power.



## Interpretation

The results highlight the challenges of severe class imbalance. While class reweighting improved performance somewhat, minority classes remained poorly predicted. This suggests that more aggressive strategies may be necessary, such as resampling (e.g., bootstrapping or SMOTE), expanding the hyperparameter search space, or exploring alternative architectures.

Despite these limitations, the study provides useful insights. It demonstrates that weather variables are the dominant predictors of avalanche hazard and that neural networks can achieve moderate accuracy with relatively simple architectures. However, improving detection of rare but high-consequence hazard categories remains an important direction for future work.


## Extra resources

[`keras`](https://cran.r-project.org/web/packages/keras/vignettes/){target="_blank"}

[`kerastuneR`](https://eagerai.github.io/kerastuneR/){target="_blank"}

## 5. Discussion and Conclusion {#sec-Conclusion}

### 5.1 Discussion

From the results, it can be seen that the neural network are capable and accurate for predicting avalanche hazard levels. They learn and find meaningful patterns and interaction through the diverse predictor sets of meteorological, snowpack and topographical features. 

However, several limitations could be observed. Classification accuracy was uneven across hazard categories. The model was accurate at predicting low and moderate hazard levels however misclassification rates increased for higher hazards. This issue occurs due to class imbalance and reduced training signal due to the scarsity of high level hazards in the dataset. It is therefore suggested that resampling strategies could be implemented to improve the model classification accuracy. Another important limitation is the interpretability of neural networks. Neural networks are treated as black boxes and therefore interpretability is limited for vital decision making.

The results from this investigation are positive for neural networks. This method is however complex and computationally expensive. It is therefore vital that there is collaboration between data scientists and experts to integrate the model to complement existing methods rather than replace them all together.

### 5.2 Conclusion

This project showcased the vital application of data-driven methods, such as neural networks, to predict avalanche hazard levels. The neural network model developed was successful at integrating all predictor features, such as meteorological, snowpack and topographical features, to find patterns and interactions that can be used to predict avalanche hazard levels. Challenges such as class imbalance, limited performance with high hazard levels and interpretability were observed but dispite these challenges, the model was still proven to be highly accurate.

## References {#sec-References}

\[1\] Bellaire, S. and Jamieson, J.B. (2013) 'Forecasting avalanche danger: recent advances', *Cold Regions Science and Technology*, 85, pp. 89-102.

\[2\] Cervinski, M.A., Bietenbeck, A., Katayev, A., Loh, T.P., van Rossum, H.H. and Badrick, T. (2023) 'Advances in clinical chemistry patient-based real-time quality control (PBRTQC)', in Makowski, G.S. (ed.) *Advances in Clinical Chemistry*. 1st ed. Amsterdam: Elsevier, pp. 223-261. doi: 10.1016/bs.acc.2023.08.003.

\[3\] Fierz, C., Armstrong, R.L., Durand, Y., Etchevers, P., Greene, E., McClung, D.M., Nishimura, K., Satyawali, P.K. and Sokratov, S.A. (2009) *The international classification for seasonal snow on the ground*. IHP-VII Technical Documents in Hydrology No. 83, IACS Contribution No. 1. Paris: UNESCO-IHP.

\[4\] Hafner, E., Bühler, Y. and Schweizer, J. (2021) 'Machine learning in avalanche research: random forests and SVMs for avalanche forecasting', *Natural Hazards and Earth System Sciences*, 21, pp. 223-239.

\[5\] Kenward, M.G. and Carpenter, J. (2007) 'Multiple imputation: current perspectives', *Statistical Methods in Medical Research*, 16(3), pp. 199-218. doi: 10.1177/0962280206075304.

\[6\] King, S. (2024) 'Deadliest, most intense, windiest: Three of the UK's worst storms', *BBC Weather*, 17 October. Available at: https://www.bbc.com/weather/articles/cvglvnyxpx0o (Accessed: 28 September 2025).

\[7\] LaChapelle, E. (1980) 'The fundamental processes in conventional avalanche forecasting', *Journal of Glaciology*, 26(94), pp. 75-84.

\[8\] McClung, D. (2002) 'The elements of applied avalanche forecasting: the human issues', *Natural Hazards*, 26, pp. 111-129.

\[9\] Mitterer, C. et al. (2016) 'Deep learning approaches for snowpack characterization', *Proceedings of the International Snow Science Workshop*.

\[10\] Molenberghs, G. and Kenward, M.G. (2006) *Missing data in clinical studies*. Chichester: Wiley Statistics in Practice.

\[11\] Morning, H. (no date) 'Wind speeds in the mountains', *Mountaineering Scotland*. Available at: https://www.mountaineering.scot/safety-and-skills/essential-skills/weather-conditions/wind-speeds (Accessed: 28 September 2025).

\[12\] SAIS (2014) *Interpreting snow profiles*. \[pdf\] Scottish Avalanche Information Service. Available at: https://www.sais.gov.uk/wp-content/uploads/2014/11/interpreting_snow_profiles.pdf (Accessed: 28 September 2025).

\[13\] Schweizer, J. and Jamieson, J.B. (2007) 'A review of stability tests for snow profiles', *Cold Regions Science and Technology*, 47, pp. 14-37.

\[14\] ScienceDirect (no date) 'Winsorization – an overview', *ScienceDirect Topics*. Available at: https://www.sciencedirect.com/topics/mathematics/winsorization (Accessed: 27 September 2025).

\[15\] Ward, R.G.W., Langmuir, E.D.G. and Beattie, B. (1985) 'Snow profiles and avalanche activity in the Cairngorm Mountains, Scotland', *Journal of Glaciology*, 31(107), pp. 18-27. doi: 10.1177/S0022143000004949.