---
title: ""
format: html
execute:
  echo: true
  warning: false
  message: false
---

```{r, echo = F}
library(kableExtra)
```

## 2. Data and Methods {#sec-Data_and_Methods}

The dataset used in this study consists of daily avalanche forecasts for six forecasting regions across Scotland (Creag Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon), collected by the Scottish Avalanche Information Service (SAIS) between 2009 and 2025. It contains 10,671 observations and 34 variables including weather conditions, topography, snowpack properties, and avalanche hazard levels.

The target variable for this project is the Forecast Avalanche Hazard (FAH), a categorical variable with five ordered levels: Low, Moderate, Considerable -, Considerable +, and High.

Data Cleaning and Preprocessing was done to remove observations that did not make logical sense, transform the data where needed and remove necessary variable. Furthermore, Exploratory Data Analysis (EDA) was then performed to assess variable distributions and perform imputation, outlier handling and scaling of data.

### 2.1 Data Cleaning and Preprocessing

The raw dataset underwent extensive cleaning prior to analysis. Initially, rows with missing FAH values (109 rows) were removed, and FAH levels were replaced with a numerical representation from 0 to 4 for compatibility with the neural network software. Other categorical variables, such as precipitation type and forecast region, were similarly transformed into numerical formats. The Date column was transformed into Month to capture seasonal effects.

Variables unrelated to forecasting or with high missingness (more than 20%), frequent errors, limited predictive value, or poorly defined measures were excluded, resulting in a set of well-defined predictors most relevant for forecasting.

Range checks were applied to remove physically impossible or implausible values based physical limits and regional context.

For example, altitude was limited to 0–1,345 m, the height of Ben Nevis (*...*), and wind speeds to 176 mph, reflecting the highest recorded gust in the Highlands (*...*).

Slope inclines were restricted to 0–90°, and wind directions were constrained to 0–360°. Cloud cover was limited to 0–100%, total snow depth to 0–1,500 cm (with unrealistic spikes removed), and foot penetration depth to 100 cm. Snow temperatures above 0.5 °C were flagged as implausible, since snow begins melting at 0 °C, and maximum temperature gradients were limited to below 25 °C per 10 cm (*...*). Observations where foot penetration exceeded total snow depth were also removed.

Circular variables such as wind direction and aspect were transformed into sine and cosine components.

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(corrplot)
library(lubridate)
library(gridExtra)
library(VIM)
library(mice)
library(caret)
library(ggplot2)
```

```{r, echo=FALSE}
data <- read.csv("scotland_avalanche_forecasts_2009_2025.csv", header = TRUE)
```

```{r, echo=FALSE}
# Only keep rows where the target variable FAH is not missing (-109 rows)
data <- data %>% filter(!is.na(FAH) & FAH != "")

# Convert categorical target variable to numeric 
data$FAH <- as.integer(factor(x = data$FAH, levels = c("Low", "Moderate", "Considerable -", "Considerable +", "High"))) - 1
data$Precip.Code <- as.integer(factor(data$Precip.Code)) - 1
data$Area <- as.integer(factor(data$Area)) - 1   # 6 areas

# Create a year variable for date & Month
# Don't have any data for July, Aug and Sept (summer months) 
# Don't know if a month variable will be important? Should i make those months values be 0?
data <- data %>% mutate(Year = year(Date), Month = month(Date))
table(data$Month)

# Remove unnecessary columns
data <- data %>% select(-OSgrid, -OAH, -Obs, -Location, -Date)
```

```{r, echo=FALSE}
missing_summary <- data %>%
  summarise_all(~sum(is.na(.))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percent = round(Missing_Count / nrow(data) * 100, 1)) %>%
  arrange(desc(Missing_Count))
missing_summary
```

```{r, echo=FALSE}
data_cleaned <- data %>%
  mutate(
    Alt = ifelse(Alt < 0 | Alt > 1345, NA_real_, Alt),
    Aspect = ifelse(Aspect < 0 | Aspect > 360, NA_real_, Aspect),
    Incline = ifelse(Incline < 0 | Incline > 90, NA_real_, Incline),
    Wind.Speed = ifelse(Wind.Speed < 0 | Wind.Speed > 176, NA_real_, Wind.Speed),
    Cloud = ifelse(Cloud < 0 | Cloud > 100, NA_real_, Cloud),
    Total.Snow.Depth = ifelse(Total.Snow.Depth < 0 | Total.Snow.Depth > 1500, NA_real_, Total.Snow.Depth),
    Summit.Wind.Speed = ifelse(Summit.Wind.Speed < 0 | Summit.Wind.Speed > 176, NA_real_, Summit.Wind.Speed),
    Foot.Pen = ifelse(Foot.Pen < 0 | Foot.Pen > 100, NA_real_, Foot.Pen),
    Wind.Dir = ifelse(Wind.Dir < 0 | Wind.Dir > 360, NA_real_, Wind.Dir),
    Summit.Wind.Dir = ifelse(Summit.Wind.Dir < 0 | Summit.Wind.Dir > 360, NA_real_, Summit.Wind.Dir),
    Snow.Temp = ifelse(Snow.Temp > 0.5, NA_real_, Snow.Temp),
    Max.Temp.Grad = ifelse(Max.Temp.Grad < 0 | Max.Temp.Grad > 2.5, NA_real_, Max.Temp.Grad))
```

```{r, echo=FALSE}
# Convert circular variables into two features using sine and cosine transformations
data_cleaned <- data_cleaned %>% 
  mutate(Aspect_sin = sin(Aspect * pi / 180), Aspect_cos = cos(Aspect * pi / 180)) %>% 
  mutate(WindDir_sin = sin(Wind.Dir * pi / 180), WindDir_cos = cos(Wind.Dir * pi / 180)) %>%
  mutate(SummitWindDir_sin = sin(Summit.Wind.Dir * pi / 180), SummitWindDir_cos = cos(Summit.Wind.Dir * pi / 180)) %>%
  select(-Wind.Dir, -Aspect, -Summit.Wind.Dir)

```

```{r, echo=FALSE}
# Remove variables with high percentage of missing values or  those that don't make sense

data_cleaned <- data_cleaned %>%
  select(-Ski.Pen, -AV.Cat, -Crystals, -Wetness, -Year, -Snow.Index, -Insolation, -No.Settle)
#names(data_cleaned)

#dim(data_cleaned)
```

```{r, echo=FALSE}
data_cleaned <- data_cleaned %>%
  filter(is.na(Foot.Pen) | is.na(Total.Snow.Depth) | Foot.Pen <= Total.Snow.Depth)
```

```{r, echo=FALSE}
missing_summary <- data_cleaned %>%
  summarise_all(~sum(is.na(.))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percent = round(Missing_Count / nrow(data_cleaned) * 100, 1)) %>%
  arrange(desc(Missing_Count))
missing_summary

# dim(data_cleaned)
```

### 2.2 Predictor Sets

```{r, echo=FALSE}

pred_1 <- c("Area", "longitude", "latitude", "Alt", "Incline", "Aspect_sin", "Aspect_cos")

pred_2 <- c("Air.Temp", "Wind.Speed", "Cloud", "Precip.Code", "Drift",
           "Summit.Air.Temp", "Summit.Wind.Speed", "WindDir_sin", "WindDir_cos", 
           "SummitWindDir_sin", "SummitWindDir_cos")

pred_3 <- c("Total.Snow.Depth", "Foot.Pen", "Rain.at.900", "Max.Temp.Grad", 
           "Max.Hardness.Grad", "Snow.Temp")

other_vars <- c("Month")

all_pred <- c(pred_1, pred_2, pred_3, other_vars)
```

The data was split into three predictor sets; pred1, pred2, and pred3, where each predictor set represents different types of information relating to snow and avalanche conditions. Furthermore, predictor set 1 represents the location data such as the area, exact location coordinates, and how steep the slope is etc. Predictor set 2 represents the air conditions such as air temperature, cloud cover, and wind speed etc. Lastly, predictors 3 represents the snow conditions such as how deep the snow is, the temperature of the snow at different layers and foot penetration. All three predictor sets essentially contribute to determining how the events have affected the snowfall. In addition, the month variable does not fit in any predictor set, but rather adds seasonal context since snow behaviour differs throughout the year. The predictor sets were separated into the following sets:

Predictor Set 1: Area, longitude, latitude, Alt, Incline, Aspect_sin, and Aspect_cos

Predictor Set 2: Air.Temp, Wind.Speed, Cloud, Precip.Code, Drift, Summit.Air.Temp, Summit.Wind.Speed, WindDir_sin, WindDir_cos, SummitWindDir_sin, and SummitWindDir_cos

Predictor Set 3: Total.Snow.Depth, Foot.Pen, Rain.at.900, Max.Temp.Grad, Max.Hardness.Grad, and Snow.Temp

Other Variables: Month

### 2.3 Imputation

Multiple Imputation by Chained Equations (MICE) handles missing data that creates multiple versions of a dataset rather than selecting the first best guessed value. The MICE function ran under the assumption of Missing at Random (MAR), which means that it was assumed that the probability of a value being missing depends on the observed values in the dataset, but are independent of the unobserved values (Kenward & Carpenter, 2007).

```{r, echo=FALSE}
# No variables have more than 50% missing data so impute all missing values.

md.pattern(data_cleaned[,c("FAH", all_pred[1:min(10, length(all_pred))])], rotate.names = TRUE)

```

The figure above shows the pattern of missing values for a subset of 10 variables, where blue represents no missing values and red represents missing values present. It can be seen that the sum of missing values for the 10 variables was 774 values, and it can be seen that aspect_sin and aspect_cos have the highest number of missing values (320 values) among these 10 variables. Furthermore, wind.speed, incline, air.temp and cloud have the next highest missing values of 59, 27, 21 and 19 respectively.

In addition, it was seen that variables like area, latitude, longitude and alt have no missing values, which shows that they are reliable predictors for the missing values in other varaiables.

```{r, echo=FALSE}

mice_data <- data_cleaned %>%
  select(FAH, all_of(all_pred))

set.seed(12345)

mice_output <- mice(mice_data, m = 5, method = 'pmm', printFlag = FALSE)

data_imputed <- complete(mice_output, 1)


```

MICE was run with five iterations creating five slightly different datasets and Predictive Mean Matching (PMM) was used, which uses the information in the dataset to predict what the missing values would most likely be. In addition, all variables that were in the cleaned dataset which contained missing values were selected for imputation, as the highest percentage of missing data was 14.2%, so there were enough observed variables to predict the unobserved variables well.

In addition, after running the MICE imputation, the density plots were analysed to determine if the variables were imputed well or not, with imputed values represented by the red line and observed values represented by the blue line. Imputed values should not change the distribution of a variable (Molenberghs & Kenward, 2006), thus, if the imputed variables shows a different distribution to the observed variables, the variable was removed from the dataset.

```{r, echo=FALSE}

# Diagnostic Plots
# Strip plot - shows distribution of imputed values

stripplot(mice_output, pch=20, cex=1.2)

```

The figure above displays a stripplot from the MICE imputation, which shows the distribution across all variables and the five different imputation sets of observed values represented by blue and the imputed values represented by red. It can be seen that the red and blue dots overlap well and follow similar distributions in multiple variables which shows that the imputation worked well.

However, variables such as Alt, Air.Temp, and Incline show that imputed values do not follow the same distribution as the observed values as the red dots do not overlap well and indicates that the data failed to impute well under the MAR assumption.

The density plots for the variables in question were plotted as well as the density plot of Max.Hardness.Grad which imputed well as a comparison,

```{r, echo=FALSE}

# densityplot(mice_output, ~ Alt) # bad
# densityplot(mice_output, ~ Aspect_sin) # maybe
# densityplot(mice_output, ~ Aspect_cos) 
# densityplot(mice_output, ~ Incline) # bad

# densityplot(mice_output, ~ Air.Temp) # bad
 # densityplot(mice_output, ~ WindDir_sin) # maybe
# densityplot(mice_output, ~ WindDir_cos) # maybe
# densityplot(mice_output, ~ Wind.Speed)
# densityplot(mice_output, ~ Cloud) # maybe

# densityplot(mice_output, ~ Summit.Air.Temp)
# densityplot(mice_output, ~ SummitWindDir_sin)
# densityplot(mice_output, ~ SummitWindDir_cos)
# densityplot(mice_output, ~ Summit.Wind.Speed)

# densityplot(mice_output, ~ Max.Temp.Grad) # maybe
# densityplot(mice_output, ~ Max.Hardness.Grad) 
# densityplot(mice_output, ~ Snow.Temp) # maybe


p1 <- densityplot(mice_output, ~ Alt, main = "Alt")
p2 <- densityplot(mice_output, ~ Incline, main = "Incline")
p3 <- densityplot(mice_output, ~ Air.Temp, main = "Air.Temp")
p4 <- densityplot(mice_output, ~ Max.Hardness.Grad, main = "Max.Hardness.Grad")

grid.arrange(p1, p2, p3, p4, ncol = 2)

```

It can be seen from the figure above that the imputed values in red do not follow the observed values in blue for Alt, Incline, and Air.Temp. However, comparing it to a well imputed variable like Max.Hardness.Grad, it can be seen that the imputed values follows the observed values distribution closely. This exercise was done for each of the imputed variable, however, just the variables that did not impute well and one variable that did impute well was shown in the final report.

```{r, echo=FALSE}

# Convergence plot

# plot(mice_output, main = "MICE Convergence - All Variables")

```

```{r, echo=FALSE}

bad_vars <- c("Alt", "Air.Temp", "Incline")

mice_data <- mice_data %>%
  select(FAH, all_of(setdiff(all_pred, bad_vars)))

# dim(mice_data)

set.seed(12345)

mice_output2 <- mice(mice_data, m = 5, method = 'pmm', printFlag = FALSE)
data_imputed2 <- complete(mice_output2, 1)
```

The variable that did not impute well was removed from the dataset, and the new dataset was refitted using MICE which ensured that the imputations better satisfied the MAR assumption and produced reliable results.

### 2.4 Outlier Handling

Outlier Capping or Winsorization is a method that handles outliers by converting the extreme high values to the value of the highest data point that is not considered an outlier (Cervinski *et al.*, 2023). This method was used on the imputed dataset to account for any values that were extreme to ensure that the distribution is not skewed.

```{r, echo=FALSE}

# Handle outliers by capping them

for(var in names(data_imputed2)) {
  if(is.numeric(data_imputed2[[var]])) {
    
    x <- data_imputed2[[var]]
    
    lower_bound <- quantile(x, 0.01, na.rm = TRUE)
    upper_bound <- quantile(x, 0.99, na.rm = TRUE)
    
    n_outliers <- sum(x < lower_bound | x > upper_bound, na.rm = TRUE)
    data_imputed2[[var]][x < lower_bound] <- lower_bound
    data_imputed2[[var]][x > upper_bound] <- upper_bound
    
    if(n_outliers > 0) {
      print(paste(var, "- Outliers capped:", n_outliers))
    }
  }
}
```

For each variable in the dataset, the 1st and 99th percentile was calculated to address the extreme values, preserving 98% of the data. If a value fell below the 1st percentile, it was capped to the 1st percentile value, furthermore, if a value fell above the 99th percentile, it was capped to the 99th percentile value.

In addition, outlier handling was intentionally done after imputation, in the event that MICE generated a value that fell outside of the realistic range of values in the original dataset, considering that data cleaning had already taken place.

Furthermore, it was seen that the need to cap outliers was needed as multiple variables had around \~1% of its data capped, which shows that the data contained extreme values either in the original dataset or from imputation.

### 2.5 Correlation Analysis

A correlation analysis was performed to examine the patterns in the variables and assess potential multicollinearity effects.

```{r, echo=FALSE}

# Use imputed data (no missing values)

corr_data <- data_imputed2  

corr_matrix <- cor(corr_data, use = "complete.obs")

# Correlation with FAH
# fah_corr <- corr_matrix[,"FAH"] %>%
#   as.data.frame() %>%
#   rownames_to_column("Variable") %>%
#   rename(Correlation = ".") %>%
#   filter(Variable != "FAH") %>%
#   arrange(desc(abs(Correlation)))

# Top 10 correlations with FAH
# print(head(fah_corr, 10))

# Correlation heatmap
corrplot(corr_matrix, method = "color", type = "upper", 
         order = "hclust", tl.cex = 0.7, tl.col = "black")

```

It can be seen from the figure above that correlation between variables exist as the dark blue squares represent strong positive correlations and the dark red squares represent strong negative correlations. The variables from predictor set 1 are represented by dark blue squares, indicating strong positive correlations between the variables. Moreover, FAH has moderate correlations with the variables as the colour range are in the lighter section of the colour scale.

Furthermore, it is important to note that the white squares represent independent variables (correlation of 0), and it can be seen that there are multiple variables such as Aspect_cos and Area have white squares which indicates that multicollinearity is not excessive.

Lastly, it can be seen that variables with similar correlation patterns have grouped together creating visible blocks of related variables, which confirms the predictor set groupings capture different types of information.

```{r, echo=FALSE}

# # FAH Correlation
# 
# data_imputed2$FAH_factor <- factor(data_imputed2$FAH,
#                            levels = 0:4,
#                            labels = c("Low", "Moderate", "Considerable -", 
#                                      "Considerable +", "High"))
# 
# # Predictor 1
# 
# predictor_1 <- pred_1[pred_1 %in% names(data_imputed2)]
# 
# data1 <- data_imputed2 %>%
#   select(FAH_factor, FAH, all_of(predictor_1))
# 
# corr1 <- cor(data1[,predictor_1], data1$FAH) %>%
#   as.data.frame() %>%
#   rownames_to_column("Variable") %>%
#   rename(FAH_Correlation = "V1") %>%
#   arrange(desc(abs(FAH_Correlation)))
# 
# # Create boxplots
# plot1 <- list()
# for(i in 1:min(4, length(predictor_1))) {
#   var <- predictor_1[i]
#   plot1[[i]] <- ggplot(data1, aes(x = factor(FAH_factor), y = .data[[var]])) +
#     geom_boxplot(fill = "darkblue", alpha = 0.7) +
#     labs(title = paste(var, "vs FAH"),
#          x = "FAH", y = var) +
#     theme_minimal()
# }
# 
# do.call(grid.arrange, c(plot1, ncol = 2))
# 
# # Predictor 2
# 
# predictor_2 <- pred_2[pred_2 %in% names(data_imputed2)]
# 
# data2 <- data_imputed2 %>%
#   select(FAH_factor, FAH, all_of(predictor_2))
# 
# corr2 <- cor(data2[,predictor_2], data2$FAH) %>%
#   as.data.frame() %>%
#   rownames_to_column("Variable") %>%
#   rename(FAH_Correlation = "V1") %>%
#   arrange(desc(abs(FAH_Correlation)))
# 
# plot2 <- list()
# for(i in 1:min(4, length(predictor_2))) {
#   var <- predictor_2[i]
#   plot2[[i]] <- ggplot(data2, aes(x = factor(FAH_factor), y = .data[[var]])) +
#     geom_boxplot(fill = "darkgreen", alpha = 0.7) +
#     labs(title = paste(var, "vs FAH"),
#          x = "FAH", y = var) +
#     theme_minimal()
# }
# 
# do.call(grid.arrange, c(plot2, ncol = 2))
# 
# # Predictor 3
# 
# predictor_3 <- pred_3[pred_3 %in% names(data_imputed2)]
# 
# data3 <- data_imputed2 %>%
#   select(FAH_factor, FAH, all_of(predictor_3))
# 
# corr3 <- cor(data3[,predictor_3], data3$FAH) %>%
#   as.data.frame() %>%
#   rownames_to_column("Variable") %>%
#   rename(FAH_Correlation = "V1") %>%
#   arrange(desc(abs(FAH_Correlation)))
# 
# plot3 <- list()
# for(i in 1:min(4, length(predictor_3))) {
#   var <- predictor_3[i]
#   plot3[[i]] <- ggplot(data3, aes(x = factor(FAH_factor), y = .data[[var]])) +
#     geom_boxplot(fill = "darkred", alpha = 0.7) +
#     labs(title = paste(var, "vs FAH"),
#          x = "FAH", y = var) +
#     theme_minimal()
# }
# 
# do.call(grid.arrange, c(plot3, ncol = 2))
```

### 2.6 Data Scaling

Scaling was performed to ensure that variables with different units and scales contributed equally to the model and variables with larger numeric values did not dominate the model.

```{r, echo=FALSE}

set.seed(12345)

train_index <- createDataPartition(data_imputed2$FAH, 
                                   p = 0.7, 
                                   list = FALSE)

train_data <- data_imputed2[train_index, ]
test_data <- data_imputed2[-train_index, ]

# Standardisation

numeric_vars <- names(data_imputed2)[sapply(data_imputed2, is.numeric)]
numeric_vars <- setdiff(numeric_vars, "FAH")

preproc_obj <- preProcess(
  train_data %>% select(all_of(numeric_vars)),
  method = c("center", "scale")  
)

train_scaled <- predict(preproc_obj, train_data)
test_scaled <- predict(preproc_obj, test_data)

predictor_set_1_train = select(train_scaled, longitude:Aspect_cos)
predictor_set_2_train = select(train_scaled, Wind.Speed:Rain.at.900)
predictor_set_3_train = select(train_scaled, Max.Temp.Grad:Snow.Temp)
predictor_set_4_train = select(train_scaled, c(-FAH, -FAH_factor))
training_data = list(predictor_set_1_train, predictor_set_2_train, 
                     predictor_set_3_train, predictor_set_4_train)
# save(training_data, file = 'data/training_data.RData')

predictor_set_1_test = select(test_scaled, longitude:Aspect_cos)
predictor_set_2_test = select(test_scaled, Wind.Speed:Rain.at.900)
predictor_set_3_test = select(test_scaled, Max.Temp.Grad:Snow.Temp)
predictor_set_4_test = select(test_scaled, c(-FAH, -FAH_factor))
testing_data = list(predictor_set_1_test, predictor_set_2_test, 
                    predictor_set_3_test, predictor_set_4_test)
# save(testing_data, file = 'data/testing_data.RData')

y_train = train_scaled$FAH
# save(y_train, file = 'data/y_train.RData')
y_test  = test_scaled$FAH
# save(y_test, file = 'data/y_test.RData')

```

The data was split using a 70/30 split, which was stratified using FAH to ensure that both the training and testing sets follow a similar distribution of the target variable, FAH. Thereafter, the numeric values from the training dataset were used to calculate the mean and standard deviation of each variable using the preProcess() function and was then applied to both the training and test sets using the predict() function. Furthermore, this approach prevents data leakage which occurs when information from the test set goes into the training set.
