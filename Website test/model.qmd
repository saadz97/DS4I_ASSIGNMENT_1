---
title: "Model"
---

## Getting set up

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = FALSE, message = FALSE)
```

A neural network was fitted to the data in R, specifically R version 4.5.1 (2025-06-13) and python version 3.11.6. The reason it is important to specify the versions is because keras and its dependencies in R are wrappers of the python libraries so ensuring cross compatibility is imperative. 

```{r packages and libraries, eval=TRUE}
library(keras)
library(kerastuneR)
library(tensorflow)
library(dplyr)
library(tidyr)
library(reticulate)
library(caret)
library(knitr)
library(kableExtra)
```

The packages that will be used in the model building and tuning are given above. The packages can be installed using **`install.packages('package', dependencies = T)`**. Note that after reading in the **`keras`** library a warning might appear saying that it is deprecated and that you should use **`keras3`**, this can be ignored for now as the provided code does not work when using **`keras3`**.


```{r data}
# ensure that inside the folder you have the project in you also have a folder
# called data that contains the data. 
data = read.csv('./data/scotland_avalanche_forecasts_2009_2025.csv')

# this is a very general solution to the problem of missing entries
# maybe imputation?
data = drop_na(data)
data = filter(data, FAH != '', OAH != '', Precip.Code != '')

## data description ##

# Date = the date that the forecast was made
# Area = one of six forecasting region
# FAH = the forecast avalanche hazard for the following da
# OAH = the observed avalanche hazard on the following day (observation made the following day)
# longitude:Incline: position and topography at forecast location (predictor set)
# Air.Temp:Summit.Wind.Speed = weather in the vicinity of the forecast location at the
# time the forecast was made (predictor set 2)
# Max.Temp.Grad:Snow.Temp = results of a ”snow pack test” of the integrity of snow at
# the forecast location (predictor set 3)

#unique(data['Area'])
#unique(data['Obs'])
#unique(data['FAH'])
#unique(data['OAH'])
#unique(data['Precip.Code'])


data$FAH         = as.integer(factor(x = data$FAH)) - 1
data$OAH         = as.integer(factor(x = data$OAH)) - 1
data$Precip.Code = as.integer(factor(data$Precip.Code)) - 1

predictor_set_1 = select(data, c('longitude':'Incline'))
predictor_set_1 = mutate(predictor_set_1, across(c(longitude : Incline), scale))
predictor_set_1 = as.matrix(predictor_set_1)

predictor_set_2 = select(data, c('Air.Temp':'Summit.Wind.Speed', 'FAH'))
predictor_set_2 = mutate(predictor_set_2,
                         across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))
predictor_set_2 = as.matrix(predictor_set_2)

predictor_set_3 = select(data, c('Max.Temp.Grad':'Snow.Temp', 'FAH'))
predictor_set_3 = mutate(predictor_set_3, across(c(Max.Temp.Grad : Snow.Temp), scale))
predictor_set_3 = as.matrix(predictor_set_3)
```

Since we are trying to predict the forecasted avalanche hazard(FAH), it is important that we do not allow the model to be trained on the observed avalanche hazard(OAH). Therefore the predictor sets are explicitly defined so that there is no accidental data leakage. The FAH, OAH and Precip.Code variables are converted to integers so that the data is compatible with **`keras`**. 

```{r traning and test split}
set.seed(2025)
training_indices = sample(1:nrow(data), floor(nrow(data) * 0.7), replace = F)

data$FAH         = as.integer(factor(x = data$FAH)) - 1
data$OAH         = as.integer(factor(x = data$OAH)) - 1
data$Precip.Code = as.integer(factor(data$Precip.Code)) - 1

predictor_set_1       = select(data, c('longitude':'Incline'))
predictor_set_1_train = mutate(predictor_set_1[training_indices, ], 
                               across(c(longitude : Incline), scale))
predictor_set_1_test  = mutate(predictor_set_1[-training_indices, ], 
                               across(c(longitude : Incline), scale))
predictor_set_1_train = as.matrix(predictor_set_1_train)
predictor_set_1_test  = as.matrix(predictor_set_1_test)

predictor_set_2       = select(data, c('Air.Temp':'Summit.Wind.Speed'))
predictor_set_2_train = mutate(predictor_set_2[training_indices, ],
                               across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))
predictor_set_2_test  = mutate(predictor_set_2[-training_indices, ],
                               across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))
predictor_set_2_train = as.matrix(predictor_set_2_train)
predictor_set_2_test  = as.matrix(predictor_set_2_test)

predictor_set_3       = select(data, c('Max.Temp.Grad':'Snow.Temp'))
predictor_set_3_train = mutate(predictor_set_3[training_indices, ],
                               across(c(Max.Temp.Grad : Snow.Temp), scale))
predictor_set_3_test  = mutate(predictor_set_3[-training_indices, ],
                               across(c(Max.Temp.Grad : Snow.Temp), scale))
predictor_set_3_train = as.matrix(predictor_set_3_train)
predictor_set_3_test  = as.matrix(predictor_set_3_test)

predictor_set_4_train = cbind(predictor_set_1_train, predictor_set_2_train,
                              predictor_set_3_train)
predictor_set_4_test  = cbind(predictor_set_1_test, predictor_set_2_test,
                              predictor_set_3_test)

training_data_list = list(predictor_set_1_train, predictor_set_2_train,
                          predictor_set_3_train, predictor_set_4_train)
testing_data_list = list(predictor_set_1_test, predictor_set_2_test,
                         predictor_set_3_test, predictor_set_4_test)

y = data$FAH
y = to_categorical(y, num_classes = 5)

y_train = y[training_indices, ]
y_test  = y[-training_indices, ]
```

A 70/30 training split was used in the data. Usually we would set aside a small portion for validation but as we will see later, **`keras`** handles this for us so we do not need to specify a validation set. Using a seed for this step is important as it ensures the split remains every time the code is ran. This is the first step toward achieving repeatable results. 

## Defining the model builder wrapper function

At this stage we would define the model and let **`keras`** find the rest. But before we can define the model we need to find the optimal specification of model hyperparameters. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, acitvation function to use on each layer, the dropout rate and the learning rate. **`keras`** is hightly flexible and allows the user control of almost every aspect of the model parameters. To find the optimal value for these hyperparameters we do hyperparameter tuning. In order to do this in R we need to use the **`kerastuneR`** library. In order to do the hyperparameter tuning with **`kerastuneR`** the model needs to be wrapped in a function that accepts as an input the hyperparameter specification. 

```{r model building function}
model_builder = function(hp){
  
  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 5, step = 1)
  lr = hp$Choice('learning_rate', values = seq(from = 1e-2, to = 1e-4, length.out = 5))  
  
  n_x   = ncol(x_train)
  input = layer_input(shape = c(n_x))

  x = input
  for (i in 1:n_layers){
    
    n_nodes = hp$Int(paste0('nodes_layer_', i),
                      min_value = 30, max_value = 50, step = 10)
    
    x = x %>%
      layer_dense(units = n_nodes, activation = 'relu') %>%
      layer_dropout(rate = 0.1)
  }
  output = x %>% 
    layer_dense(units = 5, activation = 'softmax')
  
  model = keras_model(inputs = input, outputs = output)
  
  model %>% compile(loss = 'categorical_crossentropy', 
                    optimizer = optimizer_adam(learning_rate = lr),
                    metrics = c(metric_categorical_accuracy()))
  
  return(model)
}
```

For this project, we have decided to tune for the number of layers, number of nodes on each layer and the learning rate. The activation function to be rectified linear units(Relu) and the dropout rate was sett to 0,1. The specific values were 1-5 layers(step size of 1) with 30-50(step size of 10) and 10 equally spaced learning rates [0.01 \ - 0.0001]. Therefore, there were a total of 75 unique models that could be fitted. The metric the Neural Network tries to minimize is the **`metric_categorical_accuracy`**. This was chosen since the target variable has more than 1 category. 

## Hyperparameter tuning

```{r model tuning}
for (i in 1:4){
  
  freqs = colSums(y_train) / nrow(y_train)
  weights = sqrt(1 / freqs)
  class_weights = dict()
  for (k in 0:(length(weights)-1)) {
    class_weights[[k]] = weights[k + 1]
  }
    
  x_train = training_data_list[[i]]
  
  tuner_randomsearch = kerastuneR::RandomSearch(hypermodel = model_builder,
                                                objective = 'val_categorical_accuracy',
                                                max_trials = 75, executions_per_trial = 3,
                                                directory = 'tuning',
                                                project_name = paste('randomsearch results', i),
                                                overwrite = TRUE)
  
  tuner_randomsearch %>% fit_tuner(x = x_train,
                                   y = y_train,
                                   epochs = 100,
                                   batch_size = 32,
                                   class_weight = class_weights,
                                   validation_split = 0.2,
                                   shuffle = TRUE)
}
```

**`kerastuneR`** has multiple tuning algorithms, we have used the **`RandomSearch`** algorithm. **`RandomSearch`** takes random combinations of the provided hyperparameters and fits the model each time. Since the combination of hyperparameters is random, there is a possibility that the same mode specification is ran multiple times by the algorithm. The algorithm does attempt to mitigate this but it not guaranteed to stop duplicate runs. We do have some control over this though by setting the **`max_trials`** variable to the  total number of unique models that can be specified from our selected tuning ranges. It has also been specified that each model should be fit 3 times by setting **`executions_per_trial = 3`**. This reduces variation in the results since there is an element of randomness in the initialisation of the model. A validation split of 20% was used and **`shuffle = T`** was used. Doing this shuffles the which observations get used as the validation set. This helps reduce the chances of overfitting. 

All the results from tuning were saved into folders so that the results can be extracted and used for further analysis. The tuning was was undertaken is by no means exhaustive, no tuning can ever be, but the range of values tuned over is relatively small and therefore the results should be taken with a pinch of salt. With more time and perhaps more compute power a better result is possible. 

## Tuning results

```{r}
library(dplyr)
library(tidyr)
library(jsonlite)

tuning_results = function(directory, max_layers){
  
  json_path = paste0(directory, '/trial.json')
  tuning_results = fromJSON(txt = json_path)
  hp               = tuning_results$hyperparameters$values
  number_of_layers = hp$number_of_layers
  learning_rate    = hp$learning_rate
  score            = tuning_results$score
  
  layers_nodes     = matrix(rep(NA, max_layers), nrow = 1)
  colnames(layers_nodes) = paste('nodes on layer ', 1:max_layers)
  for (i in 1:number_of_layers) {
    node_name = paste0('nodes_layer_', i)
    layers_nodes[1, i] = hp[[node_name]]
  }
  
  row_df = data.frame( Val_accuracy = score,  LR = learning_rate, 
                       layers = number_of_layers, layers_nodes, 
                       check.names = FALSE)
  return(row_df)
}

# Test
# tuning_results('tuning/randomsearch results 1/trial_19', 5)

directories              = list.dirs('tuning', recursive = FALSE)
randomsearch_search      = 'randomsearch'
randomsearch_directories = grep(randomsearch_search, directories, value = TRUE)

results_compiler = function(method_directory){

  for (i in 1:length(method_directory)){
    trial_directories = list.dirs(method_directory[i])
    results_df = do.call(rbind, lapply(trial_directories[-1], tuning_results, max_layers = 5))
    results_df = arrange(results_df, desc(Val_accuracy))
    save(results_df, file = paste0(method_directory[i], '/summary.RData'))
  }
}

# Test
#results_compiler('tuning/randomsearch results 1')

# the results automatically get saved locally, inside the same tuning folder
results_compiler(randomsearch_directories)
```

**`kerastuneR`** saves the tuning results as .json files. Each trial will be its own folder and inside that folder there will be a .json file containing information about the configuration of the Neural Network and the validation accuracy it achieved. The results were compiled into a single table containing the top 3 configurations from each of the predictor sets.

```{r}
library(dplyr)
library(keras)
library(tensorflow)
library(reticulate)
library(caret)
library(knitr)
library(kableExtra)

files = c('tuning/randomsearch results 1/summary.RData',
          'tuning/randomsearch results 2/summary.RData',
          'tuning/randomsearch results 3/summary.RData',
          'tuning/randomsearch results 4/summary.RData')
var_names = c('rs_results_1', 'rs_results_2', 'rs_results_3', 'rs_results_4')

for (i in seq_along(files)){
  temp_env = new.env()                       # temporary environment
  load(files[i], envir = temp_env)           # load into temp
  assign(var_names[i], temp_env$results_df)  # assign with custom name
}

results = list(rs_results_1, rs_results_2, rs_results_3, rs_results_4)
df = data.frame()
for (i in 1:4){
  results_df = results[[i]]
  top_3_models = results_df[1:3, ]
  top_3_models = mutate(top_3_models, across(c(Val_accuracy, LR), round, 5))  
  colnames(top_3_models) = c('Validation accuracy',
                             'Learning rate',
                             'Number of layers',
                             paste('nodes on layer ', 1:5)) # make sure to make this the maximum layers
  df = bind_rows(df, top_3_models)
}
df = mutate(df, 'Predictor set' = rep(1:4, each = 3), .before = 'Validation accuracy')

save(df, file = 'tuning/tuning_summary_table.RData')
```

```{r tuning table, results='asis', eval=TRUE}
load('../tuning/tuning_summary_table.RData')

summary_table = kable(df, format = 'html', booktabs = TRUE) %>% 
  kable_styling(full_width = FALSE, position = 'center') %>%
  collapse_rows(columns = 1, valign = 'middle')

for (i in seq(3, nrow(df), by = 3)) {
  summary_table = row_spec(summary_table, i, 
                           extra_css = "border-bottom: 2px solid black;")
}

summary_table
```

From the table it is evident that predictor sets 2 and 4 perform relatively equally with validation accuracies of 58% and 60% respectively. Predictor set 3 has slightly worse predictive capability with a prediction accuracy around 53%. Predictor set 1 is firmly the worst, achieving 42% validation accuracy.

```{r code for plot of the best model}
{
data = read.csv('data/scotland_avalanche_forecasts_2009_2025.csv')

# this is a very general solution to the problem of missing entries
# maybe imputation?
data = drop_na(data)
data = filter(data, FAH != '', OAH != '', Precip.Code != '')

data = filter(data, Alt <= 1300,  Alt >= 0)
data = filter(data, Aspect <= 360, Aspect >= 0)
data = filter(data, Incline <= 90, Incline >= 0)
data = filter(data, Wind.Dir <= 360, Wind.Dir >= 0)
data = filter(data, Wind.Speed >= 0)
data = filter(data, Cloud >= 0)
# Drift needs to be a factor
data = filter(data, Total.Snow.Depth >= 0)
data = filter(data, Foot.Pen >= 0)
data = filter(data, Ski.Pen >= 0)
# Rain.at.900 needs to be a factor
data = filter(data, Summit.Wind.Speed >= 0)
data = filter(data, Summit.Wind.Dir <= 360, Summit.Wind.Dir >= 0)
#data = filter(data, Max.Temp.Grad >= 10)
# Max.Hardness.Grad is a categorical
data = filter(data, Snow.Temp <= 5)


## data description ##

# Date = the date that the forecast was made
# Area = one of six forecasting region
# FAH = the forecast avalanche hazard for the following da
# OAH = the observed avalanche hazard on the following day (observation made the following day)
# longitude:Incline: position and topography at forecast location (predictor set)
# Air.Temp:Summit.Wind.Speed = weather in the vicinity of the forecast location at the
# time the forecast was made (predictor set 2)
# Max.Temp.Grad:Snow.Temp = results of a ”snow pack test” of the integrity of snow at
# the forecast location (predictor set 3)

#unique(data['Area'])
#unique(data['Obs'])
#unique(data['FAH'])
#unique(data['OAH'])
#unique(data['Precip.Code'])


set.seed(2025)
training_indices = sample(1:nrow(data), floor(nrow(data) * 0.7), replace = F)

data$FAH         = as.integer(factor(x = data$FAH)) - 1
data$OAH         = as.integer(factor(x = data$OAH)) - 1
data$Precip.Code = as.integer(factor(data$Precip.Code)) - 1

predictor_set_1       = select(data, c('longitude':'Incline'))
predictor_set_1_train = mutate(predictor_set_1[training_indices, ], 
                               across(c(longitude : Incline), scale))
predictor_set_1_test  = mutate(predictor_set_1[-training_indices, ], 
                               across(c(longitude : Incline), scale))
predictor_set_1_train = as.matrix(predictor_set_1_train)
predictor_set_1_test  = as.matrix(predictor_set_1_test)

predictor_set_2       = select(data, c('Air.Temp':'Summit.Wind.Speed'))
predictor_set_2_train = mutate(predictor_set_2[training_indices, ],
                               across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))
predictor_set_2_test  = mutate(predictor_set_2[-training_indices, ],
                               across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))
predictor_set_2_train = as.matrix(predictor_set_2_train)
predictor_set_2_test  = as.matrix(predictor_set_2_test)

predictor_set_3       = select(data, c('Max.Temp.Grad':'Snow.Temp'))
predictor_set_3_train = mutate(predictor_set_3[training_indices, ],
                               across(c(Max.Temp.Grad : Snow.Temp), scale))
predictor_set_3_test  = mutate(predictor_set_3[-training_indices, ],
                               across(c(Max.Temp.Grad : Snow.Temp), scale))
predictor_set_3_train = as.matrix(predictor_set_3_train)
predictor_set_3_test  = as.matrix(predictor_set_3_test)

predictor_set_4_train = cbind(predictor_set_1_train, predictor_set_2_train,
                              predictor_set_3_train)
predictor_set_4_test  = cbind(predictor_set_1_test, predictor_set_2_test,
                              predictor_set_3_test)


training_data_list = list(predictor_set_1_train, predictor_set_2_train,
                          predictor_set_3_train, predictor_set_4_train)
testing_data_list = list(predictor_set_1_test, predictor_set_2_test,
                         predictor_set_3_test, predictor_set_4_test)

y = data$FAH
y = to_categorical(y, num_classes = 5)

y_train = y[training_indices, ]
y_test  = y[-training_indices, ]

x_train = predictor_set_4_train
x_test = predictor_set_4_test
}

model_builder = function(hp){
  
  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 3, step = 1)
  lr = hp$Choice('learning_rate', values = c(1e-2, 1e-3, 1e-4))  
  
  n_x   = ncol(x_train)
  input = layer_input(shape = c(n_x))
  
  x = input
  for (i in 1:n_layers){
    
    n_nodes = hp$Int(paste0('nodes_layer_', i),
                     min_value = 30, max_value = 50, step = 10)
    
    x = x %>%
      layer_dense(units = n_nodes, activation = 'relu') %>%
      layer_dropout(rate = 0.1)
  }
  output = x %>% 
    layer_dense(units = 5, activation = 'softmax')
  
  model = keras_model(inputs = input, outputs = output)
  
  model %>% compile(loss = 'categorical_crossentropy', 
                    optimizer = optimizer_adam(learning_rate = lr),
                    metrics = c(metric_categorical_accuracy()))
  
  return(model)
}

tuner = kerastuneR::RandomSearch(hypermodel = model_builder, 
                                 objective = 'val_categorical_accuracy',
                                 max_trials = 1, 
                                 executions_per_trial = 1,
                                 directory = 'tuning',
                                 project_name = 'randomsearch results 4')

tuner$reload()
best_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] 

tuner$reload()
best_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] 

tf = tensorflow::tf
plot_model = tf$keras$utils$plot_model
plot_model(best_model, show_shapes = TRUE, show_layer_names = TRUE,
           expand_nested = FALSE,
           show_layer_activations = TRUE,
           dpi = 500,
           to_file = 'best_model_plot.png')
```

```{r plot of best model, eval=TRUE}

```

```{r predictions}
{
data = read.csv('data/scotland_avalanche_forecasts_2009_2025.csv')

# this is a very general solution to the problem of missing entries
# maybe imputation?
data = drop_na(data)
data = filter(data, FAH != '', OAH != '', Precip.Code != '')

data = filter(data, Alt <= 1300,  Alt >= 0)
data = filter(data, Aspect <= 360, Aspect >= 0)
data = filter(data, Incline <= 90, Incline >= 0)
data = filter(data, Wind.Dir <= 360, Wind.Dir >= 0)
data = filter(data, Wind.Speed >= 0)
data = filter(data, Cloud >= 0)
# Drift needs to be a factor
data = filter(data, Total.Snow.Depth >= 0)
data = filter(data, Foot.Pen >= 0)
data = filter(data, Ski.Pen >= 0)
# Rain.at.900 needs to be a factor
data = filter(data, Summit.Wind.Speed >= 0)
data = filter(data, Summit.Wind.Dir <= 360, Summit.Wind.Dir >= 0)
#data = filter(data, Max.Temp.Grad >= 10)
# Max.Hardness.Grad is a categorical
data = filter(data, Snow.Temp <= 5)


## data description ##

# Date = the date that the forecast was made
# Area = one of six forecasting region
# FAH = the forecast avalanche hazard for the following da
# OAH = the observed avalanche hazard on the following day (observation made the following day)
# longitude:Incline: position and topography at forecast location (predictor set)
# Air.Temp:Summit.Wind.Speed = weather in the vicinity of the forecast location at the
# time the forecast was made (predictor set 2)
# Max.Temp.Grad:Snow.Temp = results of a ”snow pack test” of the integrity of snow at
# the forecast location (predictor set 3)

#unique(data['Area'])
#unique(data['Obs'])
#unique(data['FAH'])
#unique(data['OAH'])
#unique(data['Precip.Code'])


set.seed(2025)
training_indices = sample(1:nrow(data), floor(nrow(data) * 0.7), replace = F)

data$FAH         = as.integer(factor(x = data$FAH)) - 1
data$OAH         = as.integer(factor(x = data$OAH)) - 1
data$Precip.Code = as.integer(factor(data$Precip.Code)) - 1

predictor_set_1       = select(data, c('longitude':'Incline'))
predictor_set_1_train = mutate(predictor_set_1[training_indices, ], 
                               across(c(longitude : Incline), scale))
predictor_set_1_test  = mutate(predictor_set_1[-training_indices, ], 
                               across(c(longitude : Incline), scale))
predictor_set_1_train = as.matrix(predictor_set_1_train)
predictor_set_1_test  = as.matrix(predictor_set_1_test)

predictor_set_2       = select(data, c('Air.Temp':'Summit.Wind.Speed'))
predictor_set_2_train = mutate(predictor_set_2[training_indices, ],
                               across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))
predictor_set_2_test  = mutate(predictor_set_2[-training_indices, ],
                               across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))
predictor_set_2_train = as.matrix(predictor_set_2_train)
predictor_set_2_test  = as.matrix(predictor_set_2_test)

predictor_set_3       = select(data, c('Max.Temp.Grad':'Snow.Temp'))
predictor_set_3_train = mutate(predictor_set_3[training_indices, ],
                               across(c(Max.Temp.Grad : Snow.Temp), scale))
predictor_set_3_test  = mutate(predictor_set_3[-training_indices, ],
                               across(c(Max.Temp.Grad : Snow.Temp), scale))
predictor_set_3_train = as.matrix(predictor_set_3_train)
predictor_set_3_test  = as.matrix(predictor_set_3_test)

predictor_set_4_train = cbind(predictor_set_1_train, predictor_set_2_train,
                              predictor_set_3_train)
predictor_set_4_test  = cbind(predictor_set_1_test, predictor_set_2_test,
                              predictor_set_3_test)


training_data_list = list(predictor_set_1_train, predictor_set_2_train,
                          predictor_set_3_train, predictor_set_4_train)
testing_data_list = list(predictor_set_1_test, predictor_set_2_test,
                         predictor_set_3_test, predictor_set_4_test)

y = data$FAH
y = to_categorical(y, num_classes = 5)

y_train = y[training_indices, ]
y_test  = y[-training_indices, ]

x_train = predictor_set_4_train
x_test = predictor_set_4_test
}

model_builder = function(hp){
  
  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 3, step = 1)
  lr = hp$Choice('learning_rate', values = c(1e-2, 1e-3, 1e-4))  
  
  n_x   = ncol(x_train)
  input = layer_input(shape = c(n_x))
  
  x = input
  for (i in 1:n_layers){
    
    n_nodes = hp$Int(paste0('nodes_layer_', i),
                     min_value = 30, max_value = 50, step = 10)
    
    x = x %>%
      layer_dense(units = n_nodes, activation = 'relu') %>%
      layer_dropout(rate = 0.1)
  }
  output = x %>% 
    layer_dense(units = 5, activation = 'softmax')
  
  model = keras_model(inputs = input, outputs = output)
  
  model %>% compile(loss = 'categorical_crossentropy', 
                    optimizer = optimizer_adam(learning_rate = lr),
                    metrics = c(metric_categorical_accuracy()))
  
  return(model)
}

tuner = kerastuneR::RandomSearch(hypermodel = model_builder, 
                                 objective = 'val_categorical_accuracy',
                                 max_trials = 1, 
                                 executions_per_trial = 1,
                                 directory = 'tuning',
                                 project_name = 'randomsearch results 4')

tuner$reload()
best_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] 

results      = best_model %>% evaluate(x_test, y_test)
y_pred_probs = best_model %>% predict(x_test)
y_pred       = max.col(y_pred_probs) - 1
true_classes = apply(y_test, 1, which.max) - 1 # it was one-hot encoded so i changed it back
metrics_list = confusionMatrix(factor(y_pred), factor(true_classes))

con_mat = as.data.frame(metrics_list$table)
con_mat = rename(con_mat, Predicted = Prediction, Reference = Reference, Count = Freq)
con_mat =  tidyr::pivot_wider(con_mat, names_from = Reference, values_from = Count,
                              values_fill = 0)
save(con_mat, file = 'test/con_mat.RData')
save(metrics_mat, file = 'test/metrics_mat.RData')
```

```{r confusion matrix, results='asis', eval=TRUE}
load('../test/con_mat.RData')
kable(con_mat, format = 'html', caption = 'Confusion matrix') %>%
  kable_styling(full_width = FALSE, position = 'center') %>%
  add_header_above(c(" " = 1, "Reference" = 5))
```

Say something about the confusion matrix here. Blah blah blah

```{r metrics matrix, results='asis', eval=TRUE}
load('../test/metrics_mat.RData')
kable(metrics_mat, format = 'html', caption = 'Class metrics', digits = 3) %>%
  kable_styling(full_width = FALSE, position = 'center')
```

Say something about the metrics matrix here. Blah blah blah
## Extra resources

[`keras`](https://cran.r-project.org/web/packages/keras/vignettes/){target='_blank'}

[`kerastuneR`](https://eagerai.github.io/kerastuneR/){target='_blank'}


