[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "data_processing.html",
    "href": "data_processing.html",
    "title": "Data processing",
    "section": "",
    "text": "Data processing"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA",
    "section": "",
    "text": "EDA"
  },
  {
    "objectID": "index.html#avalanche-hazard-forecasting-with-neural-networks",
    "href": "index.html#avalanche-hazard-forecasting-with-neural-networks",
    "title": "Home",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "For the modelling section of the project, keras was used for the training, tuning and testing. keras is an open-source python library that allows users to to build Neural Networks. It is known for modularity and it’s relatively gentle learning curve compared to other libraries such as pytorch. The modularity makes keras a good library for projects requiring lots of experimentation such as this one. It should be noted that when working with keras in R, all the functions used are wrapper functions of the native python functions. This makes version control extremely important. For this project, the models were created, trained and tuned using R version 4.5.1 (2025-06-13) and python version 3.11.6.\nThe goal for this project was to train a neural network to predict the forecasted avalanche hazard(FAH). Since we are predicting the forecast, the observed avalanche hazard(OAH) needs to be removed otherwise the model would be trained on information that would not be available on new data, i.e, data leakage. Therefore the predictor sets are explicitly defined so that there is no data leakage.\nA 70/30 training split was used in the data. Usually we would set aside a small portion for validation but as we will see later, keras handles this for us so we do not need to specify a validation set. Using a seed for this step is important as it ensures the split remains every time the code is ran. This is the first step toward achieving repeatable results.\n\n\n\nComparison of percentage of each category in the training and test sets\n\n\n\n\n\n\n\n\n\n\n\ncategory 0\ncategory 1\ncategory 2\ncategory 3\ncategory 4\n\n\n\n\ntrain\n0.328\n0.303\n0.236\n0.088\n0.044\n\n\ntest\n0.320\n0.312\n0.236\n0.089\n0.043\n\n\n\n\n\nThe table above reports the percentage of each category of FAH that is in the training and test data. It is important that the percentages are approximately equal in both sets and indeed this is the case. This is good but there is a problem. For the best results there should be approximately the same number of observations from each category in both the training and test sets, which is not the case for this data. Category 0 and 1 both are roughly the same at roughly 30% of total observations each. Category 2 is slightly lower with 23% but category 3 and 4 are hugely unrepresented in the data with only 9% and 4% respectively. This is an issue because big imbalances such as this will cause the model to optimise for correctly identifying observations from categories 0, 1 and 2.\nThere a few possible methods we can use to mitigate this issue but the method chosen for this project was to define custom class weights so that incorrect predictions have different penalties for each category. The idea is that underrepresented categories have higher weights so that in the training the model gets penalised more heavily and hopefully the final model is better at making predictions for the underpresented categories. The weights chosen were inversely proportional to the percentage of appearance in the training data. A possible area of further experimentation is using bootstrap sampling to equalise the percentages of each category.\nFor this project there were 4 predictor sets that were evaluated. Predictor set 1 contained variables relating to the location of the observation. Predictor set 2 contained variables relating to the weather conditions of the observation. Predictor set 3 contained variables relating to a snow pack test. Predictor set 4 contained all the varibles in the dataset and thus contained all three predictor sets inside it. The final model will be built using all the available data, so predictor set 4. The remaining three predictor sets serve as yardstick to which we can compare which sets of variables are important."
  },
  {
    "objectID": "model.html#model-building",
    "href": "model.html#model-building",
    "title": "Model",
    "section": "",
    "text": "A neural network was fitted to the data in R, specifically R version 4.5.1 (2025-06-13) and python version 3.11.6. The reason it is important to specify the versions is because keras and its dependencies in R are wrappers of the python libraries so ensuring cross compatibility is imperative.\n\nlibrary(keras)\nlibrary(kerastuneR)\nlibrary(tensorflow)\nlibrary(dplyr)\nlibrary(tidyr)\n\nThe packages that will be used in the model building and tuning are given above. The packages can be installed using install.packages('package', dependencies = T). Note that a warning might appear saying keras is deprecated and that you should use keras3, this can be ignored for now as the provided code does not work when using keras3.\n\n# ensure that inside the folder you have the project in you also have a folder\n# called data that contains the data. \ndata = read.csv('./data/scotland_avalanche_forecasts_2009_2025.csv')\n\n# this is a very general solution to the problem of missing entries\n# maybe imputation?\ndata = drop_na(data)\ndata = filter(data, FAH != '', OAH != '', Precip.Code != '')\n\n## data description ##\n\n# Date = the date that the forecast was made\n# Area = one of six forecasting region\n# FAH = the forecast avalanche hazard for the following da\n# OAH = the observed avalanche hazard on the following day (observation made the following day)\n# longitude:Incline: position and topography at forecast location (predictor set)\n# Air.Temp:Summit.Wind.Speed = weather in the vicinity of the forecast location at the\n# time the forecast was made (predictor set 2)\n# Max.Temp.Grad:Snow.Temp = results of a ”snow pack test” of the integrity of snow at\n# the forecast location (predictor set 3)\n\n#unique(data['Area'])\n#unique(data['Obs'])\n#unique(data['FAH'])\n#unique(data['OAH'])\n#unique(data['Precip.Code'])\n\n\ndata$FAH         = as.integer(factor(x = data$FAH)) - 1\ndata$OAH         = as.integer(factor(x = data$OAH)) - 1\ndata$Precip.Code = as.integer(factor(data$Precip.Code)) - 1\n\n\npredictor_set_1 = select(data, c('longitude':'Incline'))\npredictor_set_1 = mutate(predictor_set_1, across(c(longitude : Incline), scale))\npredictor_set_1 = as.matrix(predictor_set_1)\n\npredictor_set_2 = select(data, c('Air.Temp':'Summit.Wind.Speed', 'FAH'))\npredictor_set_2 = mutate(predictor_set_2,\n                         across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))\npredictor_set_2 = as.matrix(predictor_set_2)\n\npredictor_set_3 = select(data, c('Max.Temp.Grad':'Snow.Temp', 'FAH'))\npredictor_set_3 = mutate(predictor_set_3, across(c(Max.Temp.Grad : Snow.Temp), scale))\npredictor_set_3 = as.matrix(predictor_set_3)\n\n\nset.seed(2025)\ntraining_indices = runif(n = floor(nrow(data) * 0.7), min = 1, max = nrow(data))\n\npredictor_set_1_train = predictor_set_1[training_indices, ] \npredictor_set_1_test  = predictor_set_1[-training_indices, ]\n\npredictor_set_2_train = predictor_set_2[training_indices, ] \npredictor_set_2_test  = predictor_set_2[-training_indices, ]\n\npredictor_set_3_train = predictor_set_3[training_indices, ] \npredictor_set_3_test  = predictor_set_3[-training_indices, ]\n\ntraining_data_list = list(predictor_set_1_train, predictor_set_2_train,\n                          predictor_set_3_train)\ntesting_data_list = list(predictor_set_1_test, predictor_set_2_test,\n                         predictor_set_3_test)\n\ny = data$FAH\ny = to_categorical(y, num_classes = 5)\n\ny_train = y[training_indices, ]\ny_test  = y[-training_indices, ]\n\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 2, max_value = 10, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-1, 1e-2, 1e-3))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n\n  x = input\n  for (i in 1:n_layers){\n    x = x %&gt;%\n      layer_dense(units = 20, activation = 'relu') %&gt;%\n      layer_dropout(rate = 0.5)\n  }\n  output = x %&gt;% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %&gt;% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c('accuracy'))\n  \n  return(model)\n}\n\n\nfor (i in 1){\n  \n  x_train = training_data_list[[i]]\n  \n  tuner_randomsearch = kerastuneR::RandomSearch(hypermodel = model_builder,\n                                                objective = 'val_accuracy',\n                                                max_trials = 50, \n                                                executions_per_trial = 3,\n                                                directory = 'tuning',\n                                                project_name = paste('randomsearch results', i),\n                                                overwrite = TRUE)\n  \n  tuner_randomsearch %&gt;% fit_tuner(x = x_train,\n                                   y = y_train,\n                                   epochs = 50,\n                                   validation_split = 0.2, shuffle = TRUE)\n  \n  #results_summary(tuner = tuner_randomsearch, num_trials = 5)\n  \n  tuner_hyperband = kerastuneR::Hyperband(hypermodel = model_builder,\n                                          objective = 'val_accuracy',\n                                          directory = 'tuning',\n                                          project_name = paste('hyperband results', i),\n                                          max_epochs = 50,\n                                          hyperband_iterations = 10,\n                                          seed = 2025)\n  \n  tuner_hyperband %&gt;% fit_tuner(x = x_train,\n                                y = y_train,\n                                epochs = 50,\n                                validation_split = 0.2, shuffle = TRUE)\n  \n  #results_summary(tuner = tuner_hyperband, num_trials = 5)\n}"
  },
  {
    "objectID": "model.html#getting-set-up",
    "href": "model.html#getting-set-up",
    "title": "Model",
    "section": "",
    "text": "A neural network was fitted to the data in R, specifically R version 4.5.1 (2025-06-13) and python version 3.11.6. The reason it is important to specify the versions is because keras and its dependencies in R are wrappers of the python libraries so ensuring cross compatibility is imperative.\nThe packages that will be used in the model building and tuning are given above. The packages can be installed using install.packages('package', dependencies = T). Note that after reading in the keras library a warning might appear saying that it is deprecated and that you should use keras3, this can be ignored for now as the provided code does not work when using keras3.\nSince we are trying to predict the forecasted avalanche hazard(FAH), it is important that we do not allow the model to be trained on the observed avalanche hazard(OAH). Therefore the predictor sets are explicitly defined so that there is no accidental data leakage. The FAH, OAH and Precip.Code variables are converted to integers so that the data is compatible with keras.\nA 70/30 training split was used in the data. Usually we would set aside a small portion for validation but as we will see later, keras handles this for us so we do not need to specify a validation set. Using a seed for this step is important as it ensures the split remains every time the code is ran. This is the first step toward achieving repeatable results."
  },
  {
    "objectID": "model.html#defining-the-model",
    "href": "model.html#defining-the-model",
    "title": "Model",
    "section": "Defining the model",
    "text": "Defining the model\nAt this stage we would define the model and let keras do it’s thing. But before we can define the model we need to find the optimal specification of model hyperparameters. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, acitvation function to use on each layer, the dropout rate and the learning rate. keras is hightly flexible and allows the user control of almost every aspect of the model parameters. To find the optimal value for these hyperparameters we do hyperparameter tuning. In order to do this in R we need to use the kerastuneR library. In order to do the hyperparameter tuning with kerastuneR the model needs to be wrapped in a function that accepts as an input the hyperparameter specification.\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 2, max_value = 10, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-1, 1e-2, 1e-3))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n\n  x = input\n  for (i in 1:n_layers){\n    x = x %&gt;%\n      layer_dense(units = 20, activation = 'relu') %&gt;%\n      layer_dropout(rate = 0.5)\n  }\n  output = x %&gt;% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %&gt;% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c('accuracy'))\n  \n  return(model)\n}\n\nFor this project, we have decided to tune for the number of layers and the learning rate. The number of nodes on each layer was chosen to be 20, the activation function to be rectified linear units(Relu) and the dropout rate was sett to 0.5. The specific values were 2-10 layers and learning rates \\(\\in \\{0.1,\\ 0.01,\\ 0.001 \\}\\). Therefore, there were a total of 27 unique models that could be fitted."
  },
  {
    "objectID": "model.html#hyperparameter-tuning",
    "href": "model.html#hyperparameter-tuning",
    "title": "Model",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nkerastuneR has multiple tuning algorithms, we have used the RandomSearch algorithm. RandomSearch takes random combinations of the provided hyperparameters and fits the model each time. Since the combination of hyperparameters is random, there is a possibility that the same mode configuration is ran multiple times by the algorithm. The algorithm does attempt to mitigate this but it not guaranteed to stop duplicate runs. We do have some control over this though by setting the max_trials variable to the total number of unique models that can be specified from our selected tuning ranges. It has also been specified that each model should be fit 3 times by setting executions_per_trial = 3. This reduces variation in the results since there is an element of randomness in the initialisation of the model. A validation split of 20% was used and shuffle = T was used. Doing this shuffles the which observations get used as the validation set. This helps reduce the chances of overfitting.\nAll the results from tuning were saved into folders so that the results can be extracted and used for further analysis. The tuning was was undertaken is by no means exhaustive, no tuning can ever be, but the range of values tuned over is relatively small and therefore the results should be taken with a pinch of salt. With more time and perhaps more compute power a better result is possible."
  },
  {
    "objectID": "model.html#defining-the-model-builder-wrapper-function",
    "href": "model.html#defining-the-model-builder-wrapper-function",
    "title": "Model",
    "section": "Defining the model builder wrapper function",
    "text": "Defining the model builder wrapper function\nNeural networks are extremely flexible and configurable models. But this configurability means that there are several parameters that need to be defined before the model can be trained. In order to find the optimal configuration, hyperparameter tuning needs to be done first. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, the acitvation function to use on each layer, the dropout rate and the learning rate. keras is flexible and allows the user control of almost every aspect of the model through the parameters. In order to do the hyperameter tuning in R, the kerastuneR library is used. But in order to do the hyperparameter tuning with kerastuneR the model needs to be wrapped in a function that accepts as an input the hyperparameter configuration, and outputs a model.\nFor this project, it was decided to tune for the number of layers, number of nodes on each layer and the learning rate. The activation function was chosen to be rectified linear units(Relu) and the dropout rate was set to 0,1. Each dense layer was followed by a dropout layer. The specific values that were tuned across were 1-5 layers(step size of 1) with 30-50(step size of 10) nodes on each layer and 5 equally spaced learning rates [0,01  - 0,0001]. Therefore, there were a total of 75 unique models that could be fitted. The metric the Neural Network tries to minimize is the metric_categorical_accuracy. This was chosen since the target variable has more than 1 category."
  },
  {
    "objectID": "model.html#extra-resources",
    "href": "model.html#extra-resources",
    "title": "Model",
    "section": "Extra resources",
    "text": "Extra resources\nkeras\nkerastuneR"
  },
  {
    "objectID": "model.html#tuning-results",
    "href": "model.html#tuning-results",
    "title": "Model",
    "section": "Tuning results",
    "text": "Tuning results\nkerastuneR saves the tuning results as .json files. Each trial will be its own folder and inside that folder there will be a .json file containing information about the configuration of the Neural Network and the validation accuracy it achieved. The results were compiled into a single table containing the top 3 configurations from each of the predictor sets.\n\n\n\nValidation accuracy by hyperparameter configuration across four predictor sets\n\n\nSystematic hyperparameter tuning across four predictor sets revealed distinct performance patterns. As summarized in Table @ref(tab:tuning_table), Predictor Set 2 (weather conditions) achieved the highest validation accuracy (66%), slightly outperforming Predictor Set 4 (all variables) at 65%. This suggests that weather variables capture the most critical signals for avalanche hazard forecasting, with topographic and snow-pack variables providing only marginal incremental value.\n\nTop hyperparameter configurations per predictor set, ranked by validation accuracy.\n\n\nPredictor set\nValidation accuracy\nLearning rate\nNumber of layers\nnodes on layer 1\nnodes on layer 2\nnodes on layer 3\nnodes on layer 4\nnodes on layer 5\n\n\n\n\n1\n0.50829\n0.01000\n4\n50\n30\n50\n50\nNA\n\n\n0.46535\n0.01000\n5\n30\n30\n50\n40\n40\n\n\n0.45376\n0.01000\n5\n30\n30\n30\n40\n30\n\n\n2\n0.65826\n0.00753\n5\n30\n40\n30\n30\n30\n\n\n0.65735\n0.00753\n5\n50\n50\n30\n30\n40\n\n\n0.65621\n0.01000\n4\n40\n30\n30\n40\nNA\n\n\n3\n0.62645\n0.01000\n3\n40\n40\n50\nNA\nNA\n\n\n0.62509\n0.01000\n4\n40\n50\n50\n30\nNA\n\n\n0.62463\n0.01000\n5\n40\n40\n30\n40\n40\n\n\n4\n0.64940\n0.00505\n4\n30\n40\n40\n40\nNA\n\n\n0.64781\n0.01000\n4\n50\n40\n50\n40\nNA\n\n\n0.64781\n0.01000\n5\n50\n40\n30\n50\n50\n\n\n\n\n\nArchitecturally, models with 4–5 layers and 30–50 nodes per layer consistently outperformed others, with no clear gains beyond 5 layers. The optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy.\n\nThe selected architecture (Figure @ref(fig:best_model_plot)) utilizes ReLU activation in hidden layers and softmax output activation, appropriate for the multi-class ordinal nature of avalanche hazard prediction. This configuration represents an optimal balance between model complexity and predictive performance for the comprehensive feature set.\n\nConfusion matrix for avalanche hazard predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted\n\n\n\nPredicted\n0\n1\n2\n3\n4\n\n\n\n\n0\n857\n420\n88\n15\n10\n\n\n1\n118\n326\n219\n51\n7\n\n\n2\n29\n231\n413\n186\n94\n\n\n3\n0\n4\n18\n20\n16\n\n\n4\n0\n0\n4\n7\n8\n\n\n\n\n\nThe confusion matrix of the the fitted model is reported above. It is clear that category 0 is the best estimated while categories 3 and 4 are estimated the worst. This result was expected since category 3 and 4 are very underepresented in both the training and test data. Concrete metrics of the model performance is given in the table below.\n\nComprehensive classification metrics by avalanche hazard class.\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: 0\n0.854\n0.751\n0.617\n0.916\n0.617\n0.854\n0.716\n0.320\n0.273\n0.443\n0.802\n\n\nClass: 1\n0.332\n0.817\n0.452\n0.729\n0.452\n0.332\n0.383\n0.312\n0.104\n0.230\n0.575\n\n\nClass: 2\n0.557\n0.775\n0.433\n0.850\n0.433\n0.557\n0.487\n0.236\n0.131\n0.303\n0.666\n\n\nClass: 3\n0.072\n0.987\n0.345\n0.916\n0.345\n0.072\n0.119\n0.089\n0.006\n0.018\n0.529\n\n\nClass: 4\n0.059\n0.996\n0.421\n0.959\n0.421\n0.059\n0.104\n0.043\n0.003\n0.006\n0.528\n\n\n\n\n\nSensitivity gives the percent of the time the model predicted an observation as belonging to a category and it actually belonging to that category. The sensitivity for class 0 is the best with a value of 0,854. There is a steep drop off for the other classes but class 2 is the second highest with a value of 0,557 and the rest are all below 0.5 meaning that more often than not, the model is unable to identify the correct category. An analogy for sensitivity is a test with high sensitivity(close to the maximum of 1) will identify most of the patients with with the flu as having the flu but this may mean that lots of patients without the flu also get identified as having the flu. An extreme case may be if the model predicted all observations as belonging to category 0. Then the sensitivity would be 1,meaning that all observations that belong to category 0 are predicted to be category 0. So there need to be a balance because we do not want this\nSpecificity gives the percent of time the an observation does not belong to a specific category and the model predicts it as not belonging to that category. High specificity(close to the maximum of 1) is analogous to a test rarely every flagging someone as having the flu when they don’t. A perfect model will have high sensitivity with a high specificity, meaning that it is able to identify when observations belong to a category and does not incorrectly predict other observations as belonging to that category. The table above indicates that all the categories have relatively high specificity. Class 4 has a specificity of 0.996 which on the surface looks great but because the sensitivity is so low, this high specificity just means that the model rarely every predicts any observations as belonging to category 4. The same follows for category 3. This is also seen in the extremely low detection rate and prevalence for these categories. These results, specifically for categories 3 and 4 are an indication that perhaps just reweighting the classes was not enough to overcome the imbalance in the data.\nA better metric for instances such as this where the data is imbalanced is the F1 score. The F1 score is the harmonic mean of the precision and recall. The precision is the percentage of observations that belonged to a category and were correctly predicted as belonging to that category. The recall is just the sensitivity. The F1 score, same as the other metrics, ranges from 0 to 1 with 0 being the worst and 1 being the best. The F1 score is useful because the harmonic mean because it is less affected by extreme values than the arithmetic mean. Categories 4 and 3 are by far the worst with values of 0,104 and 0,119. Category 0 has the best F1 score with a value of 0,716. Then category 1 has a value of 0,383 and category 3 has a score of 0,487. All these values, except for category are quite poor and indicate lack of predictive power. This is reflected in the fact that the model achieved an accuracy of 51,7% on the test set. The model is better than blindly guessing which we expect to return an accuracy of 20% but the model lacks predictive power. Expanding the search are for tuning as well as utalising bootstrapping may yield better results."
  },
  {
    "objectID": "model.html#introduction-to-keras",
    "href": "model.html#introduction-to-keras",
    "title": "Model",
    "section": "",
    "text": "keras is an open-source python library that allows users to to build Neural Networks. It is known for modularity and it’s relatively gentle learning curve compared to other libraries such as pytorch. The modularity makes keras a good library for projects requiring experimentation such as this one. It should be noted that when working with keras in R, all the functions used are wrapper functions of the python functions. This makes version control extremely important. For this project, the models were created, trained and tuned using R version 4.5.1 (2025-06-13) and python version 3.11.6.\nFor this project, the goal is to train a neural network to predict the forecasted avalanche hazard(FAH). Since we are predicting the forecast, the observed avalanche hazard(OAH) needs to be removed otherwise the model would be trained on information that would not be available on new data, i.e, data leakage. Therefore the predictor sets are explicitly defined so that there is no data leakage.\nA 70/30 training split was used in the data. Usually we would set aside a small portion for validation but as we will see later, keras handles this for us so we do not need to specify a validation set. Using a seed for this step is important as it ensures the split remains every time the code is ran. This is the first step toward achieving repeatable results.\n\n\n\nComparison of percentage of each category in the training and test sets\n\n\n\n\n\n\n\n\n\n\n\ncategory 0\ncategory 1\ncategory 2\ncategory 3\ncategory 4\n\n\n\n\ntrain\n0.328\n0.303\n0.236\n0.088\n0.044\n\n\ntest\n0.320\n0.312\n0.236\n0.089\n0.043\n\n\n\n\n\nThe table above reports the percentage of each category of FAH that is in the training and test data. It is important that the percentages are approximately equal in both sets and indeed this is the case. This is good but there is a problem. For the best results there should be approximately the same number of observations from each category in both the training and test sets, which is not the case for this data. Category 0 and 1 both are roughly the same at roughly 30% of total observations each. Category 2 is slightly lower with 23% but category 3 and 4 are hugely unrepresented in the data with only 9% and 4% respectively. This is an issue because big imbalances such as this will cause the model to optimise for correctly identifying observations from categories 0, 1 and 2."
  },
  {
    "objectID": "model.html#methods",
    "href": "model.html#methods",
    "title": "Model",
    "section": "",
    "text": "For the modelling section of the project, keras was used for the training, tuning and testing. keras is an open-source python library that allows users to to build Neural Networks. It is known for modularity and it’s relatively gentle learning curve compared to other libraries such as pytorch. The modularity makes keras a good library for projects requiring lots of experimentation such as this one. It should be noted that when working with keras in R, all the functions used are wrapper functions of the native python functions. This makes version control extremely important. For this project, the models were created, trained and tuned using R version 4.5.1 (2025-06-13) and python version 3.11.6.\nThe goal for this project was to train a neural network to predict the forecasted avalanche hazard(FAH). Since we are predicting the forecast, the observed avalanche hazard(OAH) needs to be removed otherwise the model would be trained on information that would not be available on new data, i.e, data leakage. Therefore the predictor sets are explicitly defined so that there is no data leakage.\nA 70/30 training split was used in the data. Usually we would set aside a small portion for validation but as we will see later, keras handles this for us so we do not need to specify a validation set. Using a seed for this step is important as it ensures the split remains every time the code is ran. This is the first step toward achieving repeatable results.\n\n\n\nComparison of percentage of each category in the training and test sets\n\n\n\n\n\n\n\n\n\n\n\ncategory 0\ncategory 1\ncategory 2\ncategory 3\ncategory 4\n\n\n\n\ntrain\n0.328\n0.303\n0.236\n0.088\n0.044\n\n\ntest\n0.320\n0.312\n0.236\n0.089\n0.043\n\n\n\n\n\nThe table above reports the percentage of each category of FAH that is in the training and test data. It is important that the percentages are approximately equal in both sets and indeed this is the case. This is good but there is a problem. For the best results there should be approximately the same number of observations from each category in both the training and test sets, which is not the case for this data. Category 0 and 1 both are roughly the same at roughly 30% of total observations each. Category 2 is slightly lower with 23% but category 3 and 4 are hugely unrepresented in the data with only 9% and 4% respectively. This is an issue because big imbalances such as this will cause the model to optimise for correctly identifying observations from categories 0, 1 and 2.\nThere a few possible methods we can use to mitigate this issue but the method chosen for this project was to define custom class weights so that incorrect predictions have different penalties for each category. The idea is that underrepresented categories have higher weights so that in the training the model gets penalised more heavily and hopefully the final model is better at making predictions for the underpresented categories. The weights chosen were inversely proportional to the percentage of appearance in the training data. A possible area of further experimentation is using bootstrap sampling to equalise the percentages of each category.\nFor this project there were 4 predictor sets that were evaluated. Predictor set 1 contained variables relating to the location of the observation. Predictor set 2 contained variables relating to the weather conditions of the observation. Predictor set 3 contained variables relating to a snow pack test. Predictor set 4 contained all the varibles in the dataset and thus contained all three predictor sets inside it. The final model will be built using all the available data, so predictor set 4. The remaining three predictor sets serve as yardstick to which we can compare which sets of variables are important."
  },
  {
    "objectID": "model_dt.html",
    "href": "model_dt.html",
    "title": "Methodology",
    "section": "",
    "text": "Neural network modelling was implemented using the library, accessed through its R wrapper. Keras is an open-source deep learning framework known for its modularity and relatively gentle learning curve compared to other libraries such as PyTorch. Its modular design makes it well suited for projects that require experimentation with multiple configurations.\nBecause Keras functions in R act as wrappers around the underlying Python functions, version control is critical to ensure reproducibility. This project was developed in with .\n\n\n\nThe modelling goal was to predict . To prevent data leakage, the variable was excluded from the predictor sets, since this information would not be available in real-world forecasting scenarios.\nA was applied, with a fixed random seed to ensure reproducibility. Although a validation set is often held out, Keras provides internal validation handling, so this was not specified manually.\nTable 1 reports the class distribution across training and test sets. The proportions are broadly consistent between the two partitions, but the data exhibits strong class imbalance. Categories 0 and 1 each account for roughly 30% of the data, category 2 for 23%, while categories 3 and 4 are severely underrepresented (9% and 4% respectively). This imbalance risks biasing the model toward predicting majority classes.\nTo mitigate this, were introduced. The weights were set inversely proportional to the prevalence of each category in the training set, so that errors on minority classes carried higher penalties during training. Alternative methods such as bootstrap resampling could also be considered, but were not explored here.\n\n\n\n\nTable 1: Comparison of percentage of each category in the training and test sets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory 0\ncategory 1\ncategory 2\ncategory 3\ncategory 4\n\n\n\n\ntrain\n0.328\n0.303\n0.236\n0.088\n0.044\n\n\ntest\n0.320\n0.312\n0.236\n0.089\n0.043\n\n\n\n\n\n\n\n\nThe table above reports the percentage of each category of FAH that is in the training and test data. It is important that the percentages are approximately equal in both sets and indeed this is the case. This is good but there is a problem. For the best results there should be approximately the same number of observations from each category in both the training and test sets, which is not the case for this data. Category 0 and 1 both are roughly the same at roughly 30% of total observations each. Category 2 is slightly lower with 23% but category 3 and 4 are hugely unrepresented in the data with only 9% and 4% respectively. This is an issue because big imbalances such as this will cause the model to optimise for correctly identifying observations from categories 0, 1 and 2.\nThere a few possible methods we can use to mitigate this issue but the method chosen for this project was to define custom class weights so that incorrect predictions have different penalties for each category. The idea is that underrepresented categories have higher weights so that in the training the model gets penalised more heavily and hopefully the final model is better at making predictions for the underpresented categories. The weights chosen were inversely proportional to the percentage of appearance in the training data. A possible area of further experimentation is using bootstrap sampling to equalise the percentages of each category.\n\n\n\nFor this project there were 4 predictor sets that were evaluated. Predictor set 1 contained variables relating to the location of the observation. Predictor set 2 contained variables relating to the weather conditions of the observation. Predictor set 3 contained variables relating to a snow pack test. Predictor set 4 contained all the varibles in the dataset and thus contained all three predictor sets inside it. The final model will be built using all the available data, so predictor set 4. The remaining three predictor sets serve as yardstick to which we can compare which sets of variables are important.\n\n\n\nNeural networks are extremely flexible and configurable models. But this configurability means that there are several parameters that need to be defined before the model can be trained. In order to find the optimal configuration, hyperparameter tuning needs to be done first. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, the acitvation function to use on each layer, the dropout rate and the learning rate. keras is flexible and allows the user control of almost every aspect of the model through the parameters. In order to do the hyperameter tuning in R, the kerastuneR library is used. But in order to do the hyperparameter tuning with kerastuneR the model needs to be wrapped in a function that accepts as an input the hyperparameter configuration, and outputs a model.\nFor this project, it was decided to tune for the number of layers, number of nodes on each layer and the learning rate. The activation function was chosen to be rectified linear units(Relu) and the dropout rate was set to 0,1. Each dense layer was followed by a dropout layer. The specific values that were tuned across were 1-5 layers(step size of 1) with 30-50(step size of 10) nodes on each layer and 5 equally spaced learning rates [0,01  - 0,0001]. Therefore, there were a total of 75 unique models that could be fitted. The metric the Neural Network tries to minimize is the metric_categorical_accuracy. This was chosen since the target variable has more than 1 category.\n\n\n\nkerastuneR has multiple tuning algorithms, we have used the RandomSearch algorithm. RandomSearch takes random combinations of the provided hyperparameters and fits the model each time. Since the combination of hyperparameters is random, there is a possibility that the same mode configuration is ran multiple times by the algorithm. The algorithm does attempt to mitigate this but it not guaranteed to stop duplicate runs. We do have some control over this though by setting the max_trials variable to the total number of unique models that can be specified from our selected tuning ranges. It has also been specified that each model should be fit 3 times by setting executions_per_trial = 3. This reduces variation in the results since there is an element of randomness in the initialisation of the model. A validation split of 20% was used and shuffle = T was used. Doing this shuffles the which observations get used as the validation set. This helps reduce the chances of overfitting.\nAll the results from tuning were saved into folders so that the results can be extracted and used for further analysis. The tuning was was undertaken is by no means exhaustive, no tuning can ever be, but the range of values tuned over is relatively small and therefore the results should be taken with a pinch of salt. With more time and perhaps more compute power a better result is possible.\nkerastuneR saves the tuning results as .json files. Each trial will be its own folder and inside that folder there will be a .json file containing information about the configuration of the Neural Network and the validation accuracy it achieved. The results were compiled into a single table containing the top 3 configurations from each of the predictor sets."
  },
  {
    "objectID": "model_dt.html#methodology",
    "href": "model_dt.html#methodology",
    "title": "Model",
    "section": "",
    "text": "Neural network modelling was implemented using the library, accessed through its R wrapper. Keras is an open-source deep learning framework known for its modularity and relatively gentle learning curve compared to other libraries such as PyTorch. Its modular design makes it well suited for projects that require experimentation with multiple configurations.\nBecause Keras functions in R act as wrappers around the underlying Python functions, version control is critical to ensure reproducibility. This project was developed in with .\nThe modelling goal was to predict . To prevent data leakage, the variable was excluded from the predictor sets, since this information would not be available in real-world forecasting scenarios.\nA was applied, with a fixed random seed to ensure reproducibility. Although a validation set is often held out, Keras provides internal validation handling, so this was not specified manually.\nTable~\\(\\ref{tab:class_dist}\\) reports the class distribution across training and test sets. The proportions are broadly consistent between the two partitions, but the data exhibits strong class imbalance. Categories 0 and 1 each account for roughly 30% of the data, category 2 for 23%, while categories 3 and 4 are severely underrepresented (9% and 4% respectively). This imbalance risks biasing the model toward predicting majority classes.\nTo mitigate this, were introduced. The weights were set inversely proportional to the prevalence of each category in the training set, so that errors on minority classes carried higher penalties during training. Alternative methods such as bootstrap resampling could also be considered, but were not explored here.\n\n\n\nComparison of percentage of each category in the training and test sets\n\n\n\n\n\n\n\n\n\n\n\ncategory 0\ncategory 1\ncategory 2\ncategory 3\ncategory 4\n\n\n\n\ntrain\n0.328\n0.303\n0.236\n0.088\n0.044\n\n\ntest\n0.320\n0.312\n0.236\n0.089\n0.043\n\n\n\n\n\nThe table above reports the percentage of each category of FAH that is in the training and test data. It is important that the percentages are approximately equal in both sets and indeed this is the case. This is good but there is a problem. For the best results there should be approximately the same number of observations from each category in both the training and test sets, which is not the case for this data. Category 0 and 1 both are roughly the same at roughly 30% of total observations each. Category 2 is slightly lower with 23% but category 3 and 4 are hugely unrepresented in the data with only 9% and 4% respectively. This is an issue because big imbalances such as this will cause the model to optimise for correctly identifying observations from categories 0, 1 and 2.\nThere a few possible methods we can use to mitigate this issue but the method chosen for this project was to define custom class weights so that incorrect predictions have different penalties for each category. The idea is that underrepresented categories have higher weights so that in the training the model gets penalised more heavily and hopefully the final model is better at making predictions for the underpresented categories. The weights chosen were inversely proportional to the percentage of appearance in the training data. A possible area of further experimentation is using bootstrap sampling to equalise the percentages of each category.\nFor this project there were 4 predictor sets that were evaluated. Predictor set 1 contained variables relating to the location of the observation. Predictor set 2 contained variables relating to the weather conditions of the observation. Predictor set 3 contained variables relating to a snow pack test. Predictor set 4 contained all the varibles in the dataset and thus contained all three predictor sets inside it. The final model will be built using all the available data, so predictor set 4. The remaining three predictor sets serve as yardstick to which we can compare which sets of variables are important.\nNeural networks are extremely flexible and configurable models. But this configurability means that there are several parameters that need to be defined before the model can be trained. In order to find the optimal configuration, hyperparameter tuning needs to be done first. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, the acitvation function to use on each layer, the dropout rate and the learning rate. keras is flexible and allows the user control of almost every aspect of the model through the parameters. In order to do the hyperameter tuning in R, the kerastuneR library is used. But in order to do the hyperparameter tuning with kerastuneR the model needs to be wrapped in a function that accepts as an input the hyperparameter configuration, and outputs a model.\nFor this project, it was decided to tune for the number of layers, number of nodes on each layer and the learning rate. The activation function was chosen to be rectified linear units(Relu) and the dropout rate was set to 0,1. Each dense layer was followed by a dropout layer. The specific values that were tuned across were 1-5 layers(step size of 1) with 30-50(step size of 10) nodes on each layer and 5 equally spaced learning rates [0,01  - 0,0001]. Therefore, there were a total of 75 unique models that could be fitted. The metric the Neural Network tries to minimize is the metric_categorical_accuracy. This was chosen since the target variable has more than 1 category.\nkerastuneR has multiple tuning algorithms, we have used the RandomSearch algorithm. RandomSearch takes random combinations of the provided hyperparameters and fits the model each time. Since the combination of hyperparameters is random, there is a possibility that the same mode configuration is ran multiple times by the algorithm. The algorithm does attempt to mitigate this but it not guaranteed to stop duplicate runs. We do have some control over this though by setting the max_trials variable to the total number of unique models that can be specified from our selected tuning ranges. It has also been specified that each model should be fit 3 times by setting executions_per_trial = 3. This reduces variation in the results since there is an element of randomness in the initialisation of the model. A validation split of 20% was used and shuffle = T was used. Doing this shuffles the which observations get used as the validation set. This helps reduce the chances of overfitting.\nAll the results from tuning were saved into folders so that the results can be extracted and used for further analysis. The tuning was was undertaken is by no means exhaustive, no tuning can ever be, but the range of values tuned over is relatively small and therefore the results should be taken with a pinch of salt. With more time and perhaps more compute power a better result is possible.\nkerastuneR saves the tuning results as .json files. Each trial will be its own folder and inside that folder there will be a .json file containing information about the configuration of the Neural Network and the validation accuracy it achieved. The results were compiled into a single table containing the top 3 configurations from each of the predictor sets.\n\n\n\nValidation accuracy by hyperparameter configuration across four predictor sets\n\n\nSystematic hyperparameter tuning across four predictor sets revealed distinct performance patterns. As summarized in Table (tab:tuning_table), Predictor Set 2 (weather conditions) achieved the highest validation accuracy (66%), slightly outperforming Predictor Set 4 (all variables) at 65%. This suggests that weather variables capture the most critical signals for avalanche hazard forecasting, with topographic and snow-pack variables providing only marginal incremental value.\n\nTop hyperparameter configurations per predictor set, ranked by validation accuracy.\n\n\nPredictor set\nValidation accuracy\nLearning rate\nNumber of layers\nnodes on layer 1\nnodes on layer 2\nnodes on layer 3\nnodes on layer 4\nnodes on layer 5\n\n\n\n\n1\n0.50829\n0.01000\n4\n50\n30\n50\n50\nNA\n\n\n0.46535\n0.01000\n5\n30\n30\n50\n40\n40\n\n\n0.45376\n0.01000\n5\n30\n30\n30\n40\n30\n\n\n2\n0.65826\n0.00753\n5\n30\n40\n30\n30\n30\n\n\n0.65735\n0.00753\n5\n50\n50\n30\n30\n40\n\n\n0.65621\n0.01000\n4\n40\n30\n30\n40\nNA\n\n\n3\n0.62645\n0.01000\n3\n40\n40\n50\nNA\nNA\n\n\n0.62509\n0.01000\n4\n40\n50\n50\n30\nNA\n\n\n0.62463\n0.01000\n5\n40\n40\n30\n40\n40\n\n\n4\n0.64940\n0.00505\n4\n30\n40\n40\n40\nNA\n\n\n0.64781\n0.01000\n4\n50\n40\n50\n40\nNA\n\n\n0.64781\n0.01000\n5\n50\n40\n30\n50\n50\n\n\n\n\n\nArchitecturally, models with 4–5 layers and 30–50 nodes per layer consistently outperformed others, with no clear gains beyond 5 layers. The optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy.\n\nThe selected architecture (Figure (fig:best_model_plot)) utilizes ReLU activation in hidden layers and softmax output activation, appropriate for the multi-class ordinal nature of avalanche hazard prediction. This configuration represents an optimal balance between model complexity and predictive performance for the comprehensive feature set. This architecture balanced model complexity with predictive accuracy and was used for final evaluation on the test set.\n\nConfusion matrix for avalanche hazard predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted\n\n\n\nPredicted\n0\n1\n2\n3\n4\n\n\n\n\n0\n857\n420\n88\n15\n10\n\n\n1\n118\n326\n219\n51\n7\n\n\n2\n29\n231\n413\n186\n94\n\n\n3\n0\n4\n18\n20\n16\n\n\n4\n0\n0\n4\n7\n8\n\n\n\n\n\nThe confusion matrix of the the fitted model is reported above. It is clear that category 0 is the best estimated while categories 3 and 4 are estimated the worst. This result was expected since category 3 and 4 are very underepresented in both the training and test data. Concrete metrics of the model performance is given in the table below.\n\nComprehensive classification metrics by avalanche hazard class.\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: 0\n0.854\n0.751\n0.617\n0.916\n0.617\n0.854\n0.716\n0.320\n0.273\n0.443\n0.802\n\n\nClass: 1\n0.332\n0.817\n0.452\n0.729\n0.452\n0.332\n0.383\n0.312\n0.104\n0.230\n0.575\n\n\nClass: 2\n0.557\n0.775\n0.433\n0.850\n0.433\n0.557\n0.487\n0.236\n0.131\n0.303\n0.666\n\n\nClass: 3\n0.072\n0.987\n0.345\n0.916\n0.345\n0.072\n0.119\n0.089\n0.006\n0.018\n0.529\n\n\nClass: 4\n0.059\n0.996\n0.421\n0.959\n0.421\n0.059\n0.104\n0.043\n0.003\n0.006\n0.528\n\n\n\n\n\nSensitivity gives the percent of the time the model predicted an observation as belonging to a category and it actually belonging to that category. The sensitivity for class 0 is the best with a value of 0,854. There is a steep drop off for the other classes but class 2 is the second highest with a value of 0,557 and the rest are all below 0.5 meaning that more often than not, the model is unable to identify the correct category. An analogy for sensitivity is a test with high sensitivity(close to the maximum of 1) will identify most of the patients with with the flu as having the flu but this may mean that lots of patients without the flu also get identified as having the flu. An extreme case may be if the model predicted all observations as belonging to category 0. Then the sensitivity would be 1,meaning that all observations that belong to category 0 are predicted to be category 0. So there need to be a balance because we do not want this\nSpecificity gives the percent of time the an observation does not belong to a specific category and the model predicts it as not belonging to that category. High specificity(close to the maximum of 1) is analogous to a test rarely every flagging someone as having the flu when they don’t. A perfect model will have high sensitivity with a high specificity, meaning that it is able to identify when observations belong to a category and does not incorrectly predict other observations as belonging to that category. The table above indicates that all the categories have relatively high specificity. Class 4 has a specificity of 0.996 which on the surface looks great but because the sensitivity is so low, this high specificity just means that the model rarely every predicts any observations as belonging to category 4. The same follows for category 3. This is also seen in the extremely low detection rate and prevalence for these categories. These results, specifically for categories 3 and 4 are an indication that perhaps just reweighting the classes was not enough to overcome the imbalance in the data.\nA better metric for instances such as this where the data is imbalanced is the F1 score. The F1 score is the harmonic mean of the precision and recall. The precision is the percentage of observations that belonged to a category and were correctly predicted as belonging to that category. The recall is just the sensitivity. The F1 score, same as the other metrics, ranges from 0 to 1 with 0 being the worst and 1 being the best. The F1 score is useful because the harmonic mean because it is less affected by extreme values than the arithmetic mean. Categories 4 and 3 are by far the worst with values of 0,104 and 0,119. Category 0 has the best F1 score with a value of 0,716. Then category 1 has a value of 0,383 and category 3 has a score of 0,487. All these values, except for category are quite poor and indicate lack of predictive power. This is reflected in the fact that the model achieved an accuracy of 51,7% on the test set. The model is better than blindly guessing which we expect to return an accuracy of 20% but the model lacks predictive power. Expanding the search are for tuning as well as utalising bootstrapping may yield better results.\nThe model achieved a test accuracy of 51.7%, substantially higher than the 20% expected from random guessing, but still limited in predictive power.\nThe results highlight the challenges of severe class imbalance. While class reweighting improved performance somewhat, minority classes remained poorly predicted. This suggests that more aggressive strategies may be necessary, such as resampling (e.g., bootstrapping or SMOTE), expanding the hyperparameter search space, or exploring alternative architectures (e.g., convolutional or recurrent layers if spatial or temporal structure is relevant).\nDespite these limitations, the study provides useful insights. It demonstrates that weather variables are the dominant predictors of avalanche hazard and that neural networks can achieve moderate accuracy with relatively simple architectures. However, improving detection of rare but high-consequence hazard categories remains an important direction for future work."
  },
  {
    "objectID": "model_dt.html#extra-resources",
    "href": "model_dt.html#extra-resources",
    "title": "Model",
    "section": "",
    "text": "keras\nkerastuneR"
  },
  {
    "objectID": "model_dt.html#software-and-setup",
    "href": "model_dt.html#software-and-setup",
    "title": "Methodology",
    "section": "",
    "text": "Neural network modelling was implemented using the library, accessed through its R wrapper. Keras is an open-source deep learning framework known for its modularity and relatively gentle learning curve compared to other libraries such as PyTorch. Its modular design makes it well suited for projects that require experimentation with multiple configurations.\nBecause Keras functions in R act as wrappers around the underlying Python functions, version control is critical to ensure reproducibility. This project was developed in with ."
  },
  {
    "objectID": "model_dt.html#data-preparation",
    "href": "model_dt.html#data-preparation",
    "title": "Methodology",
    "section": "",
    "text": "The modelling goal was to predict . To prevent data leakage, the variable was excluded from the predictor sets, since this information would not be available in real-world forecasting scenarios.\nA was applied, with a fixed random seed to ensure reproducibility. Although a validation set is often held out, Keras provides internal validation handling, so this was not specified manually.\nTable 1 reports the class distribution across training and test sets. The proportions are broadly consistent between the two partitions, but the data exhibits strong class imbalance. Categories 0 and 1 each account for roughly 30% of the data, category 2 for 23%, while categories 3 and 4 are severely underrepresented (9% and 4% respectively). This imbalance risks biasing the model toward predicting majority classes.\nTo mitigate this, were introduced. The weights were set inversely proportional to the prevalence of each category in the training set, so that errors on minority classes carried higher penalties during training. Alternative methods such as bootstrap resampling could also be considered, but were not explored here.\n\n\n\n\nTable 1: Comparison of percentage of each category in the training and test sets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategory 0\ncategory 1\ncategory 2\ncategory 3\ncategory 4\n\n\n\n\ntrain\n0.328\n0.303\n0.236\n0.088\n0.044\n\n\ntest\n0.320\n0.312\n0.236\n0.089\n0.043\n\n\n\n\n\n\n\n\nThe table above reports the percentage of each category of FAH that is in the training and test data. It is important that the percentages are approximately equal in both sets and indeed this is the case. This is good but there is a problem. For the best results there should be approximately the same number of observations from each category in both the training and test sets, which is not the case for this data. Category 0 and 1 both are roughly the same at roughly 30% of total observations each. Category 2 is slightly lower with 23% but category 3 and 4 are hugely unrepresented in the data with only 9% and 4% respectively. This is an issue because big imbalances such as this will cause the model to optimise for correctly identifying observations from categories 0, 1 and 2.\nThere a few possible methods we can use to mitigate this issue but the method chosen for this project was to define custom class weights so that incorrect predictions have different penalties for each category. The idea is that underrepresented categories have higher weights so that in the training the model gets penalised more heavily and hopefully the final model is better at making predictions for the underpresented categories. The weights chosen were inversely proportional to the percentage of appearance in the training data. A possible area of further experimentation is using bootstrap sampling to equalise the percentages of each category."
  },
  {
    "objectID": "model_dt.html#predictor-sets",
    "href": "model_dt.html#predictor-sets",
    "title": "Methodology",
    "section": "",
    "text": "For this project there were 4 predictor sets that were evaluated. Predictor set 1 contained variables relating to the location of the observation. Predictor set 2 contained variables relating to the weather conditions of the observation. Predictor set 3 contained variables relating to a snow pack test. Predictor set 4 contained all the varibles in the dataset and thus contained all three predictor sets inside it. The final model will be built using all the available data, so predictor set 4. The remaining three predictor sets serve as yardstick to which we can compare which sets of variables are important."
  },
  {
    "objectID": "model_dt.html#model-development",
    "href": "model_dt.html#model-development",
    "title": "Methodology",
    "section": "",
    "text": "Neural networks are extremely flexible and configurable models. But this configurability means that there are several parameters that need to be defined before the model can be trained. In order to find the optimal configuration, hyperparameter tuning needs to be done first. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, the acitvation function to use on each layer, the dropout rate and the learning rate. keras is flexible and allows the user control of almost every aspect of the model through the parameters. In order to do the hyperameter tuning in R, the kerastuneR library is used. But in order to do the hyperparameter tuning with kerastuneR the model needs to be wrapped in a function that accepts as an input the hyperparameter configuration, and outputs a model.\nFor this project, it was decided to tune for the number of layers, number of nodes on each layer and the learning rate. The activation function was chosen to be rectified linear units(Relu) and the dropout rate was set to 0,1. Each dense layer was followed by a dropout layer. The specific values that were tuned across were 1-5 layers(step size of 1) with 30-50(step size of 10) nodes on each layer and 5 equally spaced learning rates [0,01  - 0,0001]. Therefore, there were a total of 75 unique models that could be fitted. The metric the Neural Network tries to minimize is the metric_categorical_accuracy. This was chosen since the target variable has more than 1 category."
  },
  {
    "objectID": "model_dt.html#hyperparameter-tuning",
    "href": "model_dt.html#hyperparameter-tuning",
    "title": "Methodology",
    "section": "",
    "text": "kerastuneR has multiple tuning algorithms, we have used the RandomSearch algorithm. RandomSearch takes random combinations of the provided hyperparameters and fits the model each time. Since the combination of hyperparameters is random, there is a possibility that the same mode configuration is ran multiple times by the algorithm. The algorithm does attempt to mitigate this but it not guaranteed to stop duplicate runs. We do have some control over this though by setting the max_trials variable to the total number of unique models that can be specified from our selected tuning ranges. It has also been specified that each model should be fit 3 times by setting executions_per_trial = 3. This reduces variation in the results since there is an element of randomness in the initialisation of the model. A validation split of 20% was used and shuffle = T was used. Doing this shuffles the which observations get used as the validation set. This helps reduce the chances of overfitting.\nAll the results from tuning were saved into folders so that the results can be extracted and used for further analysis. The tuning was was undertaken is by no means exhaustive, no tuning can ever be, but the range of values tuned over is relatively small and therefore the results should be taken with a pinch of salt. With more time and perhaps more compute power a better result is possible.\nkerastuneR saves the tuning results as .json files. Each trial will be its own folder and inside that folder there will be a .json file containing information about the configuration of the Neural Network and the validation accuracy it achieved. The results were compiled into a single table containing the top 3 configurations from each of the predictor sets."
  },
  {
    "objectID": "model_dt.html#tuning-outcomes",
    "href": "model_dt.html#tuning-outcomes",
    "title": "Methodology",
    "section": "Tuning Outcomes",
    "text": "Tuning Outcomes\n\n\n\nValidation accuracy by hyperparameter configuration across four predictor sets\n\n\nSystematic hyperparameter tuning across four predictor sets revealed distinct performance patterns. As summarized in Table 2, Predictor Set 2 (weather conditions) achieved the highest validation accuracy (66%), slightly outperforming Predictor Set 4 (all variables) at 65%. This suggests that weather variables capture the most critical signals for avalanche hazard forecasting, with topographic and snow-pack variables providing only marginal incremental value.\n\n\n\nTable 2: Top hyperparameter configurations per predictor set, ranked by validation accuracy.\n\n\n\n\n\nPredictor set\nValidation accuracy\nLearning rate\nNumber of layers\nnodes on layer 1\nnodes on layer 2\nnodes on layer 3\nnodes on layer 4\nnodes on layer 5\n\n\n\n\n1\n0.50829\n0.01000\n4\n50\n30\n50\n50\nNA\n\n\n0.46535\n0.01000\n5\n30\n30\n50\n40\n40\n\n\n0.45376\n0.01000\n5\n30\n30\n30\n40\n30\n\n\n2\n0.65826\n0.00753\n5\n30\n40\n30\n30\n30\n\n\n0.65735\n0.00753\n5\n50\n50\n30\n30\n40\n\n\n0.65621\n0.01000\n4\n40\n30\n30\n40\nNA\n\n\n3\n0.62645\n0.01000\n3\n40\n40\n50\nNA\nNA\n\n\n0.62509\n0.01000\n4\n40\n50\n50\n30\nNA\n\n\n0.62463\n0.01000\n5\n40\n40\n30\n40\n40\n\n\n4\n0.64940\n0.00505\n4\n30\n40\n40\n40\nNA\n\n\n0.64781\n0.01000\n4\n50\n40\n50\n40\nNA\n\n\n0.64781\n0.01000\n5\n50\n40\n30\n50\n50\n\n\n\n\n\n\n\n\nArchitecturally, models with 4–5 layers and 30–50 nodes per layer consistently outperformed others, with no clear gains beyond 5 layers. The optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy."
  },
  {
    "objectID": "model_dt.html#optimal-architecture",
    "href": "model_dt.html#optimal-architecture",
    "title": "Methodology",
    "section": "Optimal Architecture",
    "text": "Optimal Architecture\n\nThe selected architecture utilizes ReLU activation in hidden layers and softmax output activation, appropriate for the multi-class ordinal nature of avalanche hazard prediction. This configuration represents an optimal balance between model complexity and predictive performance for the comprehensive feature set. This architecture balanced model complexity with predictive accuracy and was used for final evaluation on the test set."
  },
  {
    "objectID": "model_dt.html#model-evaluation",
    "href": "model_dt.html#model-evaluation",
    "title": "Methodology",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nConfusion Matrix\n\nConfusion matrix for avalanche hazard predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted\n\n\n\nPredicted\n0\n1\n2\n3\n4\n\n\n\n\n0\n857\n420\n88\n15\n10\n\n\n1\n118\n326\n219\n51\n7\n\n\n2\n29\n231\n413\n186\n94\n\n\n3\n0\n4\n18\n20\n16\n\n\n4\n0\n0\n4\n7\n8\n\n\n\n\n\nThe confusion matrix of the the fitted model is reported above. It is clear that category 0 is the best estimated while categories 3 and 4 are estimated the worst. This result was expected since category 3 and 4 are very underepresented in both the training and test data. Concrete metrics of the model performance is given in the table below.\n\n\nClass-Level Metrics\n\nComprehensive classification metrics by avalanche hazard class.\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: 0\n0.854\n0.751\n0.617\n0.916\n0.617\n0.854\n0.716\n0.320\n0.273\n0.443\n0.802\n\n\nClass: 1\n0.332\n0.817\n0.452\n0.729\n0.452\n0.332\n0.383\n0.312\n0.104\n0.230\n0.575\n\n\nClass: 2\n0.557\n0.775\n0.433\n0.850\n0.433\n0.557\n0.487\n0.236\n0.131\n0.303\n0.666\n\n\nClass: 3\n0.072\n0.987\n0.345\n0.916\n0.345\n0.072\n0.119\n0.089\n0.006\n0.018\n0.529\n\n\nClass: 4\n0.059\n0.996\n0.421\n0.959\n0.421\n0.059\n0.104\n0.043\n0.003\n0.006\n0.528\n\n\n\n\n\nSensitivity gives the percent of the time the model predicted an observation as belonging to a category and it actually belonging to that category. The sensitivity for class 0 is the best with a value of 0,854. There is a steep drop off for the other classes but class 2 is the second highest with a value of 0,557 and the rest are all below 0.5 meaning that more often than not, the model is unable to identify the correct category. An analogy for sensitivity is a test with high sensitivity(close to the maximum of 1) will identify most of the patients with with the flu as having the flu but this may mean that lots of patients without the flu also get identified as having the flu. An extreme case may be if the model predicted all observations as belonging to category 0. Then the sensitivity would be 1,meaning that all observations that belong to category 0 are predicted to be category 0. So there need to be a balance because we do not want this\nSpecificity gives the percent of time the an observation does not belong to a specific category and the model predicts it as not belonging to that category. High specificity(close to the maximum of 1) is analogous to a test rarely every flagging someone as having the flu when they don’t. A perfect model will have high sensitivity with a high specificity, meaning that it is able to identify when observations belong to a category and does not incorrectly predict other observations as belonging to that category. The table above indicates that all the categories have relatively high specificity. Class 4 has a specificity of 0.996 which on the surface looks great but because the sensitivity is so low, this high specificity just means that the model rarely every predicts any observations as belonging to category 4. The same follows for category 3. This is also seen in the extremely low detection rate and prevalence for these categories. These results, specifically for categories 3 and 4 are an indication that perhaps just reweighting the classes was not enough to overcome the imbalance in the data.\nA better metric for instances such as this where the data is imbalanced is the F1 score. The F1 score is the harmonic mean of the precision and recall. The precision is the percentage of observations that belonged to a category and were correctly predicted as belonging to that category. The recall is just the sensitivity. The F1 score, same as the other metrics, ranges from 0 to 1 with 0 being the worst and 1 being the best. The F1 score is useful because the harmonic mean because it is less affected by extreme values than the arithmetic mean. Categories 4 and 3 are by far the worst with values of 0,104 and 0,119. Category 0 has the best F1 score with a value of 0,716. Then category 1 has a value of 0,383 and category 3 has a score of 0,487.\nAll these values, except for category are quite poor and indicate lack of predictive power. This is reflected in the fact that the model achieved an accuracy of 51,7% on the test set. The model is better than blindly guessing which we expect to return an accuracy of 20% but the model lacks predictive power."
  },
  {
    "objectID": "model_dt.html#interpretation",
    "href": "model_dt.html#interpretation",
    "title": "Methodology",
    "section": "Interpretation",
    "text": "Interpretation\nThe results highlight the challenges of severe class imbalance. While class reweighting improved performance somewhat, minority classes remained poorly predicted. This suggests that more aggressive strategies may be necessary, such as resampling (e.g., bootstrapping or SMOTE), expanding the hyperparameter search space, or exploring alternative architectures.\nDespite these limitations, the study provides useful insights. It demonstrates that weather variables are the dominant predictors of avalanche hazard and that neural networks can achieve moderate accuracy with relatively simple architectures. However, improving detection of rare but high-consequence hazard categories remains an important direction for future work."
  }
]