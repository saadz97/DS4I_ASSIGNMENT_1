[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "data_processing.html",
    "href": "data_processing.html",
    "title": "Data processing",
    "section": "",
    "text": "Data processing"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA",
    "section": "",
    "text": "EDA"
  },
  {
    "objectID": "index.html#avalanche-hazard-forecasting-with-neural-networks",
    "href": "index.html#avalanche-hazard-forecasting-with-neural-networks",
    "title": "Home",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "A neural network was fitted to the data in R, specifically R version 4.5.1 (2025-06-13) and python version 3.11.6. The reason it is important to specify the versions is because keras and its dependencies in R are wrappers of the python libraries so ensuring cross compatibility is imperative.\n\nlibrary(keras)\nlibrary(kerastuneR)\nlibrary(tensorflow)\nlibrary(dplyr)\nlibrary(tidyr)\n\nThe packages that will be used in the model building and tuning are given above. The packages can be installed using install.packages('package', dependencies = T). Note that after reading in the keras library a warning might appear saying that it is deprecated and that you should use keras3, this can be ignored for now as the provided code does not work when using keras3.\n\n# ensure that inside the folder you have the project in you also have a folder\n# called data that contains the data. \ndata = read.csv('./data/scotland_avalanche_forecasts_2009_2025.csv')\n\n# this is a very general solution to the problem of missing entries\n# maybe imputation?\ndata = drop_na(data)\ndata = filter(data, FAH != '', OAH != '', Precip.Code != '')\n\n## data description ##\n\n# Date = the date that the forecast was made\n# Area = one of six forecasting region\n# FAH = the forecast avalanche hazard for the following da\n# OAH = the observed avalanche hazard on the following day (observation made the following day)\n# longitude:Incline: position and topography at forecast location (predictor set)\n# Air.Temp:Summit.Wind.Speed = weather in the vicinity of the forecast location at the\n# time the forecast was made (predictor set 2)\n# Max.Temp.Grad:Snow.Temp = results of a ”snow pack test” of the integrity of snow at\n# the forecast location (predictor set 3)\n\n#unique(data['Area'])\n#unique(data['Obs'])\n#unique(data['FAH'])\n#unique(data['OAH'])\n#unique(data['Precip.Code'])\n\n\ndata$FAH         = as.integer(factor(x = data$FAH)) - 1\ndata$OAH         = as.integer(factor(x = data$OAH)) - 1\ndata$Precip.Code = as.integer(factor(data$Precip.Code)) - 1\n\npredictor_set_1 = select(data, c('longitude':'Incline'))\npredictor_set_1 = mutate(predictor_set_1, across(c(longitude : Incline), scale))\npredictor_set_1 = as.matrix(predictor_set_1)\n\npredictor_set_2 = select(data, c('Air.Temp':'Summit.Wind.Speed', 'FAH'))\npredictor_set_2 = mutate(predictor_set_2,\n                         across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))\npredictor_set_2 = as.matrix(predictor_set_2)\n\npredictor_set_3 = select(data, c('Max.Temp.Grad':'Snow.Temp', 'FAH'))\npredictor_set_3 = mutate(predictor_set_3, across(c(Max.Temp.Grad : Snow.Temp), scale))\npredictor_set_3 = as.matrix(predictor_set_3)\n\nSince we are trying to predict the forecasted avalanche hazard(FAH), it is important that we do not allow the model to be trained on the observed avalanche hazard(OAH). Therefore the predictor sets are explicitly defined so that there is no accidental data leakage. The FAH, OAH and Precip.Code variables are converted to integers so that the data is compatible with keras.\n\nset.seed(2025)\ntraining_indices = runif(n = floor(nrow(data) * 0.7), min = 1, max = nrow(data))\n\npredictor_set_1_train = predictor_set_1[training_indices, ] \npredictor_set_1_test  = predictor_set_1[-training_indices, ]\n\npredictor_set_2_train = predictor_set_2[training_indices, ] \npredictor_set_2_test  = predictor_set_2[-training_indices, ]\n\npredictor_set_3_train = predictor_set_3[training_indices, ] \npredictor_set_3_test  = predictor_set_3[-training_indices, ]\n\ntraining_data_list = list(predictor_set_1_train, predictor_set_2_train,\n                          predictor_set_3_train)\ntesting_data_list = list(predictor_set_1_test, predictor_set_2_test,\n                         predictor_set_3_test)\n\ny = data$FAH\ny = to_categorical(y, num_classes = 5)\n\ny_train = y[training_indices, ]\ny_test  = y[-training_indices, ]\n\nA 70/30 training split was used in the data. Usually we would set aside a small portion for validation but as we will see later, keras handles this for us so we do not need to specify a validation set. Using a seed for this step is important as it ensures the split remains every time the code is ran. This is the first step toward achiving repeatable results."
  },
  {
    "objectID": "model.html#model-building",
    "href": "model.html#model-building",
    "title": "Model",
    "section": "",
    "text": "A neural network was fitted to the data in R, specifically R version 4.5.1 (2025-06-13) and python version 3.11.6. The reason it is important to specify the versions is because keras and its dependencies in R are wrappers of the python libraries so ensuring cross compatibility is imperative.\n\nlibrary(keras)\nlibrary(kerastuneR)\nlibrary(tensorflow)\nlibrary(dplyr)\nlibrary(tidyr)\n\nThe packages that will be used in the model building and tuning are given above. The packages can be installed using install.packages('package', dependencies = T). Note that a warning might appear saying keras is deprecated and that you should use keras3, this can be ignored for now as the provided code does not work when using keras3.\n\n# ensure that inside the folder you have the project in you also have a folder\n# called data that contains the data. \ndata = read.csv('./data/scotland_avalanche_forecasts_2009_2025.csv')\n\n# this is a very general solution to the problem of missing entries\n# maybe imputation?\ndata = drop_na(data)\ndata = filter(data, FAH != '', OAH != '', Precip.Code != '')\n\n## data description ##\n\n# Date = the date that the forecast was made\n# Area = one of six forecasting region\n# FAH = the forecast avalanche hazard for the following da\n# OAH = the observed avalanche hazard on the following day (observation made the following day)\n# longitude:Incline: position and topography at forecast location (predictor set)\n# Air.Temp:Summit.Wind.Speed = weather in the vicinity of the forecast location at the\n# time the forecast was made (predictor set 2)\n# Max.Temp.Grad:Snow.Temp = results of a ”snow pack test” of the integrity of snow at\n# the forecast location (predictor set 3)\n\n#unique(data['Area'])\n#unique(data['Obs'])\n#unique(data['FAH'])\n#unique(data['OAH'])\n#unique(data['Precip.Code'])\n\n\ndata$FAH         = as.integer(factor(x = data$FAH)) - 1\ndata$OAH         = as.integer(factor(x = data$OAH)) - 1\ndata$Precip.Code = as.integer(factor(data$Precip.Code)) - 1\n\n\npredictor_set_1 = select(data, c('longitude':'Incline'))\npredictor_set_1 = mutate(predictor_set_1, across(c(longitude : Incline), scale))\npredictor_set_1 = as.matrix(predictor_set_1)\n\npredictor_set_2 = select(data, c('Air.Temp':'Summit.Wind.Speed', 'FAH'))\npredictor_set_2 = mutate(predictor_set_2,\n                         across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))\npredictor_set_2 = as.matrix(predictor_set_2)\n\npredictor_set_3 = select(data, c('Max.Temp.Grad':'Snow.Temp', 'FAH'))\npredictor_set_3 = mutate(predictor_set_3, across(c(Max.Temp.Grad : Snow.Temp), scale))\npredictor_set_3 = as.matrix(predictor_set_3)\n\n\nset.seed(2025)\ntraining_indices = runif(n = floor(nrow(data) * 0.7), min = 1, max = nrow(data))\n\npredictor_set_1_train = predictor_set_1[training_indices, ] \npredictor_set_1_test  = predictor_set_1[-training_indices, ]\n\npredictor_set_2_train = predictor_set_2[training_indices, ] \npredictor_set_2_test  = predictor_set_2[-training_indices, ]\n\npredictor_set_3_train = predictor_set_3[training_indices, ] \npredictor_set_3_test  = predictor_set_3[-training_indices, ]\n\ntraining_data_list = list(predictor_set_1_train, predictor_set_2_train,\n                          predictor_set_3_train)\ntesting_data_list = list(predictor_set_1_test, predictor_set_2_test,\n                         predictor_set_3_test)\n\ny = data$FAH\ny = to_categorical(y, num_classes = 5)\n\ny_train = y[training_indices, ]\ny_test  = y[-training_indices, ]\n\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 2, max_value = 10, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-1, 1e-2, 1e-3))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n\n  x = input\n  for (i in 1:n_layers){\n    x = x %&gt;%\n      layer_dense(units = 20, activation = 'relu') %&gt;%\n      layer_dropout(rate = 0.5)\n  }\n  output = x %&gt;% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %&gt;% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c('accuracy'))\n  \n  return(model)\n}\n\n\nfor (i in 1){\n  \n  x_train = training_data_list[[i]]\n  \n  tuner_randomsearch = kerastuneR::RandomSearch(hypermodel = model_builder,\n                                                objective = 'val_accuracy',\n                                                max_trials = 50, \n                                                executions_per_trial = 3,\n                                                directory = 'tuning',\n                                                project_name = paste('randomsearch results', i),\n                                                overwrite = TRUE)\n  \n  tuner_randomsearch %&gt;% fit_tuner(x = x_train,\n                                   y = y_train,\n                                   epochs = 50,\n                                   validation_split = 0.2, shuffle = TRUE)\n  \n  #results_summary(tuner = tuner_randomsearch, num_trials = 5)\n  \n  tuner_hyperband = kerastuneR::Hyperband(hypermodel = model_builder,\n                                          objective = 'val_accuracy',\n                                          directory = 'tuning',\n                                          project_name = paste('hyperband results', i),\n                                          max_epochs = 50,\n                                          hyperband_iterations = 10,\n                                          seed = 2025)\n  \n  tuner_hyperband %&gt;% fit_tuner(x = x_train,\n                                y = y_train,\n                                epochs = 50,\n                                validation_split = 0.2, shuffle = TRUE)\n  \n  #results_summary(tuner = tuner_hyperband, num_trials = 5)\n}"
  },
  {
    "objectID": "model.html#getting-set-up",
    "href": "model.html#getting-set-up",
    "title": "Model",
    "section": "",
    "text": "A neural network was fitted to the data in R, specifically R version 4.5.1 (2025-06-13) and python version 3.11.6. The reason it is important to specify the versions is because keras and its dependencies in R are wrappers of the python libraries so ensuring cross compatibility is imperative.\n\nlibrary(keras)\nlibrary(kerastuneR)\nlibrary(tensorflow)\nlibrary(dplyr)\nlibrary(tidyr)\n\nThe packages that will be used in the model building and tuning are given above. The packages can be installed using install.packages('package', dependencies = T). Note that after reading in the keras library a warning might appear saying that it is deprecated and that you should use keras3, this can be ignored for now as the provided code does not work when using keras3.\n\n# ensure that inside the folder you have the project in you also have a folder\n# called data that contains the data. \ndata = read.csv('./data/scotland_avalanche_forecasts_2009_2025.csv')\n\n# this is a very general solution to the problem of missing entries\n# maybe imputation?\ndata = drop_na(data)\ndata = filter(data, FAH != '', OAH != '', Precip.Code != '')\n\n## data description ##\n\n# Date = the date that the forecast was made\n# Area = one of six forecasting region\n# FAH = the forecast avalanche hazard for the following da\n# OAH = the observed avalanche hazard on the following day (observation made the following day)\n# longitude:Incline: position and topography at forecast location (predictor set)\n# Air.Temp:Summit.Wind.Speed = weather in the vicinity of the forecast location at the\n# time the forecast was made (predictor set 2)\n# Max.Temp.Grad:Snow.Temp = results of a ”snow pack test” of the integrity of snow at\n# the forecast location (predictor set 3)\n\n#unique(data['Area'])\n#unique(data['Obs'])\n#unique(data['FAH'])\n#unique(data['OAH'])\n#unique(data['Precip.Code'])\n\n\ndata$FAH         = as.integer(factor(x = data$FAH)) - 1\ndata$OAH         = as.integer(factor(x = data$OAH)) - 1\ndata$Precip.Code = as.integer(factor(data$Precip.Code)) - 1\n\npredictor_set_1 = select(data, c('longitude':'Incline'))\npredictor_set_1 = mutate(predictor_set_1, across(c(longitude : Incline), scale))\npredictor_set_1 = as.matrix(predictor_set_1)\n\npredictor_set_2 = select(data, c('Air.Temp':'Summit.Wind.Speed', 'FAH'))\npredictor_set_2 = mutate(predictor_set_2,\n                         across(c(Air.Temp : Summit.Wind.Speed, - Precip.Code), scale))\npredictor_set_2 = as.matrix(predictor_set_2)\n\npredictor_set_3 = select(data, c('Max.Temp.Grad':'Snow.Temp', 'FAH'))\npredictor_set_3 = mutate(predictor_set_3, across(c(Max.Temp.Grad : Snow.Temp), scale))\npredictor_set_3 = as.matrix(predictor_set_3)\n\nSince we are trying to predict the forecasted avalanche hazard(FAH), it is important that we do not allow the model to be trained on the observed avalanche hazard(OAH). Therefore the predictor sets are explicitly defined so that there is no accidental data leakage. The FAH, OAH and Precip.Code variables are converted to integers so that the data is compatible with keras.\n\nset.seed(2025)\ntraining_indices = runif(n = floor(nrow(data) * 0.7), min = 1, max = nrow(data))\n\npredictor_set_1_train = predictor_set_1[training_indices, ] \npredictor_set_1_test  = predictor_set_1[-training_indices, ]\n\npredictor_set_2_train = predictor_set_2[training_indices, ] \npredictor_set_2_test  = predictor_set_2[-training_indices, ]\n\npredictor_set_3_train = predictor_set_3[training_indices, ] \npredictor_set_3_test  = predictor_set_3[-training_indices, ]\n\ntraining_data_list = list(predictor_set_1_train, predictor_set_2_train,\n                          predictor_set_3_train)\ntesting_data_list = list(predictor_set_1_test, predictor_set_2_test,\n                         predictor_set_3_test)\n\ny = data$FAH\ny = to_categorical(y, num_classes = 5)\n\ny_train = y[training_indices, ]\ny_test  = y[-training_indices, ]\n\nA 70/30 training split was used in the data. Usually we would set aside a small portion for validation but as we will see later, keras handles this for us so we do not need to specify a validation set. Using a seed for this step is important as it ensures the split remains every time the code is ran. This is the first step toward achiving repeatable results."
  },
  {
    "objectID": "model.html#defining-the-model",
    "href": "model.html#defining-the-model",
    "title": "Model",
    "section": "Defining the model",
    "text": "Defining the model\nAt this stage we would define the model and let keras do it’s thing. But before we can define the model we need to find the optimal specification of model hyperparameters. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, acitvation function to use on each layer, the dropout rate and the learning rate. keras is hightly flexible and allows the user control of almost every aspect of the model parameters. To find the optimal value for these hyperparameters we do hyperparameter tuning. In order to do this in R we need to use the kerastuneR library. In order to do the hyperparameter tuning with kerastuneR the model needs to be wrapped in a function that accepts as an input the hyperparameter specification.\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 2, max_value = 10, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-1, 1e-2, 1e-3))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n\n  x = input\n  for (i in 1:n_layers){\n    x = x %&gt;%\n      layer_dense(units = 20, activation = 'relu') %&gt;%\n      layer_dropout(rate = 0.5)\n  }\n  output = x %&gt;% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %&gt;% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c('accuracy'))\n  \n  return(model)\n}\n\nFor this project, we have decided to tune for the number of layers and the learning rate. The number of nodes on each layer was chosen to be 20, the activation function to be rectified linear units(Relu) and the dropout rate was sett to 0.5. The specific values were 2-10 layers and learning rates \\(\\in \\{0.1,\\ 0.01,\\ 0.001 \\}\\). Therefore, there were a total of 27 unique models that could be fitted."
  },
  {
    "objectID": "model.html#hyperparameter-tuning",
    "href": "model.html#hyperparameter-tuning",
    "title": "Model",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\n\nfor (i in 1){\n  \n  x_train = training_data_list[[i]]\n  \n  tuner_randomsearch = kerastuneR::RandomSearch(hypermodel = model_builder,\n                                                objective = 'val_accuracy',\n                                                max_trials = 50, \n                                                executions_per_trial = 3,\n                                                directory = 'tuning',\n                                                project_name = paste('randomsearch results', i),\n                                                overwrite = TRUE)\n  \n  tuner_randomsearch %&gt;% fit_tuner(x = x_train,\n                                   y = y_train,\n                                   epochs = 50,\n                                   validation_split = 0.2, shuffle = TRUE)\n  \n  #results_summary(tuner = tuner_randomsearch, num_trials = 5)\n  \n  tuner_hyperband = kerastuneR::Hyperband(hypermodel = model_builder,\n                                          objective = 'val_accuracy',\n                                          directory = 'tuning',\n                                          project_name = paste('hyperband results', i),\n                                          max_epochs = 50,\n                                          hyperband_iterations = 10,\n                                          seed = 2025)\n  \n  tuner_hyperband %&gt;% fit_tuner(x = x_train,\n                                y = y_train,\n                                epochs = 50,\n                                validation_split = 0.2, shuffle = TRUE)\n  \n  #results_summary(tuner = tuner_hyperband, num_trials = 5)\n}\n\nkerastuneR has multiple tuning algorithms, we have used the RandomSearch and Hyperband algorithms. RandomSearch takes random combinations of the provided hyperparameters and fits the model each time. Since the combination of hyperparameters is random there is a possibility that the same mode specification is ran multiple times by the algorithm, however, the algorithm does attempt to mitigate this. We can also control this by setting the max_trials variable to a number larger than the total number of unique models that can be specified. We also specify that each model should be fit 3 times by setting executions_per_trial = 3. This just reduces variation in the results. The Hyperband algorithm aims to improve the efficiency of the tuning by utalizing efficient resource allocation. It is a bandit-based approach to hyperparameter optimisation(need to cite a source here). For this algorithm max_epochs was set to 50, this decided by trial and error. It just needs to be set to a value large enough to ensure convergence in all models. hyperband_iterations is the number of times to iterate over the full hyperband algorithm. It is suggested to set this value as high as your machinery can handle. We set it to 10 but it can be set higher. In both algorithms, we use a validation split of 20% and setting shuffle = T. Doing this shuffles the training data after each eopch and helps reduce the chances of overfitting."
  },
  {
    "objectID": "model.html#defining-the-model-builder-wrapper-function",
    "href": "model.html#defining-the-model-builder-wrapper-function",
    "title": "Model",
    "section": "Defining the model builder wrapper function",
    "text": "Defining the model builder wrapper function\nAt this stage we would define the model and let keras do it’s thing. But before we can define the model we need to find the optimal specification of model hyperparameters. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, acitvation function to use on each layer, the dropout rate and the learning rate. keras is hightly flexible and allows the user control of almost every aspect of the model parameters. To find the optimal value for these hyperparameters we do hyperparameter tuning. In order to do this in R we need to use the kerastuneR library. In order to do the hyperparameter tuning with kerastuneR the model needs to be wrapped in a function that accepts as an input the hyperparameter specification.\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 2, max_value = 10, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-1, 1e-2, 1e-3))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n\n  x = input\n  for (i in 1:n_layers){\n    x = x %&gt;%\n      layer_dense(units = 20, activation = 'relu') %&gt;%\n      layer_dropout(rate = 0.5)\n  }\n  output = x %&gt;% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %&gt;% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c('accuracy'))\n  \n  return(model)\n}\n\nFor this project, we have decided to tune for the number of layers and the learning rate. The number of nodes on each layer was chosen to be 20, the activation function to be rectified linear units(Relu) and the dropout rate was sett to 0.5. The specific values were 2-10 layers and learning rates \\(\\in \\{0.1,\\ 0.01,\\ 0.001 \\}\\). Therefore, there were a total of 27 unique models that could be fitted."
  },
  {
    "objectID": "model.html#extra-resources",
    "href": "model.html#extra-resources",
    "title": "Model",
    "section": "Extra resources",
    "text": "Extra resources\nkeras\nkerastuneR"
  }
]