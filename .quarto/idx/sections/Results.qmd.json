{"title":"3. Results","markdown":{"yaml":{"title":"","format":"html","execute":{"echo":true,"warning":false,"message":false}},"headingText":"3. Results","headingAttr":{"id":"sec-Results","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n```{r, echo = F}\nlibrary(kableExtra)\n```\n\n\n\n\n\n\n```{r packages and libraries, eval=TRUE, include =F}\nlibrary(keras)\nlibrary(kerastuneR)\nlibrary(tensorflow)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate)\nlibrary(caret)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\n```\n\n```{r data}\nload('../data/training_data.RData')\nload('../data/testing_data.RData')\n\nload('../data/y_train.RData')\nload('../data/y_test.RData')\n\ny_train = to_categorical(y_train, num_classes = 5)\ny_test  = to_categorical(y_test, num_classes = 5)\n\ncomp_mat = matrix(c(colSums(y_train) / nrow(y_train), \n                    colSums(y_test) / nrow(y_test)),\n                  byrow = T, nrow = 2)\ncolnames(comp_mat) = paste('category ', 0:4)\nrownames(comp_mat) = c('train', 'test')\nsave(comp_mat, file = 'test/category_imbalance.RData')\n```\n\n```{r imbalance_matrix, eval=TRUE, include = F}\n#| label: tbl-imbalance\nload('../test/category_imbalance.RData')\nkable(comp_mat, digits = 3, caption = 'Comparison of percentage of each category in the training and test sets')\n```\n\n```{r model building function, include = F}\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 5, step = 1)\n  lr = hp$Choice('learning_rate', values = seq(from = 1e-2, to = 1e-4, length.out = 5))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n\n  x = input\n  for (i in 1:n_layers){\n    \n    n_nodes = hp$Int(paste0('nodes_layer_', i),\n                      min_value = 30, max_value = 50, step = 10)\n    \n    x = x %>%\n      layer_dense(units = n_nodes, activation = 'relu') %>%\n      layer_dropout(rate = 0.1)\n  }\n  output = x %>% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %>% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c(metric_categorical_accuracy()))\n  \n  return(model)\n}\n```\n\n```{r model tuning, include = F}\nfor (i in 1:4){\n  \n  freqs = colSums(y_train) / nrow(y_train)\n  weights = sqrt(1 / freqs)\n  class_weights = dict()\n  for (k in 0:(length(weights)-1)) {\n    class_weights[[k]] = weights[k + 1]\n  }\n    \n  x_train = training_data_list[[i]]\n  \n  tuner_randomsearch = kerastuneR::RandomSearch(hypermodel = model_builder,\n                                                objective = 'val_categorical_accuracy',\n                                                max_trials = 75, executions_per_trial = 3,\n                                                directory = 'tuning',\n                                                project_name = paste('randomsearch results', i),\n                                                overwrite = TRUE)\n  \n  tuner_randomsearch %>% fit_tuner(x = x_train,\n                                   y = y_train,\n                                   epochs = 100,\n                                   batch_size = 32,\n                                   class_weight = class_weights,\n                                   validation_split = 0.2,\n                                   shuffle = TRUE)\n}\n```\n\n\n# Results\n\nThis section presents the comprehensive outcomes of our neural network approach to avalanche hazard prediction. We begin by examining the performance hierarchy across different predictor sets, followed by detailed analysis of optimal hyperparameter configurations. The section culminates in an evaluation of the final model's predictive capability, with particular attention to class-wise performance across the five avalanche hazard levels.\n\n## Hyperparameter Tuning Outcomes\n\n```{r}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(jsonlite)\n\ntuning_results = function(directory, max_layers){\n  \n  json_path = paste0(directory, '/trial.json')\n  tuning_results = fromJSON(txt = json_path)\n  hp               = tuning_results$hyperparameters$values\n  number_of_layers = hp$number_of_layers\n  learning_rate    = hp$learning_rate\n  score            = tuning_results$score\n  \n  layers_nodes     = matrix(rep(NA, max_layers), nrow = 1)\n  colnames(layers_nodes) = paste('nodes on layer ', 1:max_layers)\n  for (i in 1:number_of_layers) {\n    node_name = paste0('nodes_layer_', i)\n    layers_nodes[1, i] = hp[[node_name]]\n  }\n  \n  row_df = data.frame( Val_accuracy = score,  LR = learning_rate, \n                       layers = number_of_layers, layers_nodes, \n                       check.names = FALSE)\n  return(row_df)\n}\n\n# Test\n# tuning_results('tuning/randomsearch results 1/trial_19', 5)\n\ndirectories              = list.dirs('tuning', recursive = FALSE)\nrandomsearch_search      = 'randomsearch'\nrandomsearch_directories = grep(randomsearch_search, directories, value = TRUE)\n\nresults_compiler = function(method_directory){\n\n  for (i in 1:length(method_directory)){\n    trial_directories = list.dirs(method_directory[i])\n    results_df = do.call(rbind, lapply(trial_directories[-1], tuning_results, max_layers = 5))\n    results_df = arrange(results_df, desc(Val_accuracy))\n    save(results_df, file = paste0(method_directory[i], '/summary.RData'))\n  }\n}\n\n# Test\n#results_compiler('tuning/randomsearch results 1')\n\n# the results automatically get saved locally, inside the same tuning folder\nresults_compiler(randomsearch_directories)\n```\n\n\n```{r}\nlibrary(dplyr)\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(reticulate)\nlibrary(caret)\nlibrary(knitr)\nlibrary(kableExtra)\n\nfiles = c('tuning/randomsearch results 1/summary.RData',\n          'tuning/randomsearch results 2/summary.RData',\n          'tuning/randomsearch results 3/summary.RData',\n          'tuning/randomsearch results 4/summary.RData')\nvar_names = c('rs_results_1', 'rs_results_2', 'rs_results_3', 'rs_results_4')\n\nfor (i in seq_along(files)){\n  temp_env = new.env()                       # temporary environment\n  load(files[i], envir = temp_env)           # load into temp\n  assign(var_names[i], temp_env$results_df)  # assign with custom name\n}\n\nresults = list(rs_results_1, rs_results_2, rs_results_3, rs_results_4)\ndf = data.frame()\nfor (i in 1:4){\n  results_df = results[[i]]\n  top_3_models = results_df[1:3, ]\n  top_3_models = mutate(top_3_models, across(c(Val_accuracy, LR), round, 5))  \n  colnames(top_3_models) = c('Validation accuracy',\n                             'Learning rate',\n                             'Number of layers',\n                             paste('nodes on layer ', 1:5)) # make sure to make this the maximum layers\n  df = bind_rows(df, top_3_models)\n}\ndf = mutate(df, 'Predictor set' = rep(1:4, each = 3), .before = 'Validation accuracy')\n\nsave(df, file = 'tuning/tuning_summary_table.RData')\n```\n\n```{r myplot, results='asis', eval=TRUE, fig.cap=\"Validation accuracy by hyperparameter configuration across four predictor sets\"}\n\n#| label: fig-myplot\n#| \nload('../tuning/tuning_summary_table.RData')\n\n### The Tuning plot\nnames(df) <- gsub(\" \", \"_\", names(df))\n\n\nset.seed(1)  # for reproducibility of shuffle\n\ntop_10_shuffled <- df %>%\n  group_by(Predictor_set) %>%\n  sample_frac(1) %>%  # shuffle rows within each Predictor_set\n  ungroup() %>%\n  rowwise() %>%\n  mutate(\n    combo_label = paste0(\n      \"LR=\", Learning_rate,\n      \", Layers=\", Number_of_layers,\n      \", Nodes=[\", \n      paste(na.omit(c_across(starts_with(\"nodes_on_layer_\"))), collapse = \", \"),\n      \"]\"\n    )\n  ) %>%\n  ungroup()\n\n# Now reorder combo_label factor so predictor sets are grouped but shuffled inside\ntop_10_shuffled <- top_10_shuffled %>%\n  arrange(Predictor_set) %>%  # predictor sets grouped in order\n  mutate(combo_label = factor(combo_label, levels = unique(combo_label)))\n\n# Find optimal points per predictor set (same as before)\noptimal_points <- top_10_shuffled %>%\n  group_by(Predictor_set) %>%\n  filter(Validation_accuracy == max(Validation_accuracy)) %>%\n  ungroup()\n\nmy_colors <- c(\n  \"1\" = \"#e41a1c\",  # bright red\n  \"2\" = \"#377eb8\",  # strong blue\n  \"3\" = \"#4daf4a\",  # vivid green\n  \"4\" = \"#ff7f00\"   # bright orange\n)\n\n\n# Plot\nggplot(top_10_shuffled, aes(x = combo_label, y = Validation_accuracy, color = factor(Predictor_set))) +\n  geom_point(size = 3) +\n  geom_point(data = optimal_points, aes(x = combo_label, y = Validation_accuracy),\n             color = \"black\", size = 5, shape = 8) +\n  geom_point(size = 3)+\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 70, hjust = 1),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  xlab(\"Hyperparameter Combination\") +\n  ylab(\"Validation Accuracy\") +\n  scale_color_manual(values = my_colors, name = \"Predictor Set\")\n###############\n\n\n\n```\n\nSystematic hyperparameter tuning across four predictor sets revealed distinct performance patterns, visualized in the above figure. The plot shows validation accuracy distributions for various architectural configurations, with Predictor Set 2 (weather conditions) achieving the highest peak performance (~66%), closely followed by Predictor Set 4 (all variables) at ~65%.\n\n\n@tbl-tuning details the top three configurations for each predictor set, confirming the performance hierarchy. The marginal improvement of Set 2 over Set 4 indicates that weather variables capture the most critical forecasting signals, with topographic and snow-pack data providing limited incremental value.\n\n\n\n```{r tuning-table, results='asis', eval=TRUE}\n#| label: tbl-tuning\nload('../tuning/tuning_summary_table.RData')\n\nsummary_table = kable(df, booktabs = TRUE, caption = \"Top hyperparameter configurations per predictor set, ranked by validation accuracy.\") %>% \n  kable_styling(full_width = FALSE, position = 'center') %>%\n  collapse_rows(columns = 1, valign = 'middle')\n\nfor (i in seq(3, nrow(df), by = 3)) {\n  summary_table = row_spec(summary_table, i, \n                           extra_css = \"border-bottom: 2px solid black;\")\n}\n\nsummary_table\n```\n\nArchitecturally, models with 4–5 layers and 30–50 nodes per layer consistently outperformed others, with no clear gains beyond 5 layers. The optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with \\[30, 40, 40, 40\\] nodes and achieved 64.9% validation accuracy.\n\nThe optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy, representing the best balance of performance and generalizability.\n\n## Optimal Architecture\n\n```{r best_model_plot, fig.cap=\"Optimal neural network architecture\"}\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 3, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-2, 1e-3, 1e-4))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n  \n  x = input\n  for (i in 1:n_layers){\n    \n    n_nodes = hp$Int(paste0('nodes_layer_', i),\n                     min_value = 30, max_value = 50, step = 10)\n    \n    x = x %>%\n      layer_dense(units = n_nodes, activation = 'relu') %>%\n      layer_dropout(rate = 0.1)\n  }\n  output = x %>% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %>% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c(metric_categorical_accuracy()))\n  \n  return(model)\n}\n\ntuner = kerastuneR::RandomSearch(hypermodel = model_builder, \n                                 objective = 'val_categorical_accuracy',\n                                 max_trials = 1, \n                                 executions_per_trial = 1,\n                                 directory = 'tuning',\n                                 project_name = 'randomsearch results 4')\n\ntuner$reload()\nbest_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] \n\ntuner$reload()\nbest_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] \n\ntf = tensorflow::tf\nplot_model = tf$keras$utils$plot_model\nplot_model(best_model, show_shapes = TRUE, show_layer_names = TRUE,\n           expand_nested = FALSE,\n           show_layer_activations = TRUE,\n           dpi = 500,\n           to_file = 'best_model_plot.png')\n```\n\n<img src=\"best_model_plot.png\" alt=\"Configuration of the best model\" width=\"500\"/>\n\nThe selected architecture utilizes ReLU activation in hidden layers and softmax output activation, appropriate for the multi-class ordinal nature of avalanche hazard prediction. This configuration represents an optimal balance between model complexity and predictive performance for the comprehensive feature set. This architecture balanced model complexity with predictive accuracy and was used for final evaluation on the test set.\n\n## Model Evaluation \n\n### Confusion Matrix\n\n\n```{r predictions}\n\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 3, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-2, 1e-3, 1e-4))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n  \n  x = input\n  for (i in 1:n_layers){\n    \n    n_nodes = hp$Int(paste0('nodes_layer_', i),\n                     min_value = 30, max_value = 50, step = 10)\n    \n    x = x %>%\n      layer_dense(units = n_nodes, activation = 'relu') %>%\n      layer_dropout(rate = 0.1)\n  }\n  output = x %>% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %>% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c(metric_categorical_accuracy()))\n  \n  return(model)\n}\n\ntuner = kerastuneR::RandomSearch(hypermodel = model_builder, \n                                 objective = 'val_categorical_accuracy',\n                                 max_trials = 1, \n                                 executions_per_trial = 1,\n                                 directory = 'tuning',\n                                 project_name = 'randomsearch results 4')\n\ntuner$reload()\nbest_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] \n\nresults      = best_model %>% evaluate(x_test, y_test)\ny_pred_probs = best_model %>% predict(x_test)\ny_pred       = max.col(y_pred_probs) - 1\ntrue_classes = apply(y_test, 1, which.max) - 1 # it was one-hot encoded so i changed it back\nmetrics_list = confusionMatrix(factor(y_pred), factor(true_classes))\n\ncon_mat = as.data.frame(metrics_list$table)\ncon_mat = rename(con_mat, Predicted = Prediction, Reference = Reference, Count = Freq)\ncon_mat =  tidyr::pivot_wider(con_mat, names_from = Reference, values_from = Count,\n                              values_fill = 0)\nsave(con_mat, file = 'test/con_mat.RData')\nsave(metrics_mat, file = 'test/metrics_mat.RData')\n```\n\n```{r confusion, results='asis', eval=TRUE}\n#| label: tbl-confusion\nload('../test/con_mat.RData')\nkable(con_mat, format = 'html', caption = \"Confusion matrix for avalanche hazard predictions.\") %>%\n  kable_styling(full_width = FALSE, position = 'center') %>%\n  add_header_above(c(' ' = 1, 'Predicted' = 5))\n```\n\nThe confusion matrix of the the fitted model is reported in @tbl-confusion. It is clear that category 0 is the best estimated while categories 3 and 4 are estimated the worst. This result was expected since category 3 and 4 are very underepresented in both the training and test data. Concrete metrics of the model performance is given in the table below.\n\n### Class-Level Metrics\n\n```{r metrics matrix, results='asis', eval=TRUE}\n#| label: tbl-class\n\nload('../test/metrics_mat.RData')\nkable(metrics_mat, digits = 3, caption = \"Comprehensive classification metrics by avalanche hazard class.\") %>%\n  kable_styling(full_width = FALSE, position = 'center')\n```\n\nSensitivity gives the percent of the time the model predicted an observation as belonging to a category and it actually belonging to that category. The sensitivity for class 0 is the best with a value of 0,854. There is a steep drop off for the other classes but class 2 is the second highest with a value of 0,557 and the rest are all below 0.5 meaning that more often than not, the model is unable to identify the correct category. An analogy for sensitivity is a test with high sensitivity(close to the maximum of 1) will identify most of the patients with with the flu as having the flu but this may mean that lots of patients without the flu also get identified as having the flu. An extreme case may be if the model predicted all observations as belonging to category 0. Then the sensitivity would be 1,meaning that all observations that belong to category 0 are predicted to be category 0. So there need to be a balance because we do not want this\n\nSpecificity gives the percent of time the an observation does not belong to a specific category and the model predicts it as not belonging to that category. High specificity(close to the maximum of 1) is analogous to a test rarely every flagging someone as having the flu when they don't. A perfect model will have high sensitivity with a high specificity, meaning that it is able to identify when observations belong to a category and does not incorrectly predict other observations as belonging to that category. The table above indicates that all the categories have relatively high specificity. Class 4 has a specificity of 0.996 which on the surface looks great but because the sensitivity is so low, this high specificity just means that the model rarely every predicts any observations as belonging to category 4. The same follows for category 3. This is also seen in the extremely low detection rate and prevalence for these categories. These results, specifically for categories 3 and 4 are an indication that perhaps just reweighting the classes was not enough to overcome the imbalance in the data.\n\nA better metric for instances such as this where the data is imbalanced is the F1 score. The F1 score is the harmonic mean of the precision and recall. The precision is the percentage of observations that belonged to a category and were correctly predicted as belonging to that category. The recall is just the sensitivity. The F1 score, same as the other metrics, ranges from 0 to 1 with 0 being the worst and 1 being the best. The F1 score is useful because the harmonic mean because it is less affected by extreme values than the arithmetic mean. Categories 4 and 3 are by far the worst with values of 0,104 and 0,119. Category 0 has the best F1 score with a value of 0,716. Then category 1 has a value of 0,383 and category 3 has a score of 0,487.\n\n\nAll these values, except for category are quite poor and indicate lack of predictive power. This is reflected in the fact that the model achieved an accuracy of 51,7% on the test set. The model is better than blindly guessing which we expect to return an accuracy of 20% but the model lacks predictive power.\n\n\n\n## Interpretation\n\nThe results highlight the challenges of severe class imbalance. While class reweighting improved performance somewhat, minority classes remained poorly predicted. This suggests that more aggressive strategies may be necessary, such as resampling (e.g., bootstrapping or SMOTE), expanding the hyperparameter search space, or exploring alternative architectures.\n\nDespite these limitations, the study provides useful insights. It demonstrates that weather variables are the dominant predictors of avalanche hazard and that neural networks can achieve moderate accuracy with relatively simple architectures. However, improving detection of rare but high-consequence hazard categories remains an important direction for future work.\n\n\n# Extra resources\n\n[`keras`](https://cran.r-project.org/web/packages/keras/vignettes/){target=\"_blank\"}\n\n[`kerastuneR`](https://eagerai.github.io/kerastuneR/){target=\"_blank\"}\n\n\n","srcMarkdownNoYaml":"\n\n```{r, echo = F}\nlibrary(kableExtra)\n```\n\n\n\n## 3. Results {#sec-Results}\n\n\n\n```{r packages and libraries, eval=TRUE, include =F}\nlibrary(keras)\nlibrary(kerastuneR)\nlibrary(tensorflow)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate)\nlibrary(caret)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\n```\n\n```{r data}\nload('../data/training_data.RData')\nload('../data/testing_data.RData')\n\nload('../data/y_train.RData')\nload('../data/y_test.RData')\n\ny_train = to_categorical(y_train, num_classes = 5)\ny_test  = to_categorical(y_test, num_classes = 5)\n\ncomp_mat = matrix(c(colSums(y_train) / nrow(y_train), \n                    colSums(y_test) / nrow(y_test)),\n                  byrow = T, nrow = 2)\ncolnames(comp_mat) = paste('category ', 0:4)\nrownames(comp_mat) = c('train', 'test')\nsave(comp_mat, file = 'test/category_imbalance.RData')\n```\n\n```{r imbalance_matrix, eval=TRUE, include = F}\n#| label: tbl-imbalance\nload('../test/category_imbalance.RData')\nkable(comp_mat, digits = 3, caption = 'Comparison of percentage of each category in the training and test sets')\n```\n\n```{r model building function, include = F}\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 5, step = 1)\n  lr = hp$Choice('learning_rate', values = seq(from = 1e-2, to = 1e-4, length.out = 5))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n\n  x = input\n  for (i in 1:n_layers){\n    \n    n_nodes = hp$Int(paste0('nodes_layer_', i),\n                      min_value = 30, max_value = 50, step = 10)\n    \n    x = x %>%\n      layer_dense(units = n_nodes, activation = 'relu') %>%\n      layer_dropout(rate = 0.1)\n  }\n  output = x %>% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %>% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c(metric_categorical_accuracy()))\n  \n  return(model)\n}\n```\n\n```{r model tuning, include = F}\nfor (i in 1:4){\n  \n  freqs = colSums(y_train) / nrow(y_train)\n  weights = sqrt(1 / freqs)\n  class_weights = dict()\n  for (k in 0:(length(weights)-1)) {\n    class_weights[[k]] = weights[k + 1]\n  }\n    \n  x_train = training_data_list[[i]]\n  \n  tuner_randomsearch = kerastuneR::RandomSearch(hypermodel = model_builder,\n                                                objective = 'val_categorical_accuracy',\n                                                max_trials = 75, executions_per_trial = 3,\n                                                directory = 'tuning',\n                                                project_name = paste('randomsearch results', i),\n                                                overwrite = TRUE)\n  \n  tuner_randomsearch %>% fit_tuner(x = x_train,\n                                   y = y_train,\n                                   epochs = 100,\n                                   batch_size = 32,\n                                   class_weight = class_weights,\n                                   validation_split = 0.2,\n                                   shuffle = TRUE)\n}\n```\n\n\n# Results\n\nThis section presents the comprehensive outcomes of our neural network approach to avalanche hazard prediction. We begin by examining the performance hierarchy across different predictor sets, followed by detailed analysis of optimal hyperparameter configurations. The section culminates in an evaluation of the final model's predictive capability, with particular attention to class-wise performance across the five avalanche hazard levels.\n\n## Hyperparameter Tuning Outcomes\n\n```{r}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(jsonlite)\n\ntuning_results = function(directory, max_layers){\n  \n  json_path = paste0(directory, '/trial.json')\n  tuning_results = fromJSON(txt = json_path)\n  hp               = tuning_results$hyperparameters$values\n  number_of_layers = hp$number_of_layers\n  learning_rate    = hp$learning_rate\n  score            = tuning_results$score\n  \n  layers_nodes     = matrix(rep(NA, max_layers), nrow = 1)\n  colnames(layers_nodes) = paste('nodes on layer ', 1:max_layers)\n  for (i in 1:number_of_layers) {\n    node_name = paste0('nodes_layer_', i)\n    layers_nodes[1, i] = hp[[node_name]]\n  }\n  \n  row_df = data.frame( Val_accuracy = score,  LR = learning_rate, \n                       layers = number_of_layers, layers_nodes, \n                       check.names = FALSE)\n  return(row_df)\n}\n\n# Test\n# tuning_results('tuning/randomsearch results 1/trial_19', 5)\n\ndirectories              = list.dirs('tuning', recursive = FALSE)\nrandomsearch_search      = 'randomsearch'\nrandomsearch_directories = grep(randomsearch_search, directories, value = TRUE)\n\nresults_compiler = function(method_directory){\n\n  for (i in 1:length(method_directory)){\n    trial_directories = list.dirs(method_directory[i])\n    results_df = do.call(rbind, lapply(trial_directories[-1], tuning_results, max_layers = 5))\n    results_df = arrange(results_df, desc(Val_accuracy))\n    save(results_df, file = paste0(method_directory[i], '/summary.RData'))\n  }\n}\n\n# Test\n#results_compiler('tuning/randomsearch results 1')\n\n# the results automatically get saved locally, inside the same tuning folder\nresults_compiler(randomsearch_directories)\n```\n\n\n```{r}\nlibrary(dplyr)\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(reticulate)\nlibrary(caret)\nlibrary(knitr)\nlibrary(kableExtra)\n\nfiles = c('tuning/randomsearch results 1/summary.RData',\n          'tuning/randomsearch results 2/summary.RData',\n          'tuning/randomsearch results 3/summary.RData',\n          'tuning/randomsearch results 4/summary.RData')\nvar_names = c('rs_results_1', 'rs_results_2', 'rs_results_3', 'rs_results_4')\n\nfor (i in seq_along(files)){\n  temp_env = new.env()                       # temporary environment\n  load(files[i], envir = temp_env)           # load into temp\n  assign(var_names[i], temp_env$results_df)  # assign with custom name\n}\n\nresults = list(rs_results_1, rs_results_2, rs_results_3, rs_results_4)\ndf = data.frame()\nfor (i in 1:4){\n  results_df = results[[i]]\n  top_3_models = results_df[1:3, ]\n  top_3_models = mutate(top_3_models, across(c(Val_accuracy, LR), round, 5))  \n  colnames(top_3_models) = c('Validation accuracy',\n                             'Learning rate',\n                             'Number of layers',\n                             paste('nodes on layer ', 1:5)) # make sure to make this the maximum layers\n  df = bind_rows(df, top_3_models)\n}\ndf = mutate(df, 'Predictor set' = rep(1:4, each = 3), .before = 'Validation accuracy')\n\nsave(df, file = 'tuning/tuning_summary_table.RData')\n```\n\n```{r myplot, results='asis', eval=TRUE, fig.cap=\"Validation accuracy by hyperparameter configuration across four predictor sets\"}\n\n#| label: fig-myplot\n#| \nload('../tuning/tuning_summary_table.RData')\n\n### The Tuning plot\nnames(df) <- gsub(\" \", \"_\", names(df))\n\n\nset.seed(1)  # for reproducibility of shuffle\n\ntop_10_shuffled <- df %>%\n  group_by(Predictor_set) %>%\n  sample_frac(1) %>%  # shuffle rows within each Predictor_set\n  ungroup() %>%\n  rowwise() %>%\n  mutate(\n    combo_label = paste0(\n      \"LR=\", Learning_rate,\n      \", Layers=\", Number_of_layers,\n      \", Nodes=[\", \n      paste(na.omit(c_across(starts_with(\"nodes_on_layer_\"))), collapse = \", \"),\n      \"]\"\n    )\n  ) %>%\n  ungroup()\n\n# Now reorder combo_label factor so predictor sets are grouped but shuffled inside\ntop_10_shuffled <- top_10_shuffled %>%\n  arrange(Predictor_set) %>%  # predictor sets grouped in order\n  mutate(combo_label = factor(combo_label, levels = unique(combo_label)))\n\n# Find optimal points per predictor set (same as before)\noptimal_points <- top_10_shuffled %>%\n  group_by(Predictor_set) %>%\n  filter(Validation_accuracy == max(Validation_accuracy)) %>%\n  ungroup()\n\nmy_colors <- c(\n  \"1\" = \"#e41a1c\",  # bright red\n  \"2\" = \"#377eb8\",  # strong blue\n  \"3\" = \"#4daf4a\",  # vivid green\n  \"4\" = \"#ff7f00\"   # bright orange\n)\n\n\n# Plot\nggplot(top_10_shuffled, aes(x = combo_label, y = Validation_accuracy, color = factor(Predictor_set))) +\n  geom_point(size = 3) +\n  geom_point(data = optimal_points, aes(x = combo_label, y = Validation_accuracy),\n             color = \"black\", size = 5, shape = 8) +\n  geom_point(size = 3)+\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 70, hjust = 1),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  xlab(\"Hyperparameter Combination\") +\n  ylab(\"Validation Accuracy\") +\n  scale_color_manual(values = my_colors, name = \"Predictor Set\")\n###############\n\n\n\n```\n\nSystematic hyperparameter tuning across four predictor sets revealed distinct performance patterns, visualized in the above figure. The plot shows validation accuracy distributions for various architectural configurations, with Predictor Set 2 (weather conditions) achieving the highest peak performance (~66%), closely followed by Predictor Set 4 (all variables) at ~65%.\n\n\n@tbl-tuning details the top three configurations for each predictor set, confirming the performance hierarchy. The marginal improvement of Set 2 over Set 4 indicates that weather variables capture the most critical forecasting signals, with topographic and snow-pack data providing limited incremental value.\n\n\n\n```{r tuning-table, results='asis', eval=TRUE}\n#| label: tbl-tuning\nload('../tuning/tuning_summary_table.RData')\n\nsummary_table = kable(df, booktabs = TRUE, caption = \"Top hyperparameter configurations per predictor set, ranked by validation accuracy.\") %>% \n  kable_styling(full_width = FALSE, position = 'center') %>%\n  collapse_rows(columns = 1, valign = 'middle')\n\nfor (i in seq(3, nrow(df), by = 3)) {\n  summary_table = row_spec(summary_table, i, \n                           extra_css = \"border-bottom: 2px solid black;\")\n}\n\nsummary_table\n```\n\nArchitecturally, models with 4–5 layers and 30–50 nodes per layer consistently outperformed others, with no clear gains beyond 5 layers. The optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with \\[30, 40, 40, 40\\] nodes and achieved 64.9% validation accuracy.\n\nThe optimal configuration from Predictor Set 4—selected for final evaluation—employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy, representing the best balance of performance and generalizability.\n\n## Optimal Architecture\n\n```{r best_model_plot, fig.cap=\"Optimal neural network architecture\"}\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 3, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-2, 1e-3, 1e-4))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n  \n  x = input\n  for (i in 1:n_layers){\n    \n    n_nodes = hp$Int(paste0('nodes_layer_', i),\n                     min_value = 30, max_value = 50, step = 10)\n    \n    x = x %>%\n      layer_dense(units = n_nodes, activation = 'relu') %>%\n      layer_dropout(rate = 0.1)\n  }\n  output = x %>% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %>% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c(metric_categorical_accuracy()))\n  \n  return(model)\n}\n\ntuner = kerastuneR::RandomSearch(hypermodel = model_builder, \n                                 objective = 'val_categorical_accuracy',\n                                 max_trials = 1, \n                                 executions_per_trial = 1,\n                                 directory = 'tuning',\n                                 project_name = 'randomsearch results 4')\n\ntuner$reload()\nbest_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] \n\ntuner$reload()\nbest_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] \n\ntf = tensorflow::tf\nplot_model = tf$keras$utils$plot_model\nplot_model(best_model, show_shapes = TRUE, show_layer_names = TRUE,\n           expand_nested = FALSE,\n           show_layer_activations = TRUE,\n           dpi = 500,\n           to_file = 'best_model_plot.png')\n```\n\n<img src=\"best_model_plot.png\" alt=\"Configuration of the best model\" width=\"500\"/>\n\nThe selected architecture utilizes ReLU activation in hidden layers and softmax output activation, appropriate for the multi-class ordinal nature of avalanche hazard prediction. This configuration represents an optimal balance between model complexity and predictive performance for the comprehensive feature set. This architecture balanced model complexity with predictive accuracy and was used for final evaluation on the test set.\n\n## Model Evaluation \n\n### Confusion Matrix\n\n\n```{r predictions}\n\n\nmodel_builder = function(hp){\n  \n  n_layers = hp$Int('number_of_layers', min_value = 1, max_value = 3, step = 1)\n  lr = hp$Choice('learning_rate', values = c(1e-2, 1e-3, 1e-4))  \n  \n  n_x   = ncol(x_train)\n  input = layer_input(shape = c(n_x))\n  \n  x = input\n  for (i in 1:n_layers){\n    \n    n_nodes = hp$Int(paste0('nodes_layer_', i),\n                     min_value = 30, max_value = 50, step = 10)\n    \n    x = x %>%\n      layer_dense(units = n_nodes, activation = 'relu') %>%\n      layer_dropout(rate = 0.1)\n  }\n  output = x %>% \n    layer_dense(units = 5, activation = 'softmax')\n  \n  model = keras_model(inputs = input, outputs = output)\n  \n  model %>% compile(loss = 'categorical_crossentropy', \n                    optimizer = optimizer_adam(learning_rate = lr),\n                    metrics = c(metric_categorical_accuracy()))\n  \n  return(model)\n}\n\ntuner = kerastuneR::RandomSearch(hypermodel = model_builder, \n                                 objective = 'val_categorical_accuracy',\n                                 max_trials = 1, \n                                 executions_per_trial = 1,\n                                 directory = 'tuning',\n                                 project_name = 'randomsearch results 4')\n\ntuner$reload()\nbest_model   = tuner$get_best_models(num_models = as.integer(1))[[1]] \n\nresults      = best_model %>% evaluate(x_test, y_test)\ny_pred_probs = best_model %>% predict(x_test)\ny_pred       = max.col(y_pred_probs) - 1\ntrue_classes = apply(y_test, 1, which.max) - 1 # it was one-hot encoded so i changed it back\nmetrics_list = confusionMatrix(factor(y_pred), factor(true_classes))\n\ncon_mat = as.data.frame(metrics_list$table)\ncon_mat = rename(con_mat, Predicted = Prediction, Reference = Reference, Count = Freq)\ncon_mat =  tidyr::pivot_wider(con_mat, names_from = Reference, values_from = Count,\n                              values_fill = 0)\nsave(con_mat, file = 'test/con_mat.RData')\nsave(metrics_mat, file = 'test/metrics_mat.RData')\n```\n\n```{r confusion, results='asis', eval=TRUE}\n#| label: tbl-confusion\nload('../test/con_mat.RData')\nkable(con_mat, format = 'html', caption = \"Confusion matrix for avalanche hazard predictions.\") %>%\n  kable_styling(full_width = FALSE, position = 'center') %>%\n  add_header_above(c(' ' = 1, 'Predicted' = 5))\n```\n\nThe confusion matrix of the the fitted model is reported in @tbl-confusion. It is clear that category 0 is the best estimated while categories 3 and 4 are estimated the worst. This result was expected since category 3 and 4 are very underepresented in both the training and test data. Concrete metrics of the model performance is given in the table below.\n\n### Class-Level Metrics\n\n```{r metrics matrix, results='asis', eval=TRUE}\n#| label: tbl-class\n\nload('../test/metrics_mat.RData')\nkable(metrics_mat, digits = 3, caption = \"Comprehensive classification metrics by avalanche hazard class.\") %>%\n  kable_styling(full_width = FALSE, position = 'center')\n```\n\nSensitivity gives the percent of the time the model predicted an observation as belonging to a category and it actually belonging to that category. The sensitivity for class 0 is the best with a value of 0,854. There is a steep drop off for the other classes but class 2 is the second highest with a value of 0,557 and the rest are all below 0.5 meaning that more often than not, the model is unable to identify the correct category. An analogy for sensitivity is a test with high sensitivity(close to the maximum of 1) will identify most of the patients with with the flu as having the flu but this may mean that lots of patients without the flu also get identified as having the flu. An extreme case may be if the model predicted all observations as belonging to category 0. Then the sensitivity would be 1,meaning that all observations that belong to category 0 are predicted to be category 0. So there need to be a balance because we do not want this\n\nSpecificity gives the percent of time the an observation does not belong to a specific category and the model predicts it as not belonging to that category. High specificity(close to the maximum of 1) is analogous to a test rarely every flagging someone as having the flu when they don't. A perfect model will have high sensitivity with a high specificity, meaning that it is able to identify when observations belong to a category and does not incorrectly predict other observations as belonging to that category. The table above indicates that all the categories have relatively high specificity. Class 4 has a specificity of 0.996 which on the surface looks great but because the sensitivity is so low, this high specificity just means that the model rarely every predicts any observations as belonging to category 4. The same follows for category 3. This is also seen in the extremely low detection rate and prevalence for these categories. These results, specifically for categories 3 and 4 are an indication that perhaps just reweighting the classes was not enough to overcome the imbalance in the data.\n\nA better metric for instances such as this where the data is imbalanced is the F1 score. The F1 score is the harmonic mean of the precision and recall. The precision is the percentage of observations that belonged to a category and were correctly predicted as belonging to that category. The recall is just the sensitivity. The F1 score, same as the other metrics, ranges from 0 to 1 with 0 being the worst and 1 being the best. The F1 score is useful because the harmonic mean because it is less affected by extreme values than the arithmetic mean. Categories 4 and 3 are by far the worst with values of 0,104 and 0,119. Category 0 has the best F1 score with a value of 0,716. Then category 1 has a value of 0,383 and category 3 has a score of 0,487.\n\n\nAll these values, except for category are quite poor and indicate lack of predictive power. This is reflected in the fact that the model achieved an accuracy of 51,7% on the test set. The model is better than blindly guessing which we expect to return an accuracy of 20% but the model lacks predictive power.\n\n\n\n## Interpretation\n\nThe results highlight the challenges of severe class imbalance. While class reweighting improved performance somewhat, minority classes remained poorly predicted. This suggests that more aggressive strategies may be necessary, such as resampling (e.g., bootstrapping or SMOTE), expanding the hyperparameter search space, or exploring alternative architectures.\n\nDespite these limitations, the study provides useful insights. It demonstrates that weather variables are the dominant predictors of avalanche hazard and that neural networks can achieve moderate accuracy with relatively simple architectures. However, improving detection of rare but high-consequence hazard categories remains an important direction for future work.\n\n\n# Extra resources\n\n[`keras`](https://cran.r-project.org/web/packages/keras/vignettes/){target=\"_blank\"}\n\n[`kerastuneR`](https://eagerai.github.io/kerastuneR/){target=\"_blank\"}\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":false,"output-file":"Results.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":"journal","smooth-scroll":true,"anchor-sections":true,"title":""},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}