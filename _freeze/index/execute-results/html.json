{
  "hash": "177ba31511a61cdcfde075c787ba0de0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"\"\ndf-print: paged\nformat: \n  html:\n    css: styles.css\n    toc: false\n    number-sections: false\n    code-fold: true\n    smooth-scroll: true\n    lightbox: true\n---\n\n\n\n\n\n\n:::{.hero}\n# Avalanche Hazard Forecasting  \n:::{.hero-subtitle}\nPredicting forecasted avalanche hazard levels from 15 years of Scottish data using a neural network.\n:::\n:::\n\n:::{.panel-tabset .tabset-pills .tabset-fade}\n\n\n\n---\ntitle: \"\"\nformat: html\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## Abstract {#sec-Abstract}\n\n### Abstract\n\nThis study investigates the application of neural networks for avalanche hazard level prediction using a large historical dataset from the Scottish Avalanche Information Service. The models are trained on predictors such as meteorological, snowpack and topographical features. Initially, the historical data was preprocessed into training, validation and tests sets. The model was trained and performance was compared against baseline expectations with feature importance also being investigated.\n\nThe results of the model are promissing with the neural networks being able to successfully capture the non-linear and complex interactions between the features. Challenges such as misclassificationrates for high hazard levels and limited interpretability of the model were additionally observed, showing potential for improvement in the future with methods such as resampling. Overall, this investigation shows the potential and limitations of implementing neural networks for avalanche hazard level prediction.\n\n\n\n---\ntitle: \"\"\nformat: html\n\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## 1 Introduction and Lit Review {#sec-Introduction_and_Lit_Review}\n\n### 1.1 Introduction\n\nScotland is a region with frequent snow accumulation as well as a high amount of human activity. It is therefore vital that avalanche forecasting is carried out as a key component of their mountain safety and reducing economic losses. Avalanche forecasting can prove to be difficult as it is dependent on many factors with complex relationships and interactions, such as snow properties, meteorological conditions and terrain features. Usually, avalanche forecasts are predicted using a combination of weather reports as well as field observations to release daily bulletins. Although these are essential, the rapidly expanding historical weather and avalanche datasets have created the oppurtunity for new, data driven methods.\n\nMachine learning, specifically neural networks, have become strong contenders for avalanche forecasting due to their ability to model complex and multi-dimensional relationships. From the ever growing historical datasets, these methods can uncover patterns that have previously not been utilised in avalanche forecasting. The Scottish Avalanche Information Service (SAIS) provides a large amount of historical data that can be utilised to train neural networks to improve avalanche risk management. In this report, this dataset will be used to train and evaluate neural network models for the potential to forecast avalanches in the Scottish region. \n\n### 1.2 Literature Review\n\nAvalanche forecasting and research spans across snow science, meteorology and data science. Initially, intuition and rules of thumb based on snowfall and temperature were used to predict avalanches. These methods proved to be inconsistent and struggled to generalise diverse snow conditions [7]. These methods moved towards statistical and probabilistic methods. This more systematic process could integrate more predictors as well as quantify uncertainty [8].\n\nDue to the ever-growing amount of data, machine learning methods have been utilised to predict avalanches. Random forrest and support vector machines can be applied to large avalanche datasets and achieve high accuracy, especially compared to more traditional models [4]. This proves the potential for machine learning methods to capture complex and nonlinear relationships between avalanche predictor variables. From machine learning methods, neural networks stand out due to their flexibility and ability to work with high dimensional predictors. Deep learning applications related to avalanches are snow depth prediction, snowpack classification and exploratory hazard modelling [9]. Although results have been positive, neural networks have not been fully developed or implemented into avalanche forecasting.\n\nFor avalanche forecasting, snowpack stability indicators are vital. These include temperature gradients and snow temperature profiles [13]. Additionally, meteorological variables, such as wind speed, air temperature and precipitation, directly affect snow deposition and terrain features, such as aspect and slope angle affect avalanche likelyhood [1]. These predictors have complex relationships and patterns that traditional methods do not capture however mehtods such as neural networks can uncover these subtle interactions.\n\nFrom the literature, it has been shown that avalanche forecasting methods have evolved from intuition and expert-based assessments to statistical methods and then further to machine learning methods. Studies have been completed to confirm the feasibility of data-driven avalanche forecasting however integration into operational practices is underdeveloped.\n\n\n\n---\ntitle: \"\"\nformat: html\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n## 2 Data and Methods {#sec-Data_and_Methods}\n\nThe dataset used in this study consists of daily avalanche forecasts for six forecasting regions across Scotland (Creag Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon), collected by the Scottish Avalanche Information Service (SAIS) between 2009 and 2025. It contains 10,671 observations and 34 variables including weather conditions, topography, snowpack properties, and avalanche hazard levels.\n\nThe target variable for this project is the Forecast Avalanche Hazard (FAH), a categorical variable with five ordered levels: Low, Moderate, Considerable -, Considerable +, and High.\n\nData Cleaning and Preprocessing was done to remove observations that did not make logical sense, transform the data where needed and remove necessary variable. Furthermore, Exploratory Data Analysis (EDA) was then performed to assess variable distributions and perform imputation, outlier handling and scaling of data.\n\n### 2.1 Data Cleaning and Preprocessing\n\nThe raw dataset underwent extensive cleaning prior to analysis. Initially, rows with missing FAH values (109 rows) were removed, and FAH levels were replaced with a numerical representation from 0 to 4 for compatibility with the neural network software. Other categorical variables, such as precipitation type and forecast region, were similarly transformed into numerical formats. The Date column was transformed into Month to capture seasonal effects.\n\nVariables unrelated to forecasting or with high missingness (more than 20%), frequent errors, limited predictive value, or poorly defined measures were excluded, resulting in a set of well-defined predictors most relevant for forecasting.\n\nRange checks were applied to remove physically impossible or implausible values based physical limits and regional context.\n\nFor example, altitude was limited to 0–1,345 m, the height of Ben Nevis, and wind speeds to 176 mph, reflecting the highest recorded gust in the Highlands [11].\n\nSlope inclines were restricted to 0–90°, and wind directions were constrained to 0–360°. Cloud cover was limited to 0–100%, total snow depth to 0–1,500 cm (with unrealistic spikes removed), and foot penetration depth to 100 cm. Snow temperatures above 0.5 °C were flagged as implausible, since snow begins melting at 0 °C, and maximum temperature gradients were limited to below 25 °C per 10 cm [15]. Observations where foot penetration exceeded total snow depth were also removed.\n\nCircular variables such as wind direction and aspect were transformed into sine and cosine components.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n### 2.2 Predictor Sets\n\n\n::: {.cell}\n\n:::\n\n\nThe data was split into three predictor sets; pred1, pred2, and pred3, where each predictor set represents different types of information relating to snow and avalanche conditions. Furthermore, predictor set 1 represents the location data such as the area, exact location coordinates, and how steep the slope is etc. Predictor set 2 represents the air conditions such as air temperature, cloud cover, and wind speed etc. Lastly, predictors 3 represents the snow conditions such as how deep the snow is, the temperature of the snow at different layers and foot penetration. All three predictor sets essentially contribute to determining how the events have affected the snowfall. In addition, the month variable does not fit in any predictor set, but rather adds seasonal context since snow behaviour differs throughout the year. The predictor sets were separated into the following sets:\n\nPredictor Set 1: Area, longitude, latitude, Alt, Incline, Aspect_sin, and Aspect_cos\n\nPredictor Set 2: Air.Temp, Wind.Speed, Cloud, Precip.Code, Drift, Summit.Air.Temp, Summit.Wind.Speed, WindDir_sin, WindDir_cos, SummitWindDir_sin, and SummitWindDir_cos\n\nPredictor Set 3: Total.Snow.Depth, Foot.Pen, Rain.at.900, Max.Temp.Grad, Max.Hardness.Grad, and Snow.Temp\n\nOther Variables: Month\n\n### 2.3 Imputation\n\nMultiple Imputation by Chained Equations (MICE) handles missing data that creates multiple versions of a dataset rather than selecting the first best guessed value. The MICE function ran under the assumption of Missing at Random (MAR), which means that it was assumed that the probability of a value being missing depends on the observed values in the dataset, but are independent of the unobserved values [5].\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      FAH Area longitude latitude Alt Cloud Air.Temp Incline Wind.Speed\n10069   1    1         1        1   1     1        1       1          1\n314     1    1         1        1   1     1        1       1          1\n41      1    1         1        1   1     1        1       1          0\n12      1    1         1        1   1     1        1       0          1\n3       1    1         1        1   1     1        1       0          1\n5       1    1         1        1   1     1        0       1          1\n3       1    1         1        1   1     1        0       1          0\n1       1    1         1        1   1     1        0       0          0\n1       1    1         1        1   1     1        0       0          0\n6       1    1         1        1   1     0        1       1          1\n1       1    1         1        1   1     0        1       1          0\n1       1    1         1        1   1     0        1       0          0\n3       1    1         1        1   1     0        0       1          0\n8       1    1         1        1   1     0        0       0          0\n6       1    1         1        1   0     1        1       1          1\n1       1    1         1        1   0     1        1       1          1\n1       1    1         1        1   0     1        1       0          1\n        0    0         0        0   8    19       21      27         59\n      Aspect_sin Aspect_cos    \n10069          1          1   0\n314            0          0   2\n41             1          1   1\n12             1          1   1\n3              0          0   3\n5              1          1   1\n3              1          1   2\n1              1          1   3\n1              0          0   5\n6              1          1   1\n1              1          1   2\n1              1          1   3\n3              1          1   3\n8              1          1   4\n6              1          1   1\n1              0          0   3\n1              0          0   4\n             320        320 774\n```\n\n\n:::\n:::\n\n\nThe figure above shows the pattern of missing values for a subset of 10 variables, where blue represents no missing values and red represents missing values present. It can be seen that the sum of missing values for the 10 variables was 774 values, and it can be seen that aspect_sin and aspect_cos have the highest number of missing values (320 values) among these 10 variables. Furthermore, wind.speed, incline, air.temp and cloud have the next highest missing values of 59, 27, 21 and 19 respectively.\n\nIn addition, it was seen that variables like area, latitude, longitude and alt have no missing values, which shows that they are reliable predictors for the missing values in other varaiables.\n\n\n::: {.cell}\n\n:::\n\n\nMICE was run with five iterations creating five slightly different datasets and Predictive Mean Matching (PMM) was used, which uses the information in the dataset to predict what the missing values would most likely be. In addition, all variables that were in the cleaned dataset which contained missing values were selected for imputation, as the highest percentage of missing data was 14.2%, so there were enough observed variables to predict the unobserved variables well.\n\nIn addition, after running the MICE imputation, the density plots were analysed to determine if the variables were imputed well or not, with imputed values represented by the red line and observed values represented by the blue line. Imputed values should not change the distribution of a variable [10], thus, if the imputed variables shows a different distribution to the observed variables, the variable was removed from the dataset.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe figure above displays a stripplot from the MICE imputation, which shows the distribution across all variables and the five different imputation sets of observed values represented by blue and the imputed values represented by red. It can be seen that the red and blue dots overlap well and follow similar distributions in multiple variables which shows that the imputation worked well.\n\nHowever, variables such as Alt, Air.Temp, and Incline show that imputed values do not follow the same distribution as the observed values as the red dots do not overlap well and indicates that the data failed to impute well under the MAR assumption.\n\nThe density plots for the variables in question were plotted as well as the density plot of Max.Hardness.Grad which imputed well as a comparison.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nIt can be seen from the figure above that the imputed values in red do not follow the observed values in blue for Alt, Incline, and Air.Temp. However, comparing it to a well imputed variable like Max.Hardness.Grad, it can be seen that the imputed values follows the observed values distribution closely. This exercise was done for each of the imputed variable, however, just the variables that did not impute well and one variable that did impute well was shown in the final report.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nThe variable that did not impute well was removed from the dataset, and the new dataset was refitted using MICE which ensured that the imputations better satisfied the MAR assumption and produced reliable results.\n\n### 2.4 Outlier Handling\n\nOutlier Capping or Winsorization is a method that handles outliers by converting the extreme high values to the value of the highest data point that is not considered an outlier [2]. This method was used on the imputed dataset to account for any values that were extreme to ensure that the distribution is not skewed.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"longitude - Outliers capped: 207\"\n[1] \"latitude - Outliers capped: 208\"\n[1] \"Wind.Speed - Outliers capped: 45\"\n[1] \"Summit.Air.Temp - Outliers capped: 207\"\n[1] \"Summit.Wind.Speed - Outliers capped: 95\"\n[1] \"Total.Snow.Depth - Outliers capped: 103\"\n[1] \"Foot.Pen - Outliers capped: 68\"\n[1] \"Max.Temp.Grad - Outliers capped: 93\"\n[1] \"Max.Hardness.Grad - Outliers capped: 48\"\n[1] \"Snow.Temp - Outliers capped: 135\"\n```\n\n\n:::\n:::\n\n\nFor each variable in the dataset, the 1st and 99th percentile was calculated to address the extreme values, preserving 98% of the data. If a value fell below the 1st percentile, it was capped to the 1st percentile value, furthermore, if a value fell above the 99th percentile, it was capped to the 99th percentile value.\n\nIn addition, outlier handling was intentionally done after imputation, in the event that MICE generated a value that fell outside of the realistic range of values in the original dataset, considering that data cleaning had already taken place.\n\nFurthermore, it was seen that the need to cap outliers was needed as multiple variables had around \\~1% of its data capped, which shows that the data contained extreme values either in the original dataset or from imputation.\n\n### 2.5 Correlation Analysis\n\nA correlation analysis was performed to examine the patterns in the variables and assess potential multicollinearity effects.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nIt can be seen from the figure above that correlation between variables exist as the dark blue squares represent strong positive correlations and the dark red squares represent strong negative correlations. The variables from predictor set 1 are represented by dark blue squares, indicating strong positive correlations between the variables. Moreover, FAH has moderate correlations with the variables as the colour range are in the lighter section of the colour scale.\n\nFurthermore, it is important to note that the white squares represent independent variables (correlation of 0), and it can be seen that there are multiple variables such as Aspect_cos and Area have white squares which indicates that multicollinearity is not excessive.\n\nLastly, it can be seen that variables with similar correlation patterns have grouped together creating visible blocks of related variables, which confirms the predictor set groupings capture different types of information.\n\n\n::: {.cell}\n\n:::\n\n\n### 2.6 Data Scaling\n\nScaling was performed to ensure that variables with different units and scales contributed equally to the model and variables with larger numeric values did not dominate the model.\n\n\n::: {.cell}\n\n:::\n\n\nThe data was split using a 70/30 split, which was stratified using FAH to ensure that both the training and testing sets follow a similar distribution of the target variable, FAH. Thereafter, the numeric values from the training dataset were used to calculate the mean and standard deviation of each variable using the preProcess() function and was then applied to both the training and test sets using the predict() function. Furthermore, this approach prevents data leakage which occurs when information from the test set goes into the training set.\n\n\n\n---\ntitle: \"\"\noutput: pdf_document\nprefer-html: true\n---\n\n\n## 3 Model building, tuning and evalutation {#sec-model_dt}\n\n### 3.1 Software and Setup\n\n\n::: {.cell}\n\n:::\n\n\nThis section outlines the comprehensive methodology employed for predicting avalanche hazard levels using neural networks. We describe the data preprocessing pipeline, experimental design with four distinct predictor sets, neural network architecture selection process, and hyperparameter tuning strategy. The methodology ensures reproducible results while systematically evaluating the contribution of different variable types to forecasting accuracy.\n\n\nNeural network modelling was implemented using the **`keras`** library, accessed through its R wrapper. Keras is an open-source deep learning framework known for its modularity and relatively gentle learning curve compared to other libraries such as PyTorch. Its modular design makes it well suited for projects that require experimentation with multiple configurations.\n\nBecause Keras functions in R act as wrappers around the underlying Python functions, version control is critical to ensure reproducibility. This project was developed in **`R version 4.5.1 (2025-06-13)`** with **`Python version 3.11.6`**.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n### 3.2 Data Preparation\nThe modelling goal was to predict Forecasted Avalanche Hazard (FAH). To prevent data leakage, the Observed Avalanche Hazard (OAH) variable was excluded from the predictor sets, since this information would not be available in real-world forecasting scenarios.\n\nA 70/30 train–test split was applied, with a fixed random seed to ensure reproducibility. Although a validation set is often held out, Keras provides internal validation handling, so this was not specified manually.\n\n\n::: {#tbl-imbalance .cell}\n::: {.cell-output-display}\n\n\nTable: Comparison of percentage of each category in the training and test sets\n\n|      | category  0| category  1| category  2| category  3| category  4|\n|:-----|-----------:|-----------:|-----------:|-----------:|-----------:|\n|train |       0.328|       0.303|       0.236|       0.088|       0.044|\n|test  |       0.320|       0.312|       0.236|       0.089|       0.043|\n\n\n:::\n:::\n\n\n@tbl-imbalance reports the class distribution across training and test sets. The proportions are broadly consistent between the two partitions, but the data exhibits strong class imbalance. Categories 0 and 1 each account for roughly 30\\% of the data, category 2 for 23\\%, while categories 3 and 4 are severely underrepresented (9\\% and 4\\% respectively). This imbalance risks biasing the model toward predicting majority classes.\n\nTo mitigate this, custom class weights were introduced. The weights were set inversely proportional to the prevalence of each category in the training set, so that errors on minority classes carried higher penalties during training. Alternative methods such as bootstrap resampling could also be considered, but were not explored here.\n\n### 3.3 Predictor Sets\n\nFor this project there were 4 predictor sets that were evaluated. Predictor set 1 contained variables relating to the location of the observation. Predictor set 2 contained variables relating to the weather conditions of the observation. Predictor set 3 contained variables relating to a snow pack test. Predictor set 4 contained all the varibles in the dataset and thus contained all three predictor sets inside it. The final model will be built using all the available data, so predictor set 4. The remaining three predictor sets serve as yardstick to which we can compare which sets of variables are important.\n\n### 3.4 Model Development\n\nNeural networks are extremely flexible and configurable models. But this configurability means that there are several parameters that need to be defined before the model can be trained. In order to find the optimal configuration, hyperparameter tuning needs to be done first. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, the acitvation function to use on each layer, the dropout rate and the learning rate. **`keras`** is flexible and allows the user control of almost every aspect of the model through the parameters. In order to do the hyperameter tuning in R, the **`kerastuneR`** library is used. But in order to do the hyperparameter tuning with **`kerastuneR`** the model needs to be wrapped in a function that accepts as an input the hyperparameter configuration, and outputs a model.\n\n\n::: {.cell}\n\n:::\n\n\nFor this project, it was decided to tune for the number of layers, number of nodes on each layer and the learning rate. The activation function was chosen to be rectified linear units(Relu) and the dropout rate was set to 0,1. Each dense layer was followed by a dropout layer. The specific values that were tuned across were 1-5 layers(step size of 1) with 30-50(step size of 10) nodes on each layer and 5 equally spaced learning rates [0,01 - 0,0001]. Therefore, there were a total of 75 unique models that could be fitted. The metric the Neural Network tries to minimize is the **`metric_categorical_accuracy`**. This was chosen since the target variable has more than 1 category.\n\n### 3.5 Hyperparameter Tuning\n\n\n::: {.cell}\n\n:::\n\n\n**`kerastuneR`** has multiple tuning algorithms, we have used the **`RandomSearch`** algorithm. **`RandomSearch`** takes random combinations of the provided hyperparameters and fits the model each time. Since the combination of hyperparameters is random, there is a possibility that the same mode configuration is ran multiple times by the algorithm. The algorithm does attempt to mitigate this but it not guaranteed to stop duplicate runs. We do have some control over this though by setting the **`max_trials`** variable to the total number of unique models that can be specified from our selected tuning ranges. It has also been specified that each model should be fit 3 times by setting **`executions_per_trial = 3`**. This reduces variation in the results since there is an element of randomness in the initialisation of the model. A validation split of 20% was used and **`shuffle = T`** was used. Doing this shuffles the which observations get used as the validation set. This helps reduce the chances of overfitting.\n\nAll the results from tuning were saved into folders so that the results can be extracted and used for further analysis. The tuning was was undertaken is by no means exhaustive, no tuning can ever be, but the range of values tuned over is relatively small and therefore the results should be taken with a pinch of salt. With more time and perhaps more compute power a better result is possible.\n\n**`kerastuneR`** saves the tuning results as .json files. Each trial will be its own folder and inside that folder there will be a .json file containing information about the configuration of the Neural Network and the validation accuracy it achieved. The results were compiled into a single table containing the top 3 configurations from each of the predictor sets.\n\n### 3.6 Results\n\nThis section presents the comprehensive outcomes of our neural network approach to avalanche hazard prediction. We begin by examining the performance hierarchy across different predictor sets, followed by detailed analysis of optimal hyperparameter configurations. The section culminates in an evaluation of the final model's predictive capability, with particular attention to class-wise performance across the five avalanche hazard levels.\n\n### 3.7 Hyperparameter Tuning Outcomes\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n![Validation accuracy by hyperparameter configuration across four predictor sets](index_files/figure-html/fig-myplot-1.png){#fig-myplot width=672}\n\n\nSystematic hyperparameter tuning across four predictor sets revealed distinct performance patterns, visualized in the above figure. The plot shows validation accuracy distributions for various architectural configurations, with Predictor Set 2 (weather conditions) achieving the highest peak performance (~66%), closely followed by Predictor Set 4 (all variables) at ~65%.\n\n@tbl-tuning details the top three configurations for each predictor set, confirming the performance hierarchy. The marginal improvement of Set 2 over Set 4 indicates that weather variables capture the most critical forecasting signals, with topographic and snow-pack data providing limited incremental value.\n\n\n::: {#tbl-tuning .cell}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Top hyperparameter configurations per predictor set, ranked by validation accuracy.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Predictor set </th>\n   <th style=\"text-align:right;\"> Validation accuracy </th>\n   <th style=\"text-align:right;\"> Learning rate </th>\n   <th style=\"text-align:right;\"> Number of layers </th>\n   <th style=\"text-align:right;\"> nodes on layer  1 </th>\n   <th style=\"text-align:right;\"> nodes on layer  2 </th>\n   <th style=\"text-align:right;\"> nodes on layer  3 </th>\n   <th style=\"text-align:right;\"> nodes on layer  4 </th>\n   <th style=\"text-align:right;\"> nodes on layer  5 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;vertical-align: middle !important;\" rowspan=\"3\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.50829 </td>\n   <td style=\"text-align:right;\"> 0.01000 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> 0.46535 </td>\n   <td style=\"text-align:right;\"> 0.01000 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 0.45376 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 0.01000 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 5 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 30 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 30 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 30 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 40 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 30 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;vertical-align: middle !important;\" rowspan=\"3\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.65826 </td>\n   <td style=\"text-align:right;\"> 0.00753 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> 0.65735 </td>\n   <td style=\"text-align:right;\"> 0.00753 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 0.65621 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 0.01000 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 4 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 40 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 30 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 30 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 40 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;vertical-align: middle !important;\" rowspan=\"3\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.62645 </td>\n   <td style=\"text-align:right;\"> 0.01000 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> 0.62509 </td>\n   <td style=\"text-align:right;\"> 0.01000 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 0.62463 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 0.01000 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 5 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 40 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 40 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 30 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 40 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;vertical-align: middle !important;\" rowspan=\"3\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.64940 </td>\n   <td style=\"text-align:right;\"> 0.00505 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> 0.64781 </td>\n   <td style=\"text-align:right;\"> 0.01000 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 40 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 0.64781 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 0.01000 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 5 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 50 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 40 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 30 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 50 </td>\n   <td style=\"text-align:right;border-bottom: 2px solid black;\"> 50 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\nArchitecturally, models with 4–5 layers and 30–50 nodes per layer consistently outperformed others, with no clear gains beyond 5 layers. The optimal configuration from Predictor Set 4, selected for final evaluation, employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy.\n\nThe optimal configuration from Predictor Set 4, selected for final evaluation, employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy, representing the best balance of performance and generalizability.\n\n### 3.8 Optimal Architecture\n\n\n::: {.cell}\n\n:::\n\n\n<img src=\"best_model_plot.png\" alt=\"Configuration of the best model\" width=\"500\"/>\n\nThe selected architecture utilizes ReLU activation in hidden layers and softmax output activation, appropriate for the multi-class ordinal nature of avalanche hazard prediction. This configuration represents an optimal balance between model complexity and predictive performance for the comprehensive feature set. This architecture balanced model complexity with predictive accuracy and was used for final evaluation on the test set.\n\n### #.9 Model Evaluation \n\n#### 3.9.1 Confusion Matrix\n\n\n\n::: {.cell}\n\n:::\n\n::: {#tbl-confusion .cell}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Confusion matrix for avalanche hazard predictions.</caption>\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"5\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Predicted</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Actual </th>\n   <th style=\"text-align:right;\"> 0 </th>\n   <th style=\"text-align:right;\"> 1 </th>\n   <th style=\"text-align:right;\"> 2 </th>\n   <th style=\"text-align:right;\"> 3 </th>\n   <th style=\"text-align:right;\"> 4 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 0 </td>\n   <td style=\"text-align:right;\"> 857 </td>\n   <td style=\"text-align:right;\"> 420 </td>\n   <td style=\"text-align:right;\"> 88 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 118 </td>\n   <td style=\"text-align:right;\"> 326 </td>\n   <td style=\"text-align:right;\"> 219 </td>\n   <td style=\"text-align:right;\"> 51 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:right;\"> 29 </td>\n   <td style=\"text-align:right;\"> 231 </td>\n   <td style=\"text-align:right;\"> 413 </td>\n   <td style=\"text-align:right;\"> 186 </td>\n   <td style=\"text-align:right;\"> 94 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 18 </td>\n   <td style=\"text-align:right;\"> 20 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\nThe confusion matrix of the the fitted model is reported in @tbl-confusion. It is clear that category 0 is the best estimated while categories 3 and 4 are estimated the worst. This result was expected since category 3 and 4 are very underepresented in both the training and test data. Concrete metrics of the model performance is given in the table below.\n\n#### 3.9.2 Class-Level Metrics\n\n\n::: {#tbl-class .cell}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Comprehensive classification metrics by avalanche hazard class.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> Sensitivity </th>\n   <th style=\"text-align:right;\"> Specificity </th>\n   <th style=\"text-align:right;\"> Pos Pred Value </th>\n   <th style=\"text-align:right;\"> Neg Pred Value </th>\n   <th style=\"text-align:right;\"> Precision </th>\n   <th style=\"text-align:right;\"> Recall </th>\n   <th style=\"text-align:right;\"> F1 </th>\n   <th style=\"text-align:right;\"> Prevalence </th>\n   <th style=\"text-align:right;\"> Detection Rate </th>\n   <th style=\"text-align:right;\"> Detection Prevalence </th>\n   <th style=\"text-align:right;\"> Balanced Accuracy </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Class: 0 </td>\n   <td style=\"text-align:right;\"> 0.854 </td>\n   <td style=\"text-align:right;\"> 0.751 </td>\n   <td style=\"text-align:right;\"> 0.617 </td>\n   <td style=\"text-align:right;\"> 0.916 </td>\n   <td style=\"text-align:right;\"> 0.617 </td>\n   <td style=\"text-align:right;\"> 0.854 </td>\n   <td style=\"text-align:right;\"> 0.716 </td>\n   <td style=\"text-align:right;\"> 0.320 </td>\n   <td style=\"text-align:right;\"> 0.273 </td>\n   <td style=\"text-align:right;\"> 0.443 </td>\n   <td style=\"text-align:right;\"> 0.802 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Class: 1 </td>\n   <td style=\"text-align:right;\"> 0.332 </td>\n   <td style=\"text-align:right;\"> 0.817 </td>\n   <td style=\"text-align:right;\"> 0.452 </td>\n   <td style=\"text-align:right;\"> 0.729 </td>\n   <td style=\"text-align:right;\"> 0.452 </td>\n   <td style=\"text-align:right;\"> 0.332 </td>\n   <td style=\"text-align:right;\"> 0.383 </td>\n   <td style=\"text-align:right;\"> 0.312 </td>\n   <td style=\"text-align:right;\"> 0.104 </td>\n   <td style=\"text-align:right;\"> 0.230 </td>\n   <td style=\"text-align:right;\"> 0.575 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Class: 2 </td>\n   <td style=\"text-align:right;\"> 0.557 </td>\n   <td style=\"text-align:right;\"> 0.775 </td>\n   <td style=\"text-align:right;\"> 0.433 </td>\n   <td style=\"text-align:right;\"> 0.850 </td>\n   <td style=\"text-align:right;\"> 0.433 </td>\n   <td style=\"text-align:right;\"> 0.557 </td>\n   <td style=\"text-align:right;\"> 0.487 </td>\n   <td style=\"text-align:right;\"> 0.236 </td>\n   <td style=\"text-align:right;\"> 0.131 </td>\n   <td style=\"text-align:right;\"> 0.303 </td>\n   <td style=\"text-align:right;\"> 0.666 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Class: 3 </td>\n   <td style=\"text-align:right;\"> 0.072 </td>\n   <td style=\"text-align:right;\"> 0.987 </td>\n   <td style=\"text-align:right;\"> 0.345 </td>\n   <td style=\"text-align:right;\"> 0.916 </td>\n   <td style=\"text-align:right;\"> 0.345 </td>\n   <td style=\"text-align:right;\"> 0.072 </td>\n   <td style=\"text-align:right;\"> 0.119 </td>\n   <td style=\"text-align:right;\"> 0.089 </td>\n   <td style=\"text-align:right;\"> 0.006 </td>\n   <td style=\"text-align:right;\"> 0.018 </td>\n   <td style=\"text-align:right;\"> 0.529 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Class: 4 </td>\n   <td style=\"text-align:right;\"> 0.059 </td>\n   <td style=\"text-align:right;\"> 0.996 </td>\n   <td style=\"text-align:right;\"> 0.421 </td>\n   <td style=\"text-align:right;\"> 0.959 </td>\n   <td style=\"text-align:right;\"> 0.421 </td>\n   <td style=\"text-align:right;\"> 0.059 </td>\n   <td style=\"text-align:right;\"> 0.104 </td>\n   <td style=\"text-align:right;\"> 0.043 </td>\n   <td style=\"text-align:right;\"> 0.003 </td>\n   <td style=\"text-align:right;\"> 0.006 </td>\n   <td style=\"text-align:right;\"> 0.528 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\nSensitivity gives the percent of the time the model predicted an observation as belonging to a category and it actually belonging to that category. The sensitivity for class 0 is the best with a value of 0,854. There is a steep drop off for the other classes but class 2 is the second highest with a value of 0,557 and the rest are all below 0.5 meaning that more often than not, the model is unable to identify the correct category. An analogy for sensitivity is a test with high sensitivity(close to the maximum of 1) will identify most of the patients with with the flu as having the flu but this may mean that lots of patients without the flu also get identified as having the flu. An extreme case may be if the model predicted all observations as belonging to category 0. Then the sensitivity would be 1,meaning that all observations that belong to category 0 are predicted to be category 0. So there need to be a balance because we do not want this\n\nSpecificity gives the percent of time the an observation does not belong to a specific category and the model predicts it as not belonging to that category. High specificity(close to the maximum of 1) is analogous to a test rarely every flagging someone as having the flu when they don't. A perfect model will have high sensitivity with a high specificity, meaning that it is able to identify when observations belong to a category and does not incorrectly predict other observations as belonging to that category. The table above indicates that all the categories have relatively high specificity. Class 4 has a specificity of 0.996 which on the surface looks great but because the sensitivity is so low, this high specificity just means that the model rarely every predicts any observations as belonging to category 4. The same follows for category 3. This is also seen in the extremely low detection rate and prevalence for these categories. These results, specifically for categories 3 and 4 are an indication that perhaps just reweighting the classes was not enough to overcome the imbalance in the data.\n\nA better metric for instances such as this where the data is imbalanced is the F1 score. The F1 score is the harmonic mean of the precision and recall. The precision is the percentage of observations that belonged to a category and were correctly predicted as belonging to that category. The recall is just the sensitivity. The F1 score, same as the other metrics, ranges from 0 to 1 with 0 being the worst and 1 being the best. The F1 score is useful because the harmonic mean because it is less affected by extreme values than the arithmetic mean. Categories 4 and 3 are by far the worst with values of 0,104 and 0,119. Category 0 has the best F1 score with a value of 0,716. Then category 1 has a value of 0,383 and category 3 has a score of 0,487.\n\n\nAll these values, except for category are quite poor and indicate lack of predictive power. This is reflected in the fact that the model achieved an accuracy of 51,7% on the test set. The model is better than blindly guessing which we expect to return an accuracy of 20% but the model lacks predictive power.\n\n\n\n### 3.10 Interpretation\n\nThe results highlight the challenges of severe class imbalance. While class reweighting improved performance somewhat, minority classes remained poorly predicted. This suggests that more aggressive strategies may be necessary, such as resampling (e.g., bootstrapping or SMOTE), expanding the hyperparameter search space, or exploring alternative architectures.\n\nDespite these limitations, the study provides useful insights. It demonstrates that weather variables are the dominant predictors of avalanche hazard and that neural networks can achieve moderate accuracy with relatively simple architectures. However, improving detection of rare but high-consequence hazard categories remains an important direction for future work.\n\n\n### Extra resources\n\n[`keras`](https://cran.r-project.org/web/packages/keras/vignettes/){target=\"_blank\"}\n\n[`kerastuneR`](https://eagerai.github.io/kerastuneR/){target=\"_blank\"}\n\n\n\n---\ntitle: \"\"  # Empty title since we use custom hero\nformat: \n  html:\n    page-layout: full\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n::: {.cell}\n\n:::\n\n\n\n## 4 Discussion and Conclusion {#sec-Conclusion}\n\n### 4.1 Discussion\n\nFrom the results, it can be seen that the neural network are capable and accurate for predicting avalanche hazard levels. They learn and find meaningful patterns and interaction through the diverse predictor sets of meteorological, snowpack and topographical features. \n\nHowever, several limitations could be observed. Classification accuracy was uneven across hazard categories. The model was accurate at predicting low and moderate hazard levels however misclassification rates increased for higher hazards. This issue occurs due to class imbalance and reduced training signal due to the scarsity of high level hazards in the dataset. It is therefore suggested that resampling strategies could be implemented to improve the model classification accuracy. Another important limitation is the interpretability of neural networks. Neural networks are treated as black boxes and therefore interpretability is limited for vital decision making.\n\nThe results from this investigation are positive for neural networks. This method is however complex and computationally expensive. It is therefore vital that there is collaboration between data scientists and experts to integrate the model to complement existing methods rather than replace them all together.\n\n### 4.2 Conclusion\n\nThis project showcased the vital application of data-driven methods, such as neural networks, to predict avalanche hazard levels. The neural network model developed was successful at integrating all predictor features, such as meteorological, snowpack and topographical features, to find patterns and interactions that can be used to predict avalanche hazard levels. Challenges such as class imbalance, limited performance with high hazard levels and interpretability were observed but dispite these challenges, the model was still proven to be highly accurate.\n\n\n\n---\ntitle: \"\"\nformat: html\n\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n::: {.cell}\n\n:::\n\n\n\n## 5 Use of Large Language Models (LLMs) {#sec-LLMs_discussion}\n\n### 5.1 Data Cleaning & Exploratory Data Analysis (EDA)\n\n-   In addition to consulting journals and websites (*references*), LLMs were used to understand the meaning of each variable, their measurement units, plausible value ranges, and to help identify potentially erroneous values in the dataset.\n\n-   LLMs assisted in assessing the reliability of variables, and whether categorical groupings of certain variables showed signs of being meaningful. Example prompt: \"How would I interpret this wetness variable: values -1, 0, 1, 2, 3, 4, 6, 10; respective counts: 51, 363, 6384, 3007, 27, 7, 2, 166.\"\n\n-   Provided suggestions for handling outliers and missing (NA) values.\n\n-   Helped write repetitive or time-consuming R code. Example prompt: \"Provide R code that will generate side-by-side boxplots for all numeric variables in the dataset.\"\n\n-   Improved visualisation aesthetics. Example prompt: \"Improve the presentation of this boxplot: 'code'.\"\n\n-   Assisted in identifying and fixing coding errors. Example prompt: \"Locate the error in this code chunk and return the corrected code: 'code'.\"\n\n### 5.2 GitHub Support\n\nLLMs provided assistance when we were dealing with GitHub issues, including resolving merge conflicts and other version control challenges. Example prompt: \"I’m trying to commit my work to the shared Git Hub repository. What does this error from my terminal in R studio mean: ‘error message’.\"\n\n### 5.3 Website Design\n\nLLMs supported the development of the website by translating design ideas into functional code. They were also useful for troubleshooting rendering issues when the website did not display in the way that its was expected to.\n\n### 5.4 Referencing\n\nLLMs assisted in creating references for the sources we used. We provided the necessary information to Claude.ai, which then formatted the references in Harvard style.\n\n### 5.5 Writing Up\n\nLLMs were used to help improve the written sections of the report. They assisted with fixing grammar, making text clearer, improving flow, and suggesting more concise wording. Example prompts included: \"Grammar check: ...\", \"Improve the way this sentence is written: ...\", and \"Make this section flow better: ...\".\n\nIn addition, LLMs were sometimes used to transform sets of bullet points into coherent, well-structured paragraphs, helping to organize ideas in a way that was easy to follow.\n\nLLMs were used for structuring report sections and generating README templates, clearly outlined requirements guided the model's output. In all cases, the results were critically evaluated and refined, with final decisions and implementations made independently.\n\nWhile LLMs contributed in these ways, all code and written content were carefully reviewed and edited to ensure its accuracy and that it clearly conveyed our thought processes, ideas, and reasonings.\n\n### 5.6 Other points\n\n-   They helped summarise and explain other group members’ work, allowing us to quickly stay up to date with the progress made on the project.\n-   For data visualisation, an efficiency-driven approach was adopted—specific outcomes were defined in advance, and the LLM was used to generate production-ready code aligned with those goals. For example, in creating plots, detailed prompts such as \"Create a professional ggplot that visualises validation accuracy across different hyperparameter combinations, clearly highlighting the best-performing combination.\" yielded well-formatted visualizations, saving time on aesthetic adjustments.\n\n\n\n---\ntitle: \"References\"\nformat: html\neditor: visual\n---\n\n\n## References {#sec-References}\n\n\\[1\\] Bellaire, S. and Jamieson, J.B. (2013) 'Forecasting avalanche danger: recent advances', *Cold Regions Science and Technology*, 85, pp. 89-102.\n\n\\[2\\] Cervinski, M.A., Bietenbeck, A., Katayev, A., Loh, T.P., van Rossum, H.H. and Badrick, T. (2023) 'Advances in clinical chemistry patient-based real-time quality control (PBRTQC)', in Makowski, G.S. (ed.) *Advances in Clinical Chemistry*. 1st ed. Amsterdam: Elsevier, pp. 223-261. doi: 10.1016/bs.acc.2023.08.003.\n\n\\[3\\] Fierz, C., Armstrong, R.L., Durand, Y., Etchevers, P., Greene, E., McClung, D.M., Nishimura, K., Satyawali, P.K. and Sokratov, S.A. (2009) *The international classification for seasonal snow on the ground*. IHP-VII Technical Documents in Hydrology No. 83, IACS Contribution No. 1. Paris: UNESCO-IHP.\n\n\\[4\\] Hafner, E., Bühler, Y. and Schweizer, J. (2021) 'Machine learning in avalanche research: random forests and SVMs for avalanche forecasting', *Natural Hazards and Earth System Sciences*, 21, pp. 223-239.\n\n\\[5\\] Kenward, M.G. and Carpenter, J. (2007) 'Multiple imputation: current perspectives', *Statistical Methods in Medical Research*, 16(3), pp. 199-218. doi: 10.1177/0962280206075304.\n\n\\[6\\] King, S. (2024) 'Deadliest, most intense, windiest: Three of the UK's worst storms', *BBC Weather*, 17 October. Available at: https://www.bbc.com/weather/articles/cvglvnyxpx0o (Accessed: 28 September 2025).\n\n\\[7\\] LaChapelle, E. (1980) 'The fundamental processes in conventional avalanche forecasting', *Journal of Glaciology*, 26(94), pp. 75-84.\n\n\\[8\\] McClung, D. (2002) 'The elements of applied avalanche forecasting: the human issues', *Natural Hazards*, 26, pp. 111-129.\n\n\\[9\\] Mitterer, C. et al. (2016) 'Deep learning approaches for snowpack characterization', *Proceedings of the International Snow Science Workshop*.\n\n\\[10\\] Molenberghs, G. and Kenward, M.G. (2006) *Missing data in clinical studies*. Chichester: Wiley Statistics in Practice.\n\n\\[11\\] Morning, H. (no date) 'Wind speeds in the mountains', *Mountaineering Scotland*. Available at: https://www.mountaineering.scot/safety-and-skills/essential-skills/weather-conditions/wind-speeds (Accessed: 28 September 2025).\n\n\\[12\\] SAIS (2014) *Interpreting snow profiles*. \\[pdf\\] Scottish Avalanche Information Service. Available at: https://www.sais.gov.uk/wp-content/uploads/2014/11/interpreting_snow_profiles.pdf (Accessed: 28 September 2025).\n\n\\[13\\] Schweizer, J. and Jamieson, J.B. (2007) 'A review of stability tests for snow profiles', *Cold Regions Science and Technology*, 47, pp. 14-37.\n\n\\[14\\] ScienceDirect (no date) 'Winsorization – an overview', *ScienceDirect Topics*. Available at: https://www.sciencedirect.com/topics/mathematics/winsorization (Accessed: 27 September 2025).\n\n\\[15\\] Ward, R.G.W., Langmuir, E.D.G. and Beattie, B. (1985) 'Snow profiles and avalanche activity in the Cairngorm Mountains, Scotland', *Journal of Glaciology*, 31(107), pp. 18-27. doi: 10.1177/S0022143000004949.\n\n\n\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}