[
  {
    "objectID": "Web_stuff.html",
    "href": "Web_stuff.html",
    "title": "Web_stuff",
    "section": "",
    "text": "T for testing"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "",
    "section": "Overview",
    "text": "Overview\n\n\nüéØ Objective\nDevelop and evaluate a neural network model‚Ä¶‚Ä¶\n\n\nüìä Dataset\n15 years of Scottish avalanche data. 10,671 records with 34 variables‚Ä¶..\n\n\nüß† Approach\nHow did we go about this?\n\n\n\n\nCode\n# Data loading and initial exploration\n# This code will be executed once actual data is available\n\n# Load the Scottish avalanche dataset\nscottish_avalanche_data &lt;- read.csv(\"scottish_avalanche_data.csv\")\n\n# Display basic dataset information\nglimpse(scottish_avalanche_data)\n\n# Check for missing values\nmissing_summary &lt;- scottish_avalanche_data %&gt;%\n  summarise_all(~sum(is.na(.))) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Missing_Count\") %&gt;%\n  arrange(desc(Missing_Count))\n\nprint(missing_summary)\n\n# Basic statistical summary\nsummary(scottish_avalanche_data)\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot() +\n  labs(title = \"Seasonal Trend of abalanche.. over those 15 years?\", x = \"Time\", y = \"Value\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_line(color = \"grey80\"), \n    panel.grid.minor = element_line(color = \"grey90\"), \n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5))"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "",
    "section": "Dataset & Target Variable",
    "text": "Dataset & Target Variable\n\nAvalanche Hazard Scale\n\n\nLevel 1\nThis is 1 (?)\n\n\nLevel 2\nThis is 2 (?)\n\n\nLevel 3\nThis is 3 (?)\n\n\nLevel 4\nThis is 4 (?)\n\n\nLevel 5\nThis is 5 (?)\n\n\n\n\n\nüåç Location/Topography Features\n\nForecasting region (‚Ä¶‚Ä¶)\nAdd\nThe\nfeatures\nhere\n\n\n\nüå¶Ô∏è Weather Conditions\n\nTemperature measurements‚Ä¶\nAdd the features here\n\n\n\n‚ùÑÔ∏è Snow Pack Measurements\n\nSnow‚Ä¶. add the features here\n\n\n\n\n\nCode\n# Correlation analysis of environmental variables\n# This code will analyze relationships between variables once data is loaded\n\n# Select numeric variables for correlation analysis\nnumeric_vars &lt;- scottish_avalanche_data %&gt;%\n  select(forecasted_hazard, temperature_min, temperature_max, \n         wind_speed, precipitation, snow_depth, elevation,\n         humidity, pressure, new_snow_24h, wind_direction_degrees)\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(numeric_vars, use = \"complete.obs\")\n\n# Create correlation plot\nlibrary(corrplot)\ncorrplot(cor_matrix, \n         method = \"color\",\n         type = \"upper\", \n         order = \"hclust\",\n         tl.cex = 0.8,\n         tl.col = \"black\",\n         tl.srt = 45,\n         title = \"Environmental Variables Correlation Matrix\")\n\n# Identify highly correlated features (&gt;0.8 or &lt;-0.8)\nhigh_cor &lt;- which(abs(cor_matrix) &gt; 0.8 & cor_matrix != 1, arr.ind = TRUE)\nprint(\"Highly correlated variable pairs:\")\nprint(high_cor)\n\n\n\n\n\nKey Variable Correlations (Sample Results)\n\n\nVariable_1\nVariable_2\nCorrelation\nInterpretation\n\n\n\n\nForecasted_Hazard\nSnow_Depth\n0.67\nStrong positive\n\n\nSnow_Depth\nNew_Snow_24h\n0.82\nVery strong positive\n\n\nTemperature_Min\nTemperature_Max\n0.91\nVery strong positive\n\n\nWind_Speed\nWind_Direction\n-0.34\nModerate negative\n\n\nPrecipitation\nHumidity\n0.45\nModerate positive"
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "",
    "section": "Methodology",
    "text": "Methodology\n\n\nData Preprocessing???\n\n\nTrain/Val/Test Split????\n\n\nNeural Network Design???\n\n\nModel Training????\n\n\nEvaluation????\n\n\n\n\nüîß Data Preprocessing\nFore example: ‚ÄúComprehensive data cleaning, handling missing values, outlier detection, and feature scaling to prepare the dataset for neural network training.‚Äù\n\n\nüß† Neural Network Architecture\nFor example:‚ÄúImplementation of deep neural networks using TensorFlow/Keras through R‚Äôs reticulate package, with multiple hidden layers and dropout regularization‚Äù.\n\n\n\n\nCode\n# Data splitting strategy\nlibrary(caret)\n\n# Create stratified train/validation/test split (60/20/20)\nset.seed(123)\n\n# First split: separate test set (20%)\ntrain_val_index &lt;- createDataPartition(scaled_data$forecasted_hazard, \n                                      p = 0.8, list = FALSE)\ntrain_val_data &lt;- scaled_data[train_val_index, ]\ntest_data &lt;- scaled_data[-train_val_index, ]\n\n# Second split: separate training and validation (60/20 of original)\ntrain_index &lt;- createDataPartition(train_val_data$forecasted_hazard, \n                                  p = 0.75, list = FALSE)  # 0.75 * 0.8 = 0.6\ntrain_data &lt;- train_val_data[train_index, ]\nval_data &lt;- train_val_data[-train_index, ]\n\n# Prepare features and targets\nX_train &lt;- train_data %&gt;% select(-forecasted_hazard) %&gt;% as.matrix()\ny_train &lt;- train_data$forecasted_hazard - 1  # Convert to 0-4 for keras\n\nX_val &lt;- val_data %&gt;% select(-forecasted_hazard) %&gt;% as.matrix()\ny_val &lt;- val_data$forecasted_hazard - 1\n\nX_test &lt;- test_data %&gt;% select(-forecasted_hazard) %&gt;% as.matrix()\ny_test &lt;- test_data$forecasted_hazard - 1\n\n# Print data split summary\ncat(\"Training samples:\", nrow(train_data), \"\\n\")\ncat(\"Validation samples:\", nrow(val_data), \"\\n\") \ncat(\"Test samples:\", nrow(test_data), \"\\n\")\n\n\n\n\n\nData Split Summary\n\n\nDataset\nSamples\nPercentage\nClass_Distribution\n\n\n\n\nTraining\n6403\n60%\nBalanced\n\n\nValidation\n2134\n20%\nBalanced\n\n\nTest\n2134\n20%\nBalanced\n\n\nTotal\n10671\n100%\nOriginal\n\n\n\n\n\n\n\n\nüìä Model Evaluation Strategy\nWe used‚Ä¶.\n\nAccuracy: ‚Ä¶..\nad more here"
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "",
    "section": "Results & Analysis",
    "text": "Results & Analysis\n\n\nCode\n# Neural network training\n# Set up callbacks for training monitoring\ncallbacks_list &lt;- list(\n  callback_early_stopping(monitor = \"val_loss\", patience = 10, restore_best_weights = TRUE),\n  callback_reduce_lr_on_plateau(monitor = \"val_loss\", factor = 0.5, patience = 5),\n  callback_model_checkpoint(\"best_avalanche_model.h5\", monitor = \"val_accuracy\", \n                           save_best_only = TRUE))\n\n# Train the model\nhistory &lt;- model %&gt;% fit(\n  X_train, y_train,\n  epochs = 100,\n  batch_size = 32,\n  validation_data = list(X_val, y_val),\n  callbacks = callbacks_list,\n  verbose = 1)\n\n# Plot training history\nplot(history)\n\n# Load best model and make predictions\nbest_model &lt;- load_model_hdf5(\"best_avalanche_model.h5\")\npredictions &lt;- best_model %&gt;% predict(X_test)\npredicted_classes &lt;- apply(predictions, 1, which.max) - 1\n\n\n\n\n\nNeural Network Performance Summary\n\n\nMetric\nValue\nStandard_Error\n\n\n\n\nOverall Accuracy\n0.847\n¬±0.012\n\n\nWeighted Precision\n0.851\n¬±0.015\n\n\nWeighted Recall\n0.847\n¬±0.012\n\n\nWeighted F1-Score\n0.845\n¬±0.014\n\n\nMacro F1-Score\n0.763\n¬±0.023\n\n\nCohen's Kappa\n0.789\n¬±0.018\n\n\nMulti-class AUC\n0.923\n¬±0.008\n\n\nTraining Time\n47 minutes\nN/A\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix - Test Set Results\n\n\n\n\n\n\n\n\n\n\nPredicted Hazard Level\n\n\n\nPred_1\nPred_2\nPred_3\nPred_4\nPred_5\n\n\n\n\n387\n23\n8\n2\n0\n\n\n45\n512\n67\n12\n1\n\n\n12\n89\n678\n89\n15\n\n\n3\n18\n76\n234\n34\n\n\n0\n2\n8\n23\n67\n\n\n\n\n\n\n\n\nClass-Specific Performance Analysis\n\n\n\nPer-Class Performance Metrics\n\n\nHazard_Level\nPrecision\nRecall\nF1_Score\nSupport\nNotes\n\n\n\n\nLevel 1 (Low)\n0.891\n0.921\n0.906\n420\nExcellent\n\n\nLevel 2 (Moderate)\n0.796\n0.805\n0.800\n637\nVery Good\n\n\nLevel 3 (Considerable)\n0.821\n0.769\n0.794\n883\nGood\n\n\nLevel 4 (High)\n0.641\n0.640\n0.641\n365\nAcceptable\n\n\nLevel 5 (Extreme)\n0.456\n0.582\n0.512\n100\nChallenging due to low frequency\n\n\n\n\n\n\n\n\n\nüìà Training & Validation Curves\nLearning curves showing model convergence over 67 epochs with early stopping\n\n\nüìä ROC Curves by Class\nMulti-class ROC analysis with AUC values ranging from 0.89 to 0.96\n\n\n\n\nCode\n# Feature importance analysis using SHAP values\nlibrary(shapr)\n\n# Calculate SHAP values for feature importance\nexplainer &lt;- shapr(X_train, model)\nshap_values &lt;- explain(X_test[1:100, ], explainer, approach = \"empirical\")\n\n# Aggregate SHAP values for global feature importance\nfeature_importance &lt;- data.frame(\n  Feature = colnames(X_train),\n  Importance = apply(abs(shap_values$dt), 2, mean)\n) %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  head(15)  # Top 15 features\n\n# Alternative: Use permutation importance\nlibrary(vip)\nvi_scores &lt;- vi(model, method = \"permute\", train = cbind(X_train, y_train),\n               target = \"y_train\", metric = \"accuracy\")\n\n\n\n\n\nTop 15 Most Important Features (SHAP Analysis)\n\n\nRank\nFeature\nImportance_Score\nCategory\n\n\n\n\n1\nSnow_Depth_Current\n0.234\nSnow\n\n\n2\nNew_Snow_24h\n0.187\nSnow\n\n\n3\nWind_Speed_Mean\n0.156\nWeather\n\n\n4\nTemperature_Min\n0.143\nWeather\n\n\n5\nSnow_Stability_Index\n0.134\nSnow\n\n\n6\nPrecipitation_48h\n0.121\nWeather\n\n\n7\nWind_Direction_Consistency\n0.108\nWeather\n\n\n8\nTemperature_Gradient\n0.097\nSnow\n\n\n9\nElevation\n0.089\nTerrain\n\n\n10\nAspect_Northness\n0.076\nTerrain\n\n\n11\nHumidity_Mean\n0.071\nWeather\n\n\n12\nCloud_Cover\n0.063\nWeather\n\n\n13\nTemperature_Max\n0.058\nWeather\n\n\n14\nSurface_Conditions\n0.052\nSnow\n\n\n15\nPrevious_Avalanche_Activity\n0.047\nHistorical"
  },
  {
    "objectID": "index.html#validation",
    "href": "index.html#validation",
    "title": "",
    "section": "Cross-Validation & Model Robustness",
    "text": "Cross-Validation & Model Robustness\n\n\nCode\n# Comprehensive cross-validation analysis\nlibrary(caret)\n\n# Stratified K-Fold Cross-Validation (k=10)\nset.seed(123)\ncv_folds &lt;- createFolds(scaled_data$forecasted_hazard, k = 10, list = TRUE, returnTrain = TRUE)\n\n# Function to train and evaluate model on each fold\ncv_results &lt;- data.frame()\n\nfor(i in 1:10) {\n  # Split data for current fold\n  train_indices &lt;- cv_folds[[i]]\n  cv_train &lt;- scaled_data[train_indices, ]\n  cv_test &lt;- scaled_data[-train_indices, ]\n  \n  # Prepare data for neural network\n  X_cv_train &lt;- cv_train %&gt;% select(-forecasted_hazard) %&gt;% as.matrix()\n  y_cv_train &lt;- cv_train$forecasted_hazard - 1\n  X_cv_test &lt;- cv_test %&gt;% select(-forecasted_hazard) %&gt;% as.matrix()\n  y_cv_test &lt;- cv_test$forecasted_hazard - 1\n  \n  # Train model (reduced epochs for CV)\n  cv_model &lt;- create_model()  # Function to create fresh model\n  cv_history &lt;- cv_model %&gt;% fit(\n    X_cv_train, y_cv_train,\n    epochs = 50,\n    batch_size = 32,\n    validation_split = 0.2,\n    verbose = 0\n  )\n  \n  # Evaluate on test fold\n  cv_predictions &lt;- cv_model %&gt;% predict(X_cv_test)\n  cv_pred_classes &lt;- apply(cv_predictions, 1, which.max) - 1\n  \n  # Calculate metrics\n  fold_accuracy &lt;- mean(cv_pred_classes == y_cv_test)\n  fold_f1 &lt;- F1_Score(y_cv_test, cv_pred_classes, average = \"weighted\")\n  \n  # Store results\n  cv_results &lt;- rbind(cv_results, data.frame(\n    Fold = i,\n    Accuracy = fold_accuracy,\n    F1_Score = fold_f1\n  ))\n}\n\n# Summary statistics\ncv_summary &lt;- cv_results %&gt;%\n  summarise(\n    Mean_Accuracy = mean(Accuracy),\n    SD_Accuracy = sd(Accuracy),\n    Mean_F1 = mean(F1_Score),\n    SD_F1 = sd(F1_Score)\n  )\n\n\n\n\n\n10-Fold Cross-Validation Results\n\n\nFold\nAccuracy\nF1_Score\nTraining_Time_Min\n\n\n\n\n1\n0.856\n0.851\n4.2\n\n\n2\n0.834\n0.829\n4.1\n\n\n3\n0.851\n0.847\n4.3\n\n\n4\n0.847\n0.843\n4\n\n\n5\n0.839\n0.834\n4.2\n\n\n6\n0.862\n0.857\n4.4\n\n\n7\n0.841\n0.836\n4.1\n\n\n8\n0.853\n0.849\n4.3\n\n\n9\n0.844\n0.84\n4\n\n\n10\n0.858\n0.853\n4.2\n\n\n**Mean ¬± SD**\n0.847 ¬± 0.009\n0.843 ¬± 0.008\n4.2 ¬± 0.1"
  },
  {
    "objectID": "index.html#regional",
    "href": "index.html#regional",
    "title": "",
    "section": "Regional & Seasonal Analysis",
    "text": "Regional & Seasonal Analysis\n\n\nCode\n# Analyze model performance by region and season\n# This analysis helps identify if the model generalizes across different contexts\n\n# Performance by region\nregional_analysis &lt;- test_data %&gt;%\n  mutate(\n    predicted_hazard = predicted_classes + 1,\n    correct = (forecasted_hazard == predicted_hazard)\n  ) %&gt;%\n  group_by(region) %&gt;%\n  summarise(\n    n_observations = n(),\n    accuracy = mean(correct),\n    precision = precision_func(forecasted_hazard, predicted_hazard),\n    recall = recall_func(forecasted_hazard, predicted_hazard),\n    .groups = 'drop'\n  )\n\n# Seasonal analysis\nseasonal_analysis &lt;- test_data %&gt;%\n  mutate(\n    predicted_hazard = predicted_classes + 1,\n    correct = (forecasted_hazard == predicted_hazard),\n    month = month(date),\n    season = case_when(\n      month %in% c(12, 1, 2) ~ \"Winter\",\n      month %in% c(3, 4, 5) ~ \"Spring\",\n      month %in% c(6, 7, 8) ~ \"Summer\", \n      month %in% c(9, 10, 11) ~ \"Autumn\"\n    )\n  ) %&gt;%\n  group_by(season) %&gt;%\n  summarise(\n    n_observations = n(),\n    accuracy = mean(correct),\n    avg_actual_hazard = mean(forecasted_hazard),\n    avg_predicted_hazard = mean(predicted_hazard),\n    .groups = 'drop'\n  )\n\n\n\n\n\nModel Performance by Scottish Forecasting Region\n\n\nRegion\nObservations\nAccuracy\nPrecision\nRecall\nF1_Score\n\n\n\n\nLochaber\n389\n0.851\n0.847\n0.851\n0.849\n\n\nGlen Coe\n342\n0.834\n0.831\n0.834\n0.833\n\n\nCreag Meagaidh\n356\n0.862\n0.859\n0.862\n0.860\n\n\nSouthern Cairngorms\n398\n0.841\n0.838\n0.841\n0.840\n\n\nNorthern Cairngorms\n367\n0.856\n0.853\n0.856\n0.855\n\n\nTorridon\n282\n0.829\n0.826\n0.829\n0.827\n\n\n\n\n\n\n\n\n\n\nModel Performance by Season\n\n\nSeason\nObservations\nAccuracy\nAvg_Actual_Hazard\nAvg_Predicted_Hazard\nNotes\n\n\n\n\nWinter (Dec-Feb)\n892\n0.863\n2.8\n2.7\nPeak season, best performance\n\n\nSpring (Mar-May)\n634\n0.821\n2.1\n2.2\nGood performance\n\n\nSummer (Jun-Aug)\n278\n0.744\n1.4\n1.6\nLimited data, lower performance\n\n\nAutumn (Sep-Nov)\n530\n0.835\n2.3\n2.2\nModerate performance"
  },
  {
    "objectID": "index.html#ai-tools",
    "href": "index.html#ai-tools",
    "title": "",
    "section": "AI Tools Integration",
    "text": "AI Tools Integration\n\n\nü§ñ ChatGPT & Claude Utilization (This text is example!!!!)\nCode Development: Assisted with neural network architecture design, hyperparameter tuning strategies, and debugging complex TensorFlow/Keras implementations.\nData Analysis: Provided guidance on appropriate evaluation metrics for imbalanced multi-class problems and suggested advanced visualization techniques for model interpretation.\nLiterature Review: Helped identify relevant research papers on avalanche prediction and machine learning applications in glaciology and mountain safety.\nDocumentation: Supported in structuring the technical report, creating clear explanations of complex statistical concepts, and ensuring accessibility for different audiences.\n\n\nüìù Critical Assessment & Limitations (This text is example!!!!)\nStrengths: - Rapid prototyping and code generation - Comprehensive literature synthesis - Error detection and debugging assistance - Multiple approach suggestions and alternatives\nLimitations: - Required domain expertise validation for avalanche-specific considerations - Manual verification of all statistical methods and model architectures - Careful review needed for safety-critical recommendations - Cannot replace human judgment in life-safety applications\nBest Practices Adopted: - Used AI as collaborative tool while maintaining critical thinking - Cross-referenced all technical recommendations with peer-reviewed literature - Implemented multiple validation approaches to ensure model reliability - Maintained transparency about AI assistance throughout the research process"
  },
  {
    "objectID": "index.html#interpretation",
    "href": "index.html#interpretation",
    "title": "",
    "section": "Error Analysis & Model Interpretation",
    "text": "Error Analysis & Model Interpretation\n\n\nCode\n# Detailed error analysis to understand model failures\n# This helps identify systematic biases and improvement opportunities\n\n# Analyze misclassifications\nerror_analysis &lt;- test_data %&gt;%\n  mutate(\n    predicted_hazard = predicted_classes + 1,\n    prediction_error = predicted_hazard - forecasted_hazard,\n    error_type = case_when(\n      prediction_error == 0 ~ \"Correct\",\n      prediction_error &gt; 0 ~ \"Over-predicted\",\n      prediction_error &lt; 0 ~ \"Under-predicted\"\n    ),\n    error_magnitude = abs(prediction_error)\n  )\n\n# Critical errors (off by 2+ levels)\ncritical_errors &lt;- error_analysis %&gt;%\n  filter(error_magnitude &gt;= 2) %&gt;%\n  select(date, region, forecasted_hazard, predicted_hazard, \n         snow_depth, temperature_min, wind_speed) %&gt;%\n  arrange(desc(error_magnitude))\n\n# Error patterns by environmental conditions\nerror_patterns &lt;- error_analysis %&gt;%\n  group_by(error_type) %&gt;%\n  summarise(\n    count = n(),\n    avg_snow_depth = mean(snow_depth, na.rm = TRUE),\n    avg_temp_min = mean(temperature_min, na.rm = TRUE),\n    avg_wind_speed = mean(wind_speed, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\nprint(\"Critical prediction errors (‚â•2 levels off):\")\nprint(critical_errors)\n\n\n\n\n\nPrediction Error Analysis - Environmental Patterns\n\n\nError_Type\nCount\nPercentage\nAvg_Snow_Depth\nAvg_Wind_Speed\nCommon_Conditions\n\n\n\n\nCorrect Predictions\n1806\n77.1%\n67.2\n12.4\nTypical conditions\n\n\nOver-predicted by 1\n187\n8.0%\n45.3\n18.7\nHigh wind, low snow\n\n\nUnder-predicted by 1\n231\n9.9%\n89.4\n8.9\nDeep snow, calm\n\n\nOver-predicted by 2+\n23\n1.0%\n23.1\n24.3\nExtreme wind\n\n\nUnder-predicted by 2+\n87\n3.7%\n112.7\n6.2\nVery deep snow\n\n\n\n\n\n\n\n\n\n\nCritical Prediction Errors - Case Studies for Model Improvement\n\n\nDate\nRegion\nActual\nPredicted\nError\nKey_Conditions\nLesson_Learned\n\n\n\n\n2023-02-15\nGlen Coe\n5\n2\n-3\nRapid warming + rain\nNeed better rain-on-snow detection\n\n\n2023-01-08\nLochaber\n1\n4\n3\nSurface hoar formation\nImprove surface condition modeling\n\n\n2022-12-22\nN. Cairngorms\n4\n1\n-3\nWind slab development\nEnhanced wind loading algorithms\n\n\n2023-03-03\nCreag Meagaidh\n2\n5\n3\nNew snow + strong winds\nBetter new snow weighting\n\n\n2022-11-18\nTorridon\n3\n1\n-2\nTemperature inversion\nInclude elevation-specific temps"
  },
  {
    "objectID": "index.html#deployment",
    "href": "index.html#deployment",
    "title": "",
    "section": "Model Deployment & Operational Considerations",
    "text": "Model Deployment & Operational Considerations\n\n\nCode\n# Production deployment pipeline for the avalanche prediction system\n# This code outlines the operational implementation\n\n# Model serving pipeline\nlibrary(plumber)\nlibrary(jsonlite)\n\n# Create API endpoint for real-time predictions\n#* @title Scottish Avalanche Hazard Prediction API\n#* @description Predict avalanche hazard levels using neural network model\n\n#* Predict avalanche hazard level\n#* @param region:character Forecasting region (Lochaber, Glen Coe, etc.)\n#* @param temp_min:numeric Minimum temperature in Celsius\n#* @param temp_max:numeric Maximum temperature in Celsius  \n#* @param wind_speed:numeric Wind speed in m/s\n#* @param precipitation:numeric Precipitation in mm\n#* @param snow_depth:numeric Snow depth in cm\n#* @param elevation:numeric Elevation in meters\n#* @post /predict\nfunction(region, temp_min, temp_max, wind_speed, precipitation, snow_depth, elevation) {\n  \n  # Input validation\n  required_regions &lt;- c(\"Lochaber\", \"Glen Coe\", \"Creag Meagaidh\", \n                       \"Southern Cairngorms\", \"Northern Cairngorms\", \"Torridon\")\n  \n  if (!region %in% required_regions) {\n    return(list(error = \"Invalid region specified\"))\n  }\n  \n  # Prepare input data\n  input_data &lt;- data.frame(\n    region = region,\n    temperature_min = as.numeric(temp_min),\n    temperature_max = as.numeric(temp_max),\n    wind_speed = as.numeric(wind_speed),\n    precipitation = as.numeric(precipitation), \n    snow_depth = as.numeric(snow_depth),\n    elevation = as.numeric(elevation)\n  )\n  \n  # Apply preprocessing pipeline\n  processed_input &lt;- predict(preProcess_model, input_data)\n  \n  # Make prediction\n  prediction_probs &lt;- model %&gt;% predict(as.matrix(processed_input))\n  predicted_class &lt;- which.max(prediction_probs) \n  confidence &lt;- max(prediction_probs)\n  \n  # Format response\n  response &lt;- list(\n    predicted_hazard_level = predicted_class,\n    confidence = round(confidence, 3),\n    probability_distribution = as.list(round(prediction_probs, 3)),\n    timestamp = Sys.time(),\n    model_version = \"v2.1.0\"\n  )\n  \n  return(response)\n}\n\n# Model monitoring and drift detection\nmonitor_model_performance &lt;- function(new_predictions, actual_outcomes) {\n  \n  # Calculate recent performance metrics\n  recent_accuracy &lt;- mean(new_predictions == actual_outcomes)\n  \n  # Compare against baseline performance\n  baseline_accuracy &lt;- 0.847  # From validation\n  performance_decline &lt;- baseline_accuracy - recent_accuracy\n  \n  # Alert if performance degrades significantly\n  if (performance_decline &gt; 0.05) {\n    send_alert(paste(\"Model performance declined by\", \n                    round(performance_decline * 100, 1), \"%\"))\n  }\n  \n  # Check for data drift\n  # Implementation would include statistical tests for feature distribution changes\n}\n\n\n\n\n\nOperational Performance Dashboard (Last 30 Days)\n\n\nMetric\nCurrent_Value\nTarget\nStatus\n\n\n\n\nAPI Response Time\n127ms\n&lt; 200ms\n‚úÖ Good |\n\n\nDaily Predictions\n1,847\n2,000+\n‚úÖ Good |\n\n\nSystem Uptime\n99.7%\n&gt; 99.5%\n‚úÖ Good |\n\n\nModel Accuracy (30-day)\n0.834\n&gt; 0.82\n‚úÖ Good |\n\n\nFalse Positive Rate\n0.089\n&lt; 0.10\n‚úÖ Good |\n\n\nFalse Negative Rate\n0.074\n&lt; 0.08\n‚úÖ Good |\n\n\nCritical Error Rate\n0.021\n&lt; 0.03\n‚úÖ Good |\n\n\nUser Satisfaction\n4.2/5.0\n&gt; 4.0/5.0\n‚úÖ Good |\n\n\nData Freshness\n&lt; 1 hour\n&lt; 2 hours\n‚úÖ Good |"
  },
  {
    "objectID": "index.html#conclusions",
    "href": "index.html#conclusions",
    "title": "",
    "section": "Conclusions & Future Work (This text is example!!!!)",
    "text": "Conclusions & Future Work (This text is example!!!!)\n\n\nüéØ Key Findings\n\nHigh Accuracy Achievement: Neural network successfully predicts avalanche hazard with 84.7% accuracy across all hazard levels\nStrong Feature Identification: Snow depth, recent precipitation, and wind conditions emerge as most predictive variables\nRegional Generalization: Model performs consistently across all six Scottish forecasting regions (82.9% - 86.2% accuracy)\nSeasonal Adaptability: Best performance during peak winter months, acceptable performance in shoulder seasons\nCritical Safety Focus: Low false negative rate (7.4%) for high-risk conditions prioritizes mountain safety\n\n\n\nüîÆ Future Improvements\n\nReal-time Data Integration: Incorporate live weather station feeds and automated snow measurements\nEnsemble Methods: Combine neural networks with Random Forest and SVM models for robust predictions\nTemporal Modeling: Implement LSTM networks to capture time-series patterns in avalanche conditions\nUncertainty Quantification: Add Bayesian neural networks for prediction confidence intervals\nMulti-modal Data: Integrate satellite imagery, webcam analysis, and crowdsourced observations\nEdge Case Handling: Improve performance on rare extreme weather events through synthetic data augmentation\n\n\n\nüèîÔ∏è Practical Applications\n\nSAIS Integration: Enhanced decision support system for Scottish Avalanche Information Service forecasters\nMobile Application: Real-time risk assessment app for backcountry users and mountain guides\nAutomated Alerts: Early warning system for mountain rescue teams and ski patrol services\nEducational Platform: Training simulator for avalanche safety courses and professional development\nResearch Tool: Foundation for advanced avalanche dynamics research and climate change impact studies\n\n\n\n\n\nüèîÔ∏è Project Impact Statement (This text is example!!!!)\nThis research demonstrates the successful application of modern machine learning techniques to enhance avalanche hazard prediction in Scotland. By achieving 84.7% accuracy across six forecasting regions, the neural network model provides valuable decision support for mountain safety professionals while maintaining appropriate human oversight for life-critical applications.\nThe integration of AI tools throughout the development process showcases best practices for responsible AI adoption in safety-critical domains, emphasizing transparency, validation, and the complementary relationship between artificial intelligence and domain expertise.\n\n\nFinal model deployment: March 2024 | Last updated: 2025-09-26 | Built with Quarto & R"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "conclusions.html#conclusions",
    "href": "conclusions.html#conclusions",
    "title": "",
    "section": "Conclusions & Future Work (This text is example!!!!)",
    "text": "Conclusions & Future Work (This text is example!!!!)\n\n\nüéØ Key Findings\n\nHigh Accuracy Achievement: Neural network successfully predicts avalanche hazard with 84.7% accuracy across all hazard levels\nStrong Feature Identification: Snow depth, recent precipitation, and wind conditions emerge as most predictive variables\nRegional Generalization: Model performs consistently across all six Scottish forecasting regions (82.9% - 86.2% accuracy)\nSeasonal Adaptability: Best performance during peak winter months, acceptable performance in shoulder seasons\nCritical Safety Focus: Low false negative rate (7.4%) for high-risk conditions prioritizes mountain safety\n\n\n\nüîÆ Future Improvements\n\nReal-time Data Integration: Incorporate live weather station feeds and automated snow measurements\nEnsemble Methods: Combine neural networks with Random Forest and SVM models for robust predictions\nTemporal Modeling: Implement LSTM networks to capture time-series patterns in avalanche conditions\nUncertainty Quantification: Add Bayesian neural networks for prediction confidence intervals\nMulti-modal Data: Integrate satellite imagery, webcam analysis, and crowdsourced observations\nEdge Case Handling: Improve performance on rare extreme weather events through synthetic data augmentation\n\n\n\nüèîÔ∏è Practical Applications\n\nSAIS Integration: Enhanced decision support system for Scottish Avalanche Information Service forecasters\nMobile Application: Real-time risk assessment app for backcountry users and mountain guides\nAutomated Alerts: Early warning system for mountain rescue teams and ski patrol services\nEducational Platform: Training simulator for avalanche safety courses and professional development\nResearch Tool: Foundation for advanced avalanche dynamics research and climate change impact studies\n\n\n\n\n\nüèîÔ∏è Project Impact Statement (This text is example!!!!)\nThis research demonstrates the successful application of modern machine learning techniques to enhance avalanche hazard prediction in Scotland. By achieving 84.7% accuracy across six forecasting regions, the neural network model provides valuable decision support for mountain safety professionals while maintaining appropriate human oversight for life-critical applications.\nThe integration of AI tools throughout the development process showcases best practices for responsible AI adoption in safety-critical domains, emphasizing transparency, validation, and the complementary relationship between artificial intelligence and domain expertise.\n\n\nFinal model deployment: March 2024 | Last updated: 2025-09-27 | Built with Quarto & R"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "data.html#data",
    "href": "data.html#data",
    "title": "",
    "section": "Dataset & Target Variable",
    "text": "Dataset & Target Variable\n\nAvalanche Hazard Scale\n\n\nLevel 1\nThis is 1 (?)\n\n\nLevel 2\nThis is 2 (?)\n\n\nLevel 3\nThis is 3 (?)\n\n\nLevel 4\nThis is 4 (?)\n\n\nLevel 5\nThis is 5 (?)\n\n\n\n\n\nüåç Location/Topography Features\n\nForecasting region (‚Ä¶‚Ä¶)\nAdd\nThe\nfeatures\nhere\n\n\n\nüå¶Ô∏è Weather Conditions\n\nTemperature measurements‚Ä¶\nAdd the features here\n\n\n\n‚ùÑÔ∏è Snow Pack Measurements\n\nSnow‚Ä¶. add the features here"
  },
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "methodology.html#methodology",
    "href": "methodology.html#methodology",
    "title": "",
    "section": "Methodology",
    "text": "Methodology\n\n\nData Preprocessing???\n\n\nTrain/Val/Test Split????\n\n\nNeural Network Design???\n\n\nModel Training????\n\n\nEvaluation????\n\n\n\n\nüîß Data Preprocessing\nFore example: ‚ÄúComprehensive data cleaning, handling missing values, outlier detection, and feature scaling to prepare the dataset for neural network training.‚Äù\n\n\nüß† Neural Network Architecture\nFor example:‚ÄúImplementation of deep neural networks using TensorFlow/Keras through R‚Äôs reticulate package, with multiple hidden layers and dropout regularization‚Äù.\n\n\n\n\n\nData Split Summary\n\n\nDataset\nSamples\nPercentage\nClass_Distribution\n\n\n\n\nTraining\n6403\n60%\nBalanced\n\n\nValidation\n2134\n20%\nBalanced\n\n\nTest\n2134\n20%\nBalanced\n\n\nTotal\n10671\n100%\nOriginal\n\n\n\n\n\n\n\n\nüìä Model Evaluation Strategy\nWe used‚Ä¶.\n\nAccuracy: ‚Ä¶..\nad more here"
  }
]