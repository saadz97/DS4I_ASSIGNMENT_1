<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Avalanche Forecasting</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8b20df304ae43519df5d458fcc8598a6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><h1 class="title display-7"></h1></header><div class="quarto-title-block"><div class="quarto-title-tools-only"><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>




<section id="avalanche-hazard-forecasting" class="level1 hero">
<h1>Avalanche Hazard Forecasting</h1>
<div class="hero-subtitle">
<p>Predicting forecasted avalanche hazard levels from 15 years of Scottish data using a neural network.</p>
</div>
</section>
<div class="tabset-margin-container"></div><div class="panel-tabset tabset-pills tabset-fade">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Abstract</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">1 Introduction and Lit Review</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false">2 Data and Methods</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" role="tab" aria-controls="tabset-1-4" aria-selected="false">3 Model building, tuning and evalutation</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-5-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-5" role="tab" aria-controls="tabset-1-5" aria-selected="false">4 Discussion and Conclusion</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-6-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-6" role="tab" aria-controls="tabset-1-6" aria-selected="false">5 Use of Large Language Models (LLMs)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-7-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-7" role="tab" aria-controls="tabset-1-7" aria-selected="false">References</a></li></ul>
<div class="tab-content tabset-pills tabset-fade">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<p>This study investigates the application of neural networks for avalanche hazard level prediction using a large historical dataset from the Scottish Avalanche Information Service. The models are trained on predictors such as meteorological, snowpack and topographical features. Initially, the historical data was preprocessed into training, validation and tests sets. The model was trained and performance was compared against baseline expectations with feature importance also being investigated.</p>
<p>The results of the model are promissing with the neural networks being able to successfully capture the non-linear and complex interactions between the features. Challenges such as misclassificationrates for high hazard levels and limited interpretability of the model were additionally observed, showing potential for improvement in the future with methods such as resampling. Overall, this investigation shows the potential and limitations of implementing neural networks for avalanche hazard level prediction.</p>
</section>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">1.1 Introduction</h3>
<p>Scotland is a region with frequent snow accumulation as well as a high amount of human activity. It is therefore vital that avalanche forecasting is carried out as a key component of their mountain safety and reducing economic losses. Avalanche forecasting can prove to be difficult as it is dependent on many factors with complex relationships and interactions, such as snow properties, meteorological conditions and terrain features. Usually, avalanche forecasts are predicted using a combination of weather reports as well as field observations to release daily bulletins. Although these are essential, the rapidly expanding historical weather and avalanche datasets have created the oppurtunity for new, data driven methods.</p>
<p>Machine learning, specifically neural networks, have become strong contenders for avalanche forecasting due to their ability to model complex and multi-dimensional relationships. From the ever growing historical datasets, these methods can uncover patterns that have previously not been utilised in avalanche forecasting. The Scottish Avalanche Information Service (SAIS) provides a large amount of historical data that can be utilised to train neural networks to improve avalanche risk management. In this report, this dataset will be used to train and evaluate neural network models for the potential to forecast avalanches in the Scottish region.</p>
</section>
<section id="literature-review" class="level3">
<h3 class="anchored" data-anchor-id="literature-review">1.2 Literature Review</h3>
<p>Avalanche forecasting and research spans across snow science, meteorology and data science. Initially, intuition and rules of thumb based on snowfall and temperature were used to predict avalanches. These methods proved to be inconsistent and struggled to generalise diverse snow conditions [7]. These methods moved towards statistical and probabilistic methods. This more systematic process could integrate more predictors as well as quantify uncertainty [8].</p>
<p>Due to the ever-growing amount of data, machine learning methods have been utilised to predict avalanches. Random forrest and support vector machines can be applied to large avalanche datasets and achieve high accuracy, especially compared to more traditional models [4]. This proves the potential for machine learning methods to capture complex and nonlinear relationships between avalanche predictor variables. From machine learning methods, neural networks stand out due to their flexibility and ability to work with high dimensional predictors. Deep learning applications related to avalanches are snow depth prediction, snowpack classification and exploratory hazard modelling [9]. Although results have been positive, neural networks have not been fully developed or implemented into avalanche forecasting.</p>
<p>For avalanche forecasting, snowpack stability indicators are vital. These include temperature gradients and snow temperature profiles [13]. Additionally, meteorological variables, such as wind speed, air temperature and precipitation, directly affect snow deposition and terrain features, such as aspect and slope angle affect avalanche likelyhood [1]. These predictors have complex relationships and patterns that traditional methods do not capture however mehtods such as neural networks can uncover these subtle interactions.</p>
<p>From the literature, it has been shown that avalanche forecasting methods have evolved from intuition and expert-based assessments to statistical methods and then further to machine learning methods. Studies have been completed to confirm the feasibility of data-driven avalanche forecasting however integration into operational practices is underdeveloped.</p>
</section>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<p>The dataset used in this study consists of daily avalanche forecasts for six forecasting regions across Scotland (Creag Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon), collected by the Scottish Avalanche Information Service (SAIS) between 2009 and 2025. It contains 10,671 observations and 34 variables including weather conditions, topography, snowpack properties, and avalanche hazard levels.</p>
<p>The target variable for this project is the Forecast Avalanche Hazard (FAH), a categorical variable with five ordered levels: Low, Moderate, Considerable -, Considerable +, and High.</p>
<p>Data Cleaning and Preprocessing was done to remove observations that did not make logical sense, transform the data where needed and remove necessary variable. Furthermore, Exploratory Data Analysis (EDA) was then performed to assess variable distributions and perform imputation, outlier handling and scaling of data.</p>
<section id="data-cleaning-and-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning-and-preprocessing">2.1 Data Cleaning and Preprocessing</h3>
<p>The raw dataset underwent extensive cleaning prior to analysis. Initially, rows with missing FAH values (109 rows) were removed, and FAH levels were replaced with a numerical representation from 0 to 4 for compatibility with the neural network software. Other categorical variables, such as precipitation type and forecast region, were similarly transformed into numerical formats. The Date column was transformed into Month to capture seasonal effects.</p>
<p>Variables unrelated to forecasting or with high missingness (more than 20%), frequent errors, limited predictive value, or poorly defined measures were excluded, resulting in a set of well-defined predictors most relevant for forecasting.</p>
<p>Range checks were applied to remove physically impossible or implausible values based physical limits and regional context.</p>
<p>For example, altitude was limited to 0–1,345 m, the height of Ben Nevis, and wind speeds to 176 mph, reflecting the highest recorded gust in the Highlands [11].</p>
<p>Slope inclines were restricted to 0–90°, and wind directions were constrained to 0–360°. Cloud cover was limited to 0–100%, total snow depth to 0–1,500 cm (with unrealistic spikes removed), and foot penetration depth to 100 cm. Snow temperatures above 0.5 °C were flagged as implausible, since snow begins melting at 0 °C, and maximum temperature gradients were limited to below 25 °C per 10 cm [15]. Observations where foot penetration exceeded total snow depth were also removed.</p>
<p>Circular variables such as wind direction and aspect were transformed into sine and cosine components.</p>
</section>
<section id="predictor-sets" class="level3">
<h3 class="anchored" data-anchor-id="predictor-sets">2.2 Predictor Sets</h3>
<p>The data was split into three predictor sets; pred1, pred2, and pred3, where each predictor set represents different types of information relating to snow and avalanche conditions. Furthermore, predictor set 1 represents the location data such as the area, exact location coordinates, and how steep the slope is etc. Predictor set 2 represents the air conditions such as air temperature, cloud cover, and wind speed etc. Lastly, predictors 3 represents the snow conditions such as how deep the snow is, the temperature of the snow at different layers and foot penetration. All three predictor sets essentially contribute to determining how the events have affected the snowfall. In addition, the month variable does not fit in any predictor set, but rather adds seasonal context since snow behaviour differs throughout the year. The predictor sets were separated into the following sets:</p>
<p>Predictor Set 1: Area, longitude, latitude, Alt, Incline, Aspect_sin, and Aspect_cos</p>
<p>Predictor Set 2: Air.Temp, Wind.Speed, Cloud, Precip.Code, Drift, Summit.Air.Temp, Summit.Wind.Speed, WindDir_sin, WindDir_cos, SummitWindDir_sin, and SummitWindDir_cos</p>
<p>Predictor Set 3: Total.Snow.Depth, Foot.Pen, Rain.at.900, Max.Temp.Grad, Max.Hardness.Grad, and Snow.Temp</p>
<p>Other Variables: Month</p>
</section>
<section id="imputation" class="level3">
<h3 class="anchored" data-anchor-id="imputation">2.3 Imputation</h3>
<p>Multiple Imputation by Chained Equations (MICE) handles missing data that creates multiple versions of a dataset rather than selecting the first best guessed value. The MICE function ran under the assumption of Missing at Random (MAR), which means that it was assumed that the probability of a value being missing depends on the observed values in the dataset, but are independent of the unobserved values [5].</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      FAH Area longitude latitude Alt Cloud Air.Temp Incline Wind.Speed
10069   1    1         1        1   1     1        1       1          1
314     1    1         1        1   1     1        1       1          1
41      1    1         1        1   1     1        1       1          0
12      1    1         1        1   1     1        1       0          1
3       1    1         1        1   1     1        1       0          1
5       1    1         1        1   1     1        0       1          1
3       1    1         1        1   1     1        0       1          0
1       1    1         1        1   1     1        0       0          0
1       1    1         1        1   1     1        0       0          0
6       1    1         1        1   1     0        1       1          1
1       1    1         1        1   1     0        1       1          0
1       1    1         1        1   1     0        1       0          0
3       1    1         1        1   1     0        0       1          0
8       1    1         1        1   1     0        0       0          0
6       1    1         1        1   0     1        1       1          1
1       1    1         1        1   0     1        1       1          1
1       1    1         1        1   0     1        1       0          1
        0    0         0        0   8    19       21      27         59
      Aspect_sin Aspect_cos    
10069          1          1   0
314            0          0   2
41             1          1   1
12             1          1   1
3              0          0   3
5              1          1   1
3              1          1   2
1              1          1   3
1              0          0   5
6              1          1   1
1              1          1   2
1              1          1   3
3              1          1   3
8              1          1   4
6              1          1   1
1              0          0   3
1              0          0   4
             320        320 774</code></pre>
</div>
</div>
<p>The figure above shows the pattern of missing values for a subset of 10 variables, where blue represents no missing values and red represents missing values present. It can be seen that the sum of missing values for the 10 variables was 774 values, and it can be seen that aspect_sin and aspect_cos have the highest number of missing values (320 values) among these 10 variables. Furthermore, wind.speed, incline, air.temp and cloud have the next highest missing values of 59, 27, 21 and 19 respectively.</p>
<p>In addition, it was seen that variables like area, latitude, longitude and alt have no missing values, which shows that they are reliable predictors for the missing values in other varaiables.</p>
<p>MICE was run with five iterations creating five slightly different datasets and Predictive Mean Matching (PMM) was used, which uses the information in the dataset to predict what the missing values would most likely be. In addition, all variables that were in the cleaned dataset which contained missing values were selected for imputation, as the highest percentage of missing data was 14.2%, so there were enough observed variables to predict the unobserved variables well.</p>
<p>In addition, after running the MICE imputation, the density plots were analysed to determine if the variables were imputed well or not, with imputed values represented by the red line and observed values represented by the blue line. Imputed values should not change the distribution of a variable [10], thus, if the imputed variables shows a different distribution to the observed variables, the variable was removed from the dataset.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The figure above displays a stripplot from the MICE imputation, which shows the distribution across all variables and the five different imputation sets of observed values represented by blue and the imputed values represented by red. It can be seen that the red and blue dots overlap well and follow similar distributions in multiple variables which shows that the imputation worked well.</p>
<p>However, variables such as Alt, Air.Temp, and Incline show that imputed values do not follow the same distribution as the observed values as the red dots do not overlap well and indicates that the data failed to impute well under the MAR assumption.</p>
<p>The density plots for the variables in question were plotted as well as the density plot of Max.Hardness.Grad which imputed well as a comparison.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>It can be seen from the figure above that the imputed values in red do not follow the observed values in blue for Alt, Incline, and Air.Temp. However, comparing it to a well imputed variable like Max.Hardness.Grad, it can be seen that the imputed values follows the observed values distribution closely. This exercise was done for each of the imputed variable, however, just the variables that did not impute well and one variable that did impute well was shown in the final report.</p>
<p>The variable that did not impute well was removed from the dataset, and the new dataset was refitted using MICE which ensured that the imputations better satisfied the MAR assumption and produced reliable results.</p>
</section>
<section id="outlier-handling" class="level3">
<h3 class="anchored" data-anchor-id="outlier-handling">2.4 Outlier Handling</h3>
<p>Outlier Capping or Winsorization is a method that handles outliers by converting the extreme high values to the value of the highest data point that is not considered an outlier [2]. This method was used on the imputed dataset to account for any values that were extreme to ensure that the distribution is not skewed.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "longitude - Outliers capped: 207"
[1] "latitude - Outliers capped: 208"
[1] "Wind.Speed - Outliers capped: 45"
[1] "Summit.Air.Temp - Outliers capped: 207"
[1] "Summit.Wind.Speed - Outliers capped: 95"
[1] "Total.Snow.Depth - Outliers capped: 103"
[1] "Foot.Pen - Outliers capped: 68"
[1] "Max.Temp.Grad - Outliers capped: 93"
[1] "Max.Hardness.Grad - Outliers capped: 48"
[1] "Snow.Temp - Outliers capped: 135"</code></pre>
</div>
</div>
<p>For each variable in the dataset, the 1st and 99th percentile was calculated to address the extreme values, preserving 98% of the data. If a value fell below the 1st percentile, it was capped to the 1st percentile value, furthermore, if a value fell above the 99th percentile, it was capped to the 99th percentile value.</p>
<p>In addition, outlier handling was intentionally done after imputation, in the event that MICE generated a value that fell outside of the realistic range of values in the original dataset, considering that data cleaning had already taken place.</p>
<p>Furthermore, it was seen that the need to cap outliers was needed as multiple variables had around ~1% of its data capped, which shows that the data contained extreme values either in the original dataset or from imputation.</p>
</section>
<section id="correlation-analysis" class="level3">
<h3 class="anchored" data-anchor-id="correlation-analysis">2.5 Correlation Analysis</h3>
<p>A correlation analysis was performed to examine the patterns in the variables and assess potential multicollinearity effects.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>It can be seen from the figure above that correlation between variables exist as the dark blue squares represent strong positive correlations and the dark red squares represent strong negative correlations. The variables from predictor set 1 are represented by dark blue squares, indicating strong positive correlations between the variables. Moreover, FAH has moderate correlations with the variables as the colour range are in the lighter section of the colour scale.</p>
<p>Furthermore, it is important to note that the white squares represent independent variables (correlation of 0), and it can be seen that there are multiple variables such as Aspect_cos and Area have white squares which indicates that multicollinearity is not excessive.</p>
<p>Lastly, it can be seen that variables with similar correlation patterns have grouped together creating visible blocks of related variables, which confirms the predictor set groupings capture different types of information.</p>
</section>
<section id="data-scaling" class="level3">
<h3 class="anchored" data-anchor-id="data-scaling">2.6 Data Scaling</h3>
<p>Scaling was performed to ensure that variables with different units and scales contributed equally to the model and variables with larger numeric values did not dominate the model.</p>
<p>The data was split using a 70/30 split, which was stratified using FAH to ensure that both the training and testing sets follow a similar distribution of the target variable, FAH. Thereafter, the numeric values from the training dataset were used to calculate the mean and standard deviation of each variable using the preProcess() function and was then applied to both the training and test sets using the predict() function. Furthermore, this approach prevents data leakage which occurs when information from the test set goes into the training set.</p>
</section>
</div>
<div id="tabset-1-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-4-tab">
<section id="software-and-setup" class="level3">
<h3 class="anchored" data-anchor-id="software-and-setup">3.1 Software and Setup</h3>
<p>This section outlines the comprehensive methodology employed for predicting avalanche hazard levels using neural networks. We describe the data preprocessing pipeline, experimental design with four distinct predictor sets, neural network architecture selection process, and hyperparameter tuning strategy. The methodology ensures reproducible results while systematically evaluating the contribution of different variable types to forecasting accuracy.</p>
<p>Neural network modelling was implemented using the <strong><code>keras</code></strong> library, accessed through its R wrapper. Keras is an open-source deep learning framework known for its modularity and relatively gentle learning curve compared to other libraries such as PyTorch. Its modular design makes it well suited for projects that require experimentation with multiple configurations.</p>
<p>Because Keras functions in R act as wrappers around the underlying Python functions, version control is critical to ensure reproducibility. This project was developed in <strong><code>R version 4.5.1 (2025-06-13)</code></strong> with <strong><code>Python version 3.11.6</code></strong>.</p>
</section>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">3.2 Data Preparation</h3>
<p>The modelling goal was to predict Forecasted Avalanche Hazard (FAH). To prevent data leakage, the Observed Avalanche Hazard (OAH) variable was excluded from the predictor sets, since this information would not be available in real-world forecasting scenarios.</p>
<p>A 70/30 train–test split was applied, with a fixed random seed to ensure reproducibility. Although a validation set is often held out, Keras provides internal validation handling, so this was not specified manually.</p>
<div class="cell">
<div id="tbl-imbalance" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-imbalance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Comparison of percentage of each category in the training and test sets
</figcaption>
<div aria-describedby="tbl-imbalance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 9%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">category 0</th>
<th style="text-align: right;">category 1</th>
<th style="text-align: right;">category 2</th>
<th style="text-align: right;">category 3</th>
<th style="text-align: right;">category 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">train</td>
<td style="text-align: right;">0.328</td>
<td style="text-align: right;">0.303</td>
<td style="text-align: right;">0.236</td>
<td style="text-align: right;">0.088</td>
<td style="text-align: right;">0.044</td>
</tr>
<tr class="even">
<td style="text-align: left;">test</td>
<td style="text-align: right;">0.320</td>
<td style="text-align: right;">0.312</td>
<td style="text-align: right;">0.236</td>
<td style="text-align: right;">0.089</td>
<td style="text-align: right;">0.043</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p><a href="#tbl-imbalance" class="quarto-xref">Table&nbsp;1</a> reports the class distribution across training and test sets. The proportions are broadly consistent between the two partitions, but the data exhibits strong class imbalance. Categories 0 and 1 each account for roughly 30% of the data, category 2 for 23%, while categories 3 and 4 are severely underrepresented (9% and 4% respectively). This imbalance risks biasing the model toward predicting majority classes.</p>
<p>To mitigate this, custom class weights were introduced. The weights were set inversely proportional to the prevalence of each category in the training set, so that errors on minority classes carried higher penalties during training. Alternative methods such as bootstrap resampling could also be considered, but were not explored here.</p>
</section>
<section id="predictor-sets-1" class="level3">
<h3 class="anchored" data-anchor-id="predictor-sets-1">3.3 Predictor Sets</h3>
<p>For this project there were 4 predictor sets that were evaluated. Predictor set 1 contained variables relating to the location of the observation. Predictor set 2 contained variables relating to the weather conditions of the observation. Predictor set 3 contained variables relating to a snow pack test. Predictor set 4 contained all the varibles in the dataset and thus contained all three predictor sets inside it. The final model will be built using all the available data, so predictor set 4. The remaining three predictor sets serve as yardstick to which we can compare which sets of variables are important.</p>
</section>
<section id="model-development" class="level3">
<h3 class="anchored" data-anchor-id="model-development">3.4 Model Development</h3>
<p>Neural networks are extremely flexible and configurable models. But this configurability means that there are several parameters that need to be defined before the model can be trained. In order to find the optimal configuration, hyperparameter tuning needs to be done first. Some of the hyperparameters we have control over include: number of layers, number of nodes per layer, the acitvation function to use on each layer, the dropout rate and the learning rate. <strong><code>keras</code></strong> is flexible and allows the user control of almost every aspect of the model through the parameters. In order to do the hyperameter tuning in R, the <strong><code>kerastuneR</code></strong> library is used. But in order to do the hyperparameter tuning with <strong><code>kerastuneR</code></strong> the model needs to be wrapped in a function that accepts as an input the hyperparameter configuration, and outputs a model.</p>
<p>For this project, it was decided to tune for the number of layers, number of nodes on each layer and the learning rate. The activation function was chosen to be rectified linear units(Relu) and the dropout rate was set to 0,1. Each dense layer was followed by a dropout layer. The specific values that were tuned across were 1-5 layers(step size of 1) with 30-50(step size of 10) nodes on each layer and 5 equally spaced learning rates [0,01 - 0,0001]. Therefore, there were a total of 75 unique models that could be fitted. The metric the Neural Network tries to minimize is the <strong><code>metric_categorical_accuracy</code></strong>. This was chosen since the target variable has more than 1 category.</p>
</section>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">3.5 Hyperparameter Tuning</h3>
<p><strong><code>kerastuneR</code></strong> has multiple tuning algorithms, we have used the <strong><code>RandomSearch</code></strong> algorithm. <strong><code>RandomSearch</code></strong> takes random combinations of the provided hyperparameters and fits the model each time. Since the combination of hyperparameters is random, there is a possibility that the same mode configuration is ran multiple times by the algorithm. The algorithm does attempt to mitigate this but it not guaranteed to stop duplicate runs. We do have some control over this though by setting the <strong><code>max_trials</code></strong> variable to the total number of unique models that can be specified from our selected tuning ranges. It has also been specified that each model should be fit 3 times by setting <strong><code>executions_per_trial = 3</code></strong>. This reduces variation in the results since there is an element of randomness in the initialisation of the model. A validation split of 20% was used and <strong><code>shuffle = T</code></strong> was used. Doing this shuffles the which observations get used as the validation set. This helps reduce the chances of overfitting.</p>
<p>All the results from tuning were saved into folders so that the results can be extracted and used for further analysis. The tuning was was undertaken is by no means exhaustive, no tuning can ever be, but the range of values tuned over is relatively small and therefore the results should be taken with a pinch of salt. With more time and perhaps more compute power a better result is possible.</p>
<p><strong><code>kerastuneR</code></strong> saves the tuning results as .json files. Each trial will be its own folder and inside that folder there will be a .json file containing information about the configuration of the Neural Network and the validation accuracy it achieved. The results were compiled into a single table containing the top 3 configurations from each of the predictor sets.</p>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">3.6 Results</h3>
<p>This section presents the comprehensive outcomes of our neural network approach to avalanche hazard prediction. We begin by examining the performance hierarchy across different predictor sets, followed by detailed analysis of optimal hyperparameter configurations. The section culminates in an evaluation of the final model’s predictive capability, with particular attention to class-wise performance across the five avalanche hazard levels.</p>
</section>
<section id="hyperparameter-tuning-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning-outcomes">3.7 Hyperparameter Tuning Outcomes</h3>
<div id="fig-myplot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-myplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-myplot-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-myplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Validation accuracy by hyperparameter configuration across four predictor sets
</figcaption>
</figure>
</div>
<p>Systematic hyperparameter tuning across four predictor sets revealed distinct performance patterns, visualized in the above figure. The plot shows validation accuracy distributions for various architectural configurations, with Predictor Set 2 (weather conditions) achieving the highest peak performance (~66%), closely followed by Predictor Set 4 (all variables) at ~65%.</p>
<p><a href="#tbl-tuning" class="quarto-xref">Table&nbsp;2</a> details the top three configurations for each predictor set, confirming the performance hierarchy. The marginal improvement of Set 2 over Set 4 indicates that weather variables capture the most critical forecasting signals, with topographic and snow-pack data providing limited incremental value.</p>
<div id="tbl-tuning" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-tuning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Top hyperparameter configurations per predictor set, ranked by validation accuracy.
</figcaption>
<div aria-describedby="tbl-tuning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: right;" data-quarto-table-cell-role="th">Predictor set</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Validation accuracy</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Learning rate</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Number of layers</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">nodes on layer 1</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">nodes on layer 2</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">nodes on layer 3</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">nodes on layer 4</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">nodes on layer 5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="3" style="text-align: right; vertical-align: middle !important;">1</td>
<td style="text-align: right;">0.50829</td>
<td style="text-align: right;">0.01000</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.46535</td>
<td style="text-align: right;">0.01000</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">40</td>
</tr>
<tr class="odd">
<td style="text-align: right; border-bottom: 2px solid black;">0.45376</td>
<td style="text-align: right; border-bottom: 2px solid black;">0.01000</td>
<td style="text-align: right; border-bottom: 2px solid black;">5</td>
<td style="text-align: right; border-bottom: 2px solid black;">30</td>
<td style="text-align: right; border-bottom: 2px solid black;">30</td>
<td style="text-align: right; border-bottom: 2px solid black;">30</td>
<td style="text-align: right; border-bottom: 2px solid black;">40</td>
<td style="text-align: right; border-bottom: 2px solid black;">30</td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: right; vertical-align: middle !important;">2</td>
<td style="text-align: right;">0.65826</td>
<td style="text-align: right;">0.00753</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">30</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.65735</td>
<td style="text-align: right;">0.00753</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">40</td>
</tr>
<tr class="even">
<td style="text-align: right; border-bottom: 2px solid black;">0.65621</td>
<td style="text-align: right; border-bottom: 2px solid black;">0.01000</td>
<td style="text-align: right; border-bottom: 2px solid black;">4</td>
<td style="text-align: right; border-bottom: 2px solid black;">40</td>
<td style="text-align: right; border-bottom: 2px solid black;">30</td>
<td style="text-align: right; border-bottom: 2px solid black;">30</td>
<td style="text-align: right; border-bottom: 2px solid black;">40</td>
<td style="text-align: right; border-bottom: 2px solid black;">NA</td>
</tr>
<tr class="odd">
<td rowspan="3" style="text-align: right; vertical-align: middle !important;">3</td>
<td style="text-align: right;">0.62645</td>
<td style="text-align: right;">0.01000</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.62509</td>
<td style="text-align: right;">0.01000</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: right; border-bottom: 2px solid black;">0.62463</td>
<td style="text-align: right; border-bottom: 2px solid black;">0.01000</td>
<td style="text-align: right; border-bottom: 2px solid black;">5</td>
<td style="text-align: right; border-bottom: 2px solid black;">40</td>
<td style="text-align: right; border-bottom: 2px solid black;">40</td>
<td style="text-align: right; border-bottom: 2px solid black;">30</td>
<td style="text-align: right; border-bottom: 2px solid black;">40</td>
<td style="text-align: right; border-bottom: 2px solid black;">40</td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: right; vertical-align: middle !important;">4</td>
<td style="text-align: right;">0.64940</td>
<td style="text-align: right;">0.00505</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.64781</td>
<td style="text-align: right;">0.01000</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="even">
<td style="text-align: right; border-bottom: 2px solid black;">0.64781</td>
<td style="text-align: right; border-bottom: 2px solid black;">0.01000</td>
<td style="text-align: right; border-bottom: 2px solid black;">5</td>
<td style="text-align: right; border-bottom: 2px solid black;">50</td>
<td style="text-align: right; border-bottom: 2px solid black;">40</td>
<td style="text-align: right; border-bottom: 2px solid black;">30</td>
<td style="text-align: right; border-bottom: 2px solid black;">50</td>
<td style="text-align: right; border-bottom: 2px solid black;">50</td>
</tr>
</tbody>
</table>


</div>
</figure>
</div>
<p>Architecturally, models with 4–5 layers and 30–50 nodes per layer consistently outperformed others, with no clear gains beyond 5 layers. The optimal configuration from Predictor Set 4, selected for final evaluation, employed 4 hidden layers with [30, 40, 40, 40] nodes and achieved 64.9% validation accuracy representing the best balance of performance and generalizability.</p>
</section>
<section id="optimal-architecture" class="level3">
<h3 class="anchored" data-anchor-id="optimal-architecture">3.8 Optimal Architecture</h3>
<p><img src="best_model_plot.png" alt="Configuration of the best model" width="500"></p>
<p>The selected architecture utilizes ReLU activation in hidden layers and softmax output activation, appropriate for the multi-class ordinal nature of avalanche hazard prediction. This configuration represents an optimal balance between model complexity and predictive performance for the comprehensive feature set. This architecture balanced model complexity with predictive accuracy and was used for final evaluation on the test set.</p>
</section>
<section id="model-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="model-evaluation">3.9 Model Evaluation</h3>
<section id="confusion-matrix" class="level4">
<h4 class="anchored" data-anchor-id="confusion-matrix">3.9.1 Confusion Matrix</h4>
<div id="tbl-confusion" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-confusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Confusion matrix for avalanche hazard predictions.
</figcaption>
<div aria-describedby="tbl-confusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: left; empty-cells: hide; border-bottom: hidden;"></th>
<th colspan="5" data-quarto-table-cell-role="th" style="text-align: center; border-bottom: hidden; padding-bottom: 0; padding-left: 3px; padding-right: 3px;"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Predicted
</div></th>
</tr>
<tr class="even">
<th style="text-align: left;" data-quarto-table-cell-role="th">Actual</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">0</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">1</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">2</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">3</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: right;">857</td>
<td style="text-align: right;">420</td>
<td style="text-align: right;">88</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: right;">118</td>
<td style="text-align: right;">326</td>
<td style="text-align: right;">219</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2</td>
<td style="text-align: right;">29</td>
<td style="text-align: right;">231</td>
<td style="text-align: right;">413</td>
<td style="text-align: right;">186</td>
<td style="text-align: right;">94</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">16</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">8</td>
</tr>
</tbody>
</table>


</div>
</figure>
</div>
<p>The confusion matrix of the the fitted model is reported in <a href="#tbl-confusion" class="quarto-xref">Table&nbsp;3</a>. It is clear that category 0 is the best estimated while categories 3 and 4 are estimated the worst. This result was expected since category 3 and 4 are very underepresented in both the training and test data. Concrete metrics of the model performance is given in the table below.</p>
</section>
<section id="class-level-metrics" class="level4">
<h4 class="anchored" data-anchor-id="class-level-metrics">3.9.2 Class-Level Metrics</h4>
<div id="tbl-class" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-class-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Comprehensive classification metrics by avalanche hazard class.
</figcaption>
<div aria-describedby="tbl-class-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Sensitivity</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Specificity</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Pos Pred Value</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Neg Pred Value</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Precision</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Recall</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">F1</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Prevalence</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Detection Rate</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Detection Prevalence</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Balanced Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Class: 0</td>
<td style="text-align: right;">0.854</td>
<td style="text-align: right;">0.751</td>
<td style="text-align: right;">0.617</td>
<td style="text-align: right;">0.916</td>
<td style="text-align: right;">0.617</td>
<td style="text-align: right;">0.854</td>
<td style="text-align: right;">0.716</td>
<td style="text-align: right;">0.320</td>
<td style="text-align: right;">0.273</td>
<td style="text-align: right;">0.443</td>
<td style="text-align: right;">0.802</td>
</tr>
<tr class="even">
<td style="text-align: left;">Class: 1</td>
<td style="text-align: right;">0.332</td>
<td style="text-align: right;">0.817</td>
<td style="text-align: right;">0.452</td>
<td style="text-align: right;">0.729</td>
<td style="text-align: right;">0.452</td>
<td style="text-align: right;">0.332</td>
<td style="text-align: right;">0.383</td>
<td style="text-align: right;">0.312</td>
<td style="text-align: right;">0.104</td>
<td style="text-align: right;">0.230</td>
<td style="text-align: right;">0.575</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Class: 2</td>
<td style="text-align: right;">0.557</td>
<td style="text-align: right;">0.775</td>
<td style="text-align: right;">0.433</td>
<td style="text-align: right;">0.850</td>
<td style="text-align: right;">0.433</td>
<td style="text-align: right;">0.557</td>
<td style="text-align: right;">0.487</td>
<td style="text-align: right;">0.236</td>
<td style="text-align: right;">0.131</td>
<td style="text-align: right;">0.303</td>
<td style="text-align: right;">0.666</td>
</tr>
<tr class="even">
<td style="text-align: left;">Class: 3</td>
<td style="text-align: right;">0.072</td>
<td style="text-align: right;">0.987</td>
<td style="text-align: right;">0.345</td>
<td style="text-align: right;">0.916</td>
<td style="text-align: right;">0.345</td>
<td style="text-align: right;">0.072</td>
<td style="text-align: right;">0.119</td>
<td style="text-align: right;">0.089</td>
<td style="text-align: right;">0.006</td>
<td style="text-align: right;">0.018</td>
<td style="text-align: right;">0.529</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Class: 4</td>
<td style="text-align: right;">0.059</td>
<td style="text-align: right;">0.996</td>
<td style="text-align: right;">0.421</td>
<td style="text-align: right;">0.959</td>
<td style="text-align: right;">0.421</td>
<td style="text-align: right;">0.059</td>
<td style="text-align: right;">0.104</td>
<td style="text-align: right;">0.043</td>
<td style="text-align: right;">0.003</td>
<td style="text-align: right;">0.006</td>
<td style="text-align: right;">0.528</td>
</tr>
</tbody>
</table>


</div>
</figure>
</div>
<p>Sensitivity gives the percent of the time the model predicted an observation as belonging to a category and it actually belonging to that category. The sensitivity for class 0 is the best with a value of 0,854. There is a steep drop off for the other classes but class 2 is the second highest with a value of 0,557 and the rest are all below 0.5 meaning that more often than not, the model is unable to identify the correct category. An analogy for sensitivity is a test with high sensitivity (close to the maximum of 1) will identify most of the patients with with the flu as having the flu but this may mean that lots of patients without the flu also get identified as having the flu. An extreme case may be if the model predicted all observations as belonging to category 0. Then the sensitivity would be 1, meaning that all observations that belong to category 0 are predicted to be category 0. So there need to be a balance because we do not want this.</p>
<p>Specificity gives the percent of time the an observation does not belong to a specific category and the model predicts it as not belonging to that category. High specificity(close to the maximum of 1) is analogous to a test rarely every flagging someone as having the flu when they don’t. A perfect model will have high sensitivity with a high specificity, meaning that it is able to identify when observations belong to a category and does not incorrectly predict other observations as belonging to that category. The table above indicates that all the categories have relatively high specificity. Class 4 has a specificity of 0.996 which on the surface looks great but because the sensitivity is so low, this high specificity just means that the model rarely every predicts any observations as belonging to category 4. The same follows for category 3. This is also seen in the extremely low detection rate and prevalence for these categories. These results, specifically for categories 3 and 4 are an indication that perhaps just reweighting the classes was not enough to overcome the imbalance in the data.</p>
<p>A better metric for instances such as this where the data is imbalanced is the F1 score. The F1 score is the harmonic mean of the precision and recall. The precision is the percentage of observations that belonged to a category and were correctly predicted as belonging to that category. The recall is just the sensitivity. The F1 score, same as the other metrics, ranges from 0 to 1 with 0 being the worst and 1 being the best. The F1 score is useful because the harmonic mean because it is less affected by extreme values than the arithmetic mean. Categories 4 and 3 are by far the worst with values of 0,104 and 0,119. Category 0 has the best F1 score with a value of 0,716. Then category 1 has a value of 0,383 and category 3 has a score of 0,487.</p>
<p>All these values, except for category are quite poor and indicate lack of predictive power. This is reflected in the fact that the model achieved an accuracy of 51,7% on the test set. The model is better than blindly guessing which we expect to return an accuracy of 20% but the model lacks predictive power.</p>
</section>
</section>
<section id="interpretation" class="level3">
<h3 class="anchored" data-anchor-id="interpretation">3.10 Interpretation</h3>
<p>The results highlight the challenges of severe class imbalance. While class reweighting improved performance somewhat, minority classes remained poorly predicted. This suggests that more aggressive strategies may be necessary, such as resampling (e.g., bootstrapping or SMOTE), expanding the hyperparameter search space, or exploring alternative architectures.</p>
<p>Despite these limitations, the study provides useful insights. It demonstrates that weather variables are the dominant predictors of avalanche hazard and that neural networks can achieve moderate accuracy with relatively simple architectures. However, improving detection of rare but high-consequence hazard categories remains an important direction for future work.</p>
</section>
<section id="extra-resources" class="level3">
<h3 class="anchored" data-anchor-id="extra-resources">Extra resources</h3>
<p><a href="https://cran.r-project.org/web/packages/keras/vignettes/" target="_blank"><code>keras</code></a></p>
<p><a href="https://eagerai.github.io/kerastuneR/" target="_blank"><code>kerastuneR</code></a></p>
</section>
</div>
<div id="tabset-1-5" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-5-tab">
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">4.1 Discussion</h3>
<p>From the results, it can be seen that the neural network are capable and accurate for predicting avalanche hazard levels. They learn and find meaningful patterns and interaction through the diverse predictor sets of meteorological, snowpack and topographical features.</p>
<p>However, several limitations could be observed. Classification accuracy was uneven across hazard categories. The model was accurate at predicting low and moderate hazard levels however misclassification rates increased for higher hazards. This issue occurs due to class imbalance and reduced training signal due to the scarsity of high level hazards in the dataset. It is therefore suggested that resampling strategies could be implemented to improve the model classification accuracy. Another important limitation is the interpretability of neural networks. Neural networks are treated as black boxes and therefore interpretability is limited for vital decision making.</p>
<p>The results from this investigation are positive for neural networks. This method is however complex and computationally expensive. It is therefore vital that there is collaboration between data scientists and experts to integrate the model to complement existing methods rather than replace them all together.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">4.2 Conclusion</h3>
<p>This project showcased the vital application of data-driven methods, such as neural networks, to predict avalanche hazard levels. The neural network model developed was successful at integrating all predictor features, such as meteorological, snowpack and topographical features, to find patterns and interactions that can be used to predict avalanche hazard levels. Challenges such as class imbalance, limited performance with high hazard levels and interpretability were observed but dispite these challenges, the model was still proven to be highly accurate.</p>
</section>
</div>
<div id="tabset-1-6" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-6-tab">
<p>Large Language Models (LLMs) were used as a collaborative tool to support certain sections by suggesting approaches and handling repetitive tasks as well as optimising and enhancing the R code to run more efficiently and produce more aesthetically pleasing figures.</p>
<section id="data-cleaning-exploratory-data-analysis-eda" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning-exploratory-data-analysis-eda">5.1 Data Cleaning &amp; Exploratory Data Analysis (EDA)</h3>
<p>In addition to consulting journals and websites (<em>references</em>), LLMs were used to understand the meaning of each variable, their measurement units, plausible value ranges, and to help identify potentially erroneous values in the dataset. Furthermore, LLMs assisted in assessing the reliability of variables, and whether categorical groupings of certain variables showed signs of being meaningful. Example prompt: “How would I interpret this wetness variable: values -1, 0, 1, 2, 3, 4, 6, 10; respective counts: 51, 363, 6384, 3007, 27, 7, 2, 166.”</p>
<p>Moreover, LLMs also provided suggestions for handling outliers and missing (NA) values and helped write repetitive or time-consuming R code. Example prompt: “Provide R code that will generate side-by-side boxplots for all numeric variables in the dataset.” and improved visualisation aesthetics. Example prompt: “Improve the presentation of this boxplot: ‘code’.” “Create a ggplot that visualises validation accuracy across different hyperparameter combinations, clearly highlighting the best-performing combination.” Finally, LLMs assisted in identifying and fixing coding errors. Example prompt: “Locate the error in this code chunk and return the corrected code: ‘code’.”</p>
</section>
<section id="github-support" class="level3">
<h3 class="anchored" data-anchor-id="github-support">5.2 GitHub Support</h3>
<p>LLMs provided assistance when dealing with GitHub issues, including resolving merge conflicts and other version control challenges. Example prompt: “I’m trying to commit my work to the shared Git Hub repository. What does this error from my terminal in R studio mean: ‘error message’.”</p>
</section>
<section id="website-design" class="level3">
<h3 class="anchored" data-anchor-id="website-design">5.3 Website Design</h3>
<p>LLMs supported the development of the website by translating design ideas into functional code. They were also useful for troubleshooting rendering issues when the website did not display in the way that its was expected to.</p>
</section>
<section id="keras" class="level3">
<h3 class="anchored" data-anchor-id="keras">5.4 Keras</h3>
<p>LLM’s assisted in the implementation of the nueral network through keras. They were used to iron out errors occuring because of mismatched library versions, deprecated libraries and unavailable packages. LLM’s were also used to diagnose structural issues that manifested themselves as unexpected outputs or glitchy behaviour. Example prompts: “I have a chunk of code that tunes a keras model over the number of layers, number of nodes per layer and the learning rate. When I check the results from tuning I see that even models with only 3 layers have a value for the number of nodes, have I done anything wrong in the code or am I misunderstanding the outputted results. This is my code for the model tuner: code inserted”. Results from the LLM were always analysed and an effort was made to understand each line of code suggested.</p>
</section>
<section id="referencing" class="level3">
<h3 class="anchored" data-anchor-id="referencing">5.5 Referencing</h3>
<p>LLMs assisted in creating references for the sources we used. We provided the necessary information to Claude.ai, which then formatted the references in Harvard style.</p>
</section>
<section id="writing-up" class="level3">
<h3 class="anchored" data-anchor-id="writing-up">5.6 Writing Up</h3>
<p>LLMs were used to help improve the written sections of the report. They assisted with fixing grammar, making text clearer, improving flow, and suggesting more concise wording. Example prompts included: “Grammar check: …”, “Improve the way this sentence is written: …”, and “Make this section flow better: …”.</p>
<p>In addition, LLMs were sometimes used to transform sets of bullet points into coherent, well-structured paragraphs, helping to organise ideas in a way that was easy to follow. Furthermore, LLMs were used for structuring report sections and generating README templates, clearly outlined requirements guided the model’s output. In all cases, the results were critically evaluated and refined, with final decisions and implementations made independently.</p>
<p>It is important to note that whilst LLMs contributed in these ways, all code and written content were carefully reviewed and edited to ensure its accuracy and that it clearly conveyed our thought processes, ideas, and reasonings.</p>
</section>
</div>
<div id="tabset-1-7" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-7-tab">
<p>[1] Bellaire, S. and Jamieson, J.B. (2013) ‘Forecasting avalanche danger: recent advances’, <em>Cold Regions Science and Technology</em>, 85, pp.&nbsp;89-102.</p>
<p>[2] Cervinski, M.A., Bietenbeck, A., Katayev, A., Loh, T.P., van Rossum, H.H. and Badrick, T. (2023) ‘Advances in clinical chemistry patient-based real-time quality control (PBRTQC)’, in Makowski, G.S. (ed.) <em>Advances in Clinical Chemistry</em>. 1st ed.&nbsp;Amsterdam: Elsevier, pp.&nbsp;223-261. doi: 10.1016/bs.acc.2023.08.003.</p>
<p>[3] Fierz, C., Armstrong, R.L., Durand, Y., Etchevers, P., Greene, E., McClung, D.M., Nishimura, K., Satyawali, P.K. and Sokratov, S.A. (2009) <em>The international classification for seasonal snow on the ground</em>. IHP-VII Technical Documents in Hydrology No.&nbsp;83, IACS Contribution No.&nbsp;1. Paris: UNESCO-IHP.</p>
<p>[4] Hafner, E., Bühler, Y. and Schweizer, J. (2021) ‘Machine learning in avalanche research: random forests and SVMs for avalanche forecasting’, <em>Natural Hazards and Earth System Sciences</em>, 21, pp.&nbsp;223-239.</p>
<p>[5] Kenward, M.G. and Carpenter, J. (2007) ‘Multiple imputation: current perspectives’, <em>Statistical Methods in Medical Research</em>, 16(3), pp.&nbsp;199-218. doi: 10.1177/0962280206075304.</p>
<p>[6] King, S. (2024) ‘Deadliest, most intense, windiest: Three of the UK’s worst storms’, <em>BBC Weather</em>, 17 October. Available at: https://www.bbc.com/weather/articles/cvglvnyxpx0o (Accessed: 28 September 2025).</p>
<p>[7] LaChapelle, E. (1980) ‘The fundamental processes in conventional avalanche forecasting’, <em>Journal of Glaciology</em>, 26(94), pp.&nbsp;75-84.</p>
<p>[8] McClung, D. (2002) ‘The elements of applied avalanche forecasting: the human issues’, <em>Natural Hazards</em>, 26, pp.&nbsp;111-129.</p>
<p>[9] Mitterer, C. et al.&nbsp;(2016) ‘Deep learning approaches for snowpack characterization’, <em>Proceedings of the International Snow Science Workshop</em>.</p>
<p>[10] Molenberghs, G. and Kenward, M.G. (2006) <em>Missing data in clinical studies</em>. Chichester: Wiley Statistics in Practice.</p>
<p>[11] Morning, H. (no date) ‘Wind speeds in the mountains’, <em>Mountaineering Scotland</em>. Available at: https://www.mountaineering.scot/safety-and-skills/essential-skills/weather-conditions/wind-speeds (Accessed: 28 September 2025).</p>
<p>[12] SAIS (2014) <em>Interpreting snow profiles</em>. [pdf] Scottish Avalanche Information Service. Available at: https://www.sais.gov.uk/wp-content/uploads/2014/11/interpreting_snow_profiles.pdf (Accessed: 28 September 2025).</p>
<p>[13] Schweizer, J. and Jamieson, J.B. (2007) ‘A review of stability tests for snow profiles’, <em>Cold Regions Science and Technology</em>, 47, pp.&nbsp;14-37.</p>
<p>[14] ScienceDirect (no date) ‘Winsorization – an overview’, <em>ScienceDirect Topics</em>. Available at: https://www.sciencedirect.com/topics/mathematics/winsorization (Accessed: 27 September 2025).</p>
<p>[15] Ward, R.G.W., Langmuir, E.D.G. and Beattie, B. (1985) ‘Snow profiles and avalanche activity in the Cairngorm Mountains, Scotland’, <em>Journal of Glaciology</em>, 31(107), pp.&nbsp;18-27. doi: 10.1177/S0022143000004949.</p>
</div>
</div>
</div>


<!-- -->


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb3" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> ""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="an">df-print:</span><span class="co"> paged</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: false</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    number-sections: false</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    smooth-scroll: true</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">    lightbox: true</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="in">```{r setup, include =FALSE}</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidyverse)</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="in">library(knitr)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>:::{.hero}</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="fu"># Avalanche Hazard Forecasting  </span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>:::{.hero-subtitle}</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>Predicting forecasted avalanche hazard levels from 15 years of Scottish data using a neural network.</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>:::{.panel-tabset .tabset-pills .tabset-fade}</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>{{&lt; include sections/Abstract.qmd &gt;}}</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>{{&lt; include sections/Introduction.qmd &gt;}}</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>{{&lt; include sections/Data_and_Methods.qmd &gt;}}</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>{{&lt; include sections/model_dt.qmd &gt;}}</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>{{&lt; include sections/Conclusion.qmd &gt;}}</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>{{&lt; include sections/LLMs_discussion.qmd &gt;}}</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>{{&lt; include sections/References.qmd &gt;}}</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>